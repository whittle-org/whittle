{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Whittle","text":"<p>Whittle is a library for Neural Architecture Search (NAS) aimed at compressing large language models (LLMs). The core idea is to treat the network as a super-network and select sub-networks that optimally balance performance and efficiency. Whittle provides the following functionalities:</p> <ul> <li>Support for a variety of LLMs to define super-networks</li> <li>Checkpoint generation for sub-networks, enabling deployment</li> <li>Downstream evaluation of sub-networks using LM-Eval-Harness</li> <li>A simple interface for various super-network training strategies and multi-objective search methods</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Whittle supports and is tested for python 3.9 to 3.11. </p> <p>You can install whittle with:  <pre><code>pip install whittle\n</code></pre></p>"},{"location":"#install-from-source","title":"Install from source","text":"<p>Install whittle from source to get the most recent version: <pre><code>git clone git@github.com:whittle-org/whittle.git\ncd whittle\npip install -e .\n</code></pre></p>"},{"location":"#projects-that-use-whittle","title":"Projects that use whittle","text":"<ul> <li>Structural Pruning of Pre-trained Language Models via Neural Architecture Search</li> <li>HW-GPT Bench</li> </ul>"},{"location":"#how-to-get-involved","title":"How to get involved","text":"<p>We more than happy for any code contribution. If you are interested in contribution to whittle,  please read our contribution guide.</p>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#contributing","title":"Contributing","text":""},{"location":"contributing/#installation","title":"Installation","text":"<pre><code># Create your own fork of the repository if required and replace whittle-org with your username\ngit clone git@github.com:whittle-org/whittle.git\ncd whittle\npip install -e \".[dev]\"  # Install what's here (the `.` part) and install the extra dev dependancies\n</code></pre> <p>Setup <code>pre-commit</code> to run on every commit</p> <pre><code>pre-commit install\n</code></pre>"},{"location":"contributing/#testing","title":"Testing","text":"<pre><code>pytest\n</code></pre>"},{"location":"contributing/#docstring-writing-guidelines","title":"Docstring Writing Guidelines","text":"<p>When adding or updating functions or classes, please ensure that each has a docstring that follows this format:</p> <ul> <li>Summary: A brief description of what the function or class does.</li> <li>args: List each argument with its name, and a short description of its purpose.</li> <li>return: Describe the return value, including what it represents. Note: After adding or updating the docstring, ensure that the code passes the following command with no warnings:</li> </ul> <pre><code>mkdocs build --clean --strict\n</code></pre>"},{"location":"contributing/#conventional-commits-and-commitizen","title":"Conventional commits and Commitizen","text":"<p>We use commitizen to manage commits. This enforces conventional commits.</p> <p>To make a commit, simply run:</p> <pre><code>cz commit\n</code></pre> <p>This will prompt you to enter a commit message and enforce conventional commit formatting.</p> <p>If you do not use <code>cz commit</code> or make a commit with a conventional commit message, your PR will not pass CI.</p>"},{"location":"contributing/#signing-commits","title":"Signing commits","text":"<p>Note: we recommend using SSH keys for signing commits for convenience (e.g., you can use the same key for commit signing and for authentication to GitHub).</p> <ol> <li>Add a SSH (or GPG) key as a signing key to you GitHub account.</li> <li>Configure <code>git</code> to use the key.</li> </ol> <p>Note: if you don't configure commit signing globally, you will need to use <code>git commit -s</code>/<code>cz commit -s</code> to sign your commits.</p> <p>&lt;!-- ## Release</p> <p>Update the version in <code>pyproject.toml</code> first, say to <code>X.Y.Z</code>. If you maintain a changelog, update it.</p> <p>This part just makes a versioned commit and tag for github and to be able to easily find code at a specific version. It will also help with versioned documentation to have a tag.</p> <pre><code>git add pyproject.toml [changelog-file-if-any]\ngit commit -m \"bump: X.Y.Z\"\ngit tag X.Y.Z\ngit push --tags\ngit push\n</code></pre> <p>Then to release on PyPI:</p> <pre><code>pip install twine # If not already\n\nrm -rf ./dist  # Remove anything currently occupying the dist folder\npython -m build --sdist  # Build a source distribution\ntwine upload dist/*  # Publish to PyPI\n``` --&gt;\n\n## Documentation\n\nView locally\n\n```bash\nmkdocs --serve\n</code></pre> <p>Build and deploy to GitHub Pages. Make sure to specify the github tag you want to deploy.</p> <pre><code>mike deploy --push --update-aliases &lt;TAG&gt; \"latest\"\n</code></pre>"},{"location":"api/whittle/pretrain_super_network/","title":"Pretrain super network","text":""},{"location":"api/whittle/pretrain_super_network/#whittle.pretrain_super_network","title":"whittle.pretrain_super_network","text":""},{"location":"api/whittle/pretrain_super_network/#whittle.pretrain_super_network.setup","title":"setup","text":"<pre><code>setup(\n    model_name: str,\n    model_config: Optional[Config] = None,\n    out_dir: Path = Path(\"../examples/gpt/out/pretrain\"),\n    precision: Literal[\n        \"bf16-true\", \"bf16-mixed\", \"32-true\", None\n    ] = None,\n    initial_checkpoint_dir: Optional[Path] = None,\n    resume: Union[bool, Literal[\"auto\"], Path] = False,\n    data: Optional[DataModule] = None,\n    train: TrainArgs = TrainArgs(\n        save_interval=1000,\n        log_interval=1,\n        global_batch_size=512,\n        micro_batch_size=4,\n        max_tokens=int(3000000000000.0),\n        max_norm=1.0,\n        min_lr=4e-05,\n        lr_warmup_steps=2000,\n        tie_embeddings=False,\n    ),\n    eval: EvalArgs = EvalArgs(interval=1000, max_iters=100),\n    optimizer: Union[str, dict] = \"AdamW\",\n    devices: Union[int, str] = \"auto\",\n    num_nodes: int = 1,\n    training_strategy: str = \"sandwich\",\n    tokenizer_dir: Optional[Path] = None,\n    logger_name: Literal[\n        \"wandb\", \"tensorboard\", \"csv\"\n    ] = \"tensorboard\",\n    seed: int = 42,\n)\n</code></pre> <p>Pretrain a model.</p> PARAMETER DESCRIPTION <code>model_name</code> <p>The name of the model to pretrain. Choose from names in <code>litgpt.config</code>. Use \"list\" to list the supported models.</p> <p> TYPE: <code>str</code> </p> <code>model_config</code> <p>A <code>litgpt.Config</code> object to define the model architecture. Mutually exclusive with <code>model_config</code>. Overrides the <code>model_name</code> if specified.</p> <p> TYPE: <code>Optional[Config]</code> DEFAULT: <code>None</code> </p> <code>out_dir</code> <p>Directory in which to save checkpoints and logs. If running in a Lightning Studio Job, look for it in /teamspace/jobs//share. <p> TYPE: <code>Path</code> DEFAULT: <code>Path('../examples/gpt/out/pretrain')</code> </p> <code>precision</code> <p>The precision to use for finetuning. Determines a compatible precision setting by default.</p> <p> TYPE: <code>Literal['bf16-true', 'bf16-mixed', '32-true', None]</code> DEFAULT: <code>None</code> </p> <code>initial_checkpoint_dir</code> <p>Optional path to a checkpoint directory to initialize the model from. Useful for continued pretraining. Mutually exclusive with <code>resume</code>.</p> <p> TYPE: <code>Optional[Path]</code> DEFAULT: <code>None</code> </p> <code>resume</code> <p>Path to a checkpoint directory to resume from in case training was interrupted, or <code>True</code> to resume from the latest checkpoint in <code>out_dir</code>. An error will be raised if no checkpoint is found. Passing <code>'auto'</code> will resume from the latest checkpoint but not error if no checkpoint exists.</p> <p> TYPE: <code>Union[bool, Literal['auto'], Path]</code> DEFAULT: <code>False</code> </p> <code>data</code> <p>Data-related arguments. If not provided, the default is <code>litgpt.data.TinyLlama</code>.</p> <p> TYPE: <code>Optional[DataModule]</code> DEFAULT: <code>None</code> </p> <code>train</code> <p>Training-related arguments. See <code>litgpt.args.TrainArgs</code> for details.</p> <p> TYPE: <code>TrainArgs</code> DEFAULT: <code>TrainArgs(save_interval=1000, log_interval=1, global_batch_size=512, micro_batch_size=4, max_tokens=int(3000000000000.0), max_norm=1.0, min_lr=4e-05, lr_warmup_steps=2000, tie_embeddings=False)</code> </p> <code>eval</code> <p>Evaluation-related arguments. See <code>litgpt.args.EvalArgs</code> for details.</p> <p> TYPE: <code>EvalArgs</code> DEFAULT: <code>EvalArgs(interval=1000, max_iters=100)</code> </p> <code>optimizer</code> <p>An optimizer name (such as \"AdamW\") or config.</p> <p> TYPE: <code>Union[str, dict]</code> DEFAULT: <code>'AdamW'</code> </p> <code>training_strategy</code> <p> TYPE: <code>str</code> DEFAULT: <code>'sandwich'</code> </p> <code>devices</code> <p>How many devices/GPUs to use. Uses all GPUs by default.</p> <p> TYPE: <code>Union[int, str]</code> DEFAULT: <code>'auto'</code> </p> <code>num_nodes</code> <p>How many nodes the code is being run on.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>tokenizer_dir</code> <p>Optional path to the tokenizer dir that was used for preprocessing the dataset. Only some data module require this.</p> <p> TYPE: <code>Optional[Path]</code> DEFAULT: <code>None</code> </p> <code>logger_name</code> <p>The name of the logger to send metrics to.</p> <p> TYPE: <code>Literal['wandb', 'tensorboard', 'csv']</code> DEFAULT: <code>'tensorboard'</code> </p> <code>seed</code> <p>The random seed to use for reproducibility.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> Source code in <code>whittle/pretrain_super_network.py</code> <pre><code>def setup(\n    model_name: str,\n    model_config: Optional[Config] = None,\n    out_dir: Path = Path(\"../examples/gpt/out/pretrain\"),\n    precision: Literal[\"bf16-true\", \"bf16-mixed\", \"32-true\", None] = None,\n    initial_checkpoint_dir: Optional[Path] = None,\n    resume: Union[bool, Literal[\"auto\"], Path] = False,\n    data: Optional[DataModule] = None,\n    train: TrainArgs = TrainArgs(\n        save_interval=1000,\n        log_interval=1,\n        global_batch_size=512,\n        micro_batch_size=4,\n        max_tokens=int(3e12),  # 3 trillion\n        max_norm=1.0,\n        min_lr=4e-5,\n        lr_warmup_steps=2000,\n        tie_embeddings=False,\n    ),\n    eval: EvalArgs = EvalArgs(interval=1000, max_iters=100),\n    optimizer: Union[str, dict] = \"AdamW\",\n    devices: Union[int, str] = \"auto\",\n    num_nodes: int = 1,\n    training_strategy: str = \"sandwich\",\n    tokenizer_dir: Optional[Path] = None,\n    logger_name: Literal[\"wandb\", \"tensorboard\", \"csv\"] = \"tensorboard\",\n    seed: int = 42,\n):\n    \"\"\"Pretrain a model.\n\n    Arguments:\n        model_name: The name of the model to pretrain. Choose from names in ``litgpt.config``. Use \"list\" to list the supported models.\n        model_config: A ``litgpt.Config`` object to define the model architecture. Mutually exclusive with\n            ``model_config``. Overrides the `model_name` if specified.\n        out_dir: Directory in which to save checkpoints and logs. If running in a Lightning Studio Job, look for it in\n            /teamspace/jobs/&lt;job-name&gt;/share.\n        precision: The precision to use for finetuning. Determines a compatible precision setting by default.\n        initial_checkpoint_dir: Optional path to a checkpoint directory to initialize the model from.\n            Useful for continued pretraining. Mutually exclusive with ``resume``.\n        resume: Path to a checkpoint directory to resume from in case training was interrupted, or ``True`` to resume\n            from the latest checkpoint in ``out_dir``. An error will be raised if no checkpoint is found. Passing\n            ``'auto'`` will resume from the latest checkpoint but not error if no checkpoint exists.\n        data: Data-related arguments. If not provided, the default is ``litgpt.data.TinyLlama``.\n        train: Training-related arguments. See ``litgpt.args.TrainArgs`` for details.\n        eval: Evaluation-related arguments. See ``litgpt.args.EvalArgs`` for details.\n        optimizer: An optimizer name (such as \"AdamW\") or config.\n        training_strategy:\n        devices: How many devices/GPUs to use. Uses all GPUs by default.\n        num_nodes: How many nodes the code is being run on.\n        tokenizer_dir: Optional path to the tokenizer dir that was used for preprocessing the dataset. Only some data\n            module require this.\n        logger_name: The name of the logger to send metrics to.\n        seed: The random seed to use for reproducibility.\n    \"\"\"\n    if model_name == \"list\":\n        available_models = \"\\n\".join(sorted(name_to_config))\n        print(f\"Available values:\\n{available_models}\")\n        quit()\n\n    if initial_checkpoint_dir is not None:\n        initial_checkpoint_dir = extend_checkpoint_dir(initial_checkpoint_dir)\n\n    if tokenizer_dir is not None:\n        tokenizer_dir = extend_checkpoint_dir(tokenizer_dir)\n\n    if model_config is None:\n        # Support both model_name options: meta-llama/Meta-Llama-3-8B &amp; Meta-Llama-3-8B\n        try:\n            model_config = Config.from_name(model_name)\n        except ValueError:\n            print(f\"Model name {model_name} is not supported.\\n\")\n            available_models = \"\\n\".join(sorted(name_to_config))\n            print(f\"Available values:\\n{available_models}\")\n            quit()\n\n    assert training_strategy in training_strategies_cls, print(\n        f\"Training strategy is {training_strategy}. Should be in {list(training_strategies_cls)}\"\n    )\n\n    hparams = capture_hparams()\n    data = TinyLlama() if data is None else data\n\n    config = Config.from_name(model_name) if model_config is None else model_config\n    config.fix_head_size = True\n\n    precision = precision or get_default_supported_precision(training=True)\n    num_devices = parse_devices(devices)\n    out_dir = init_out_dir(out_dir)\n    # in case the dataset requires the Tokenizer\n    tokenizer = Tokenizer(tokenizer_dir) if tokenizer_dir is not None else None\n\n    logger = choose_logger(\n        logger_name,\n        out_dir,\n        name=f\"pretrain-{config.name}\",\n        resume=bool(resume),\n        log_interval=train.log_interval,\n    )\n\n    if num_devices * num_nodes &gt; 1:\n        strategy = FSDPStrategy(\n            auto_wrap_policy={Block},\n            state_dict_type=\"full\",\n            sharding_strategy=\"HYBRID_SHARD\",\n        )\n    else:\n        strategy = \"auto\"\n\n    fabric = L.Fabric(\n        devices=num_devices,\n        num_nodes=num_nodes,\n        strategy=strategy,\n        precision=precision,\n        loggers=[logger],\n    )\n\n    if torch.cuda.is_available() and num_devices &gt; 1:\n        check_nvlink_connectivity(fabric)\n\n    fabric.launch()\n\n    fabric.print(pprint.pformat(hparams))\n    if logger_name in (\"tensorboard\", \"wandb\"):\n        fabric.logger.log_hyperparams(hparams)\n\n    main(\n        fabric,\n        num_devices,\n        seed,\n        initial_checkpoint_dir,\n        resume,\n        config,\n        data,\n        out_dir,\n        tokenizer_dir,\n        tokenizer,\n        train,\n        eval,\n        optimizer,\n        training_strategy,\n    )\n</code></pre>"},{"location":"api/whittle/eval/utils/","title":"Utils","text":""},{"location":"api/whittle/eval/utils/#whittle.eval.utils","title":"whittle.eval.utils","text":""},{"location":"api/whittle/eval/utils/#whittle.eval.utils.convert_and_evaluate","title":"convert_and_evaluate","text":"<pre><code>convert_and_evaluate(\n    model: GPT,\n    tasks: str | None = None,\n    out_dir: Path | str = \"evaluate\",\n    force_conversion: bool = False,\n    num_fewshot: int | None = None,\n    batch_size: int | str = 1,\n    device: str | None = None,\n    dtype: str | dtype | None = None,\n    limit: float | None = None,\n    seed: int = 1234,\n    save_filepath: Path | None = None,\n    access_token: str | None = None,\n) -&gt; None\n</code></pre> <p>Evaluate a model with the LM Evaluation Harness.</p> PARAMETER DESCRIPTION <code>out_dir</code> <p>Directory in which to save the converted checkpoints for evaluation. Saves to <code>checkpoint_dir</code>/evaluate by default.</p> <p> TYPE: <code>Path | str</code> DEFAULT: <code>'evaluate'</code> </p> <code>force_conversion</code> <p>Set to <code>True</code> to reconvert the model and override an existing model.pth from a previous evaluation call.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>tasks</code> <p>CSV of task names to evaluate. Example: \"hellaswag,truthfulqa_mc2,mmlu\"</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>num_fewshot</code> <p>Number of examples in few-shot context.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>batch_size</code> <p>Batch size configuration as positive integer value (default: 1), \"auto\", in the format 'auto:N', where 'auto:4' recomputes the batch size 4 times.</p> <p> TYPE: <code>int | str</code> DEFAULT: <code>1</code> </p> <code>device</code> <p>Device to use for evaluation, for example, \"cuda\" or \"cuda:0\".</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>limit</code> <p>Limit on number of examples per task.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>Random seed.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1234</code> </p> <code>save_filepath</code> <p>The file where the results will be saved. Saves to <code>out_dir/results.json</code> by default.</p> <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> <code>access_token</code> <p>Optional API token to access models with restrictions.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>whittle/eval/utils.py</code> <pre><code>def convert_and_evaluate(\n    model: GPT,\n    tasks: str | None = None,\n    out_dir: Path | str = \"evaluate\",\n    force_conversion: bool = False,\n    num_fewshot: int | None = None,\n    batch_size: int | str = 1,\n    device: str | None = None,\n    dtype: str | torch.dtype | None = None,\n    limit: float | None = None,\n    seed: int = 1234,\n    save_filepath: Path | None = None,\n    access_token: str | None = None,\n) -&gt; None:\n    \"\"\"Evaluate a model with the LM Evaluation Harness.\n\n    Arguments:\n        out_dir: Directory in which to save the converted checkpoints for evaluation.\n            Saves to `checkpoint_dir`/evaluate by default.\n        force_conversion: Set to `True` to reconvert the model and override\n            an existing model.pth from a previous evaluation call.\n        tasks: CSV of task names to evaluate. Example: \"hellaswag,truthfulqa_mc2,mmlu\"\n        num_fewshot: Number of examples in few-shot context.\n        batch_size: Batch size configuration as positive integer value (default: 1),\n            \"auto\", in the format 'auto:N', where 'auto:4' recomputes the batch size 4 times.\n        device: Device to use for evaluation, for example, \"cuda\" or \"cuda:0\".\n        limit: Limit on number of examples per task.\n        seed: Random seed.\n        save_filepath: The file where the results will be saved.\n            Saves to `out_dir/results.json` by default.\n        access_token: Optional API token to access models with restrictions.\n    \"\"\"\n    if tasks is None:\n        from lm_eval.tasks import TaskManager\n\n        taskm = TaskManager()\n        print(\"\\n\".join(taskm.task_index.keys()))\n        print(\n            \"\\n\\nTo evaluate multiple tasks, you can chain the task names \"\n            \"listed above via a comma-separated list.\"\n            \"\\nFor example: `--tasks 'hellaswag,truthfulqa_mc2,mmlu'`. \"\n            \"\\nTo search for a specific task, use `litgpt evaluate list | grep task_name`.\"\n        )\n        return\n\n    pprint(locals())\n\n    if not (isinstance(batch_size, int) and batch_size &gt; 0) and not (\n        isinstance(batch_size, str) and batch_size.startswith(\"auto\")\n    ):\n        raise ValueError(\n            \"batch_size must be a positive integer, 'auto', or in the format 'auto:N'.\"\n        )\n\n    from lm_eval import evaluator\n\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    model = WhittleLM(\n        pretrained=model, device=device, batch_size=batch_size, dtype=dtype\n    )\n\n    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n    results = evaluator.simple_evaluate(\n        model=model,\n        tasks=tasks.split(\",\"),\n        num_fewshot=num_fewshot,\n        batch_size=batch_size,\n        device=device,\n        limit=limit,\n        random_seed=seed,\n        numpy_random_seed=seed,\n        torch_random_seed=seed,\n    )\n    out_dir = Path(out_dir)\n    out_dir.mkdir(parents=True, exist_ok=True)\n    save_filepath = (\n        out_dir / Path(\"results.json\") if save_filepath is None else Path(save_filepath)\n    )\n    prepare_results(results, save_filepath)\n</code></pre>"},{"location":"api/whittle/eval/whittle_llms/","title":"Whittle llms","text":""},{"location":"api/whittle/eval/whittle_llms/#whittle.eval.whittle_llms","title":"whittle.eval.whittle_llms","text":""},{"location":"api/whittle/eval/whittle_llms/#whittle.eval.whittle_llms.WhittleLM","title":"WhittleLM","text":"<pre><code>WhittleLM(\n    pretrained: GPT,\n    backend: (\n        Literal[\"default\", \"causal\", \"seq2seq\"] | None\n    ) = \"default\",\n    revision: str | None = \"main\",\n    subfolder=None,\n    tokenizer: (\n        str\n        | PreTrainedTokenizer\n        | PreTrainedTokenizerFast\n        | None\n    ) = None,\n    truncation: bool | None = False,\n    logits_cache: bool = True,\n    max_length=None,\n    device=\"cuda\",\n    dtype=\"auto\",\n    batch_size=1,\n    max_batch_size=64,\n    trust_remote_code: bool | None = False,\n    use_fast_tokenizer: bool | None = True,\n    add_bos_token: bool | None = False,\n    prefix_token_id: int | None = None,\n    parallelize: bool | None = False,\n    device_map_option=\"auto\",\n    max_memory_per_gpu=None,\n    max_cpu_memory=None,\n    offload_folder=\"./offload\",\n    peft: str | None = None,\n    delta: str | None = None,\n    autogptq: bool | str | None = False,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>TemplateLM</code></p> <p>An abstracted whittle model class. Enables usage with both models of <code>whittle.models.gpt.GPT</code></p> Source code in <code>whittle/eval/whittle_llms.py</code> <pre><code>def __init__(\n    self,\n    pretrained: GPT,\n    backend: Literal[\"default\", \"causal\", \"seq2seq\"] | None = \"default\",\n    # override whether the model should be treated as decoder-only (causal) or encoder-decoder (seq2seq)\n    revision: str | None = \"main\",\n    subfolder=None,\n    tokenizer: str\n    | transformers.PreTrainedTokenizer\n    | transformers.PreTrainedTokenizerFast\n    | None = None,\n    truncation: bool | None = False,\n    logits_cache: bool = True,\n    max_length=None,\n    device=\"cuda\",\n    dtype=\"auto\",\n    batch_size=1,\n    max_batch_size=64,\n    trust_remote_code: bool | None = False,\n    use_fast_tokenizer: bool | None = True,\n    add_bos_token: bool | None = False,\n    prefix_token_id: int | None = None,\n    # arguments used for splitting a model across GPUs naively.\n    # only used if `parallelize=True`.\n    parallelize: bool | None = False,\n    device_map_option=\"auto\",\n    max_memory_per_gpu=None,\n    max_cpu_memory=None,\n    offload_folder=\"./offload\",\n    # PEFT, delta weights and quantization options\n    peft: str | None = None,\n    delta: str | None = None,\n    autogptq: bool | str | None = False,\n    **kwargs,\n) -&gt; None:\n    super().__init__()\n\n    # optionally: take in an already-initialized transformers.PreTrainedModel\n    eval_logger.warning(\n        \"`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\"\n    )\n    assert not parallelize, \"`parallelize=True` is not compatible with passing pre-initialized model to `pretrained`\"\n    self._model = pretrained\n    self._device = device\n    self._config = self._model.config\n\n    if tokenizer:\n        assert isinstance(\n            tokenizer, transformers.PreTrainedTokenizer\n        ) or isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n        self.tokenizer = tokenizer\n    else:\n        # Get tokenizer\n        model_name = self._model.name_or_path\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n            model_name,\n            revision=revision,\n            trust_remote_code=trust_remote_code,\n            use_fast=use_fast_tokenizer,\n        )\n\n    # determine which of 'causal' and 'seq2seq' backends to use\n    self._get_backend(\n        config=self.config, backend=backend, trust_remote_code=trust_remote_code\n    )\n\n    # load tokenizer so we know tokenizer vocabulary size before loading model and PEFT\n    self._create_tokenizer(\n        pretrained,\n        tokenizer,\n        revision=revision,\n        trust_remote_code=trust_remote_code,\n        use_fast_tokenizer=use_fast_tokenizer,\n    )\n\n    # access self._model through self.model property outside this method\n    if isinstance(self.model, torch.nn.Module):\n        self.model.eval()\n        self.model.tie_weights()\n    self.truncation = truncation\n    self.logits_cache = logits_cache\n    self.vocab_size = self.tokenizer.vocab_size\n    # select (or create) a pad token to use\n    self.tokenizer = configure_pad_token(self.tokenizer, model_config=self.config)\n\n    self.add_bos_token = add_bos_token\n    if \"gemma\" in getattr(self.config, \"model_type\", \"\"):\n        self.add_bos_token = True\n        eval_logger.info(\n            f\"Model type is '{self.config.model_type}', part of the Gemma family--a BOS token will be used as Gemma underperforms without it.\"\n        )\n\n    self._max_length = max_length\n    self.pretrained = pretrained\n    self.delta = delta\n    self.peft = peft\n    self.revision = revision\n    self.batch_schedule: float = 1\n    self.batch_sizes: dict = {}\n    self.max_batch_size = max_batch_size\n\n    if str(batch_size).startswith(\"auto\"):\n        batch_size = batch_size.split(\":\")\n        self.batch_size_per_gpu = batch_size[0]\n        self.batch_schedule = float(batch_size[1]) if len(batch_size) &gt; 1 else 1\n    else:\n        self.batch_size_per_gpu = int(batch_size)\n\n    # if a PreTrainedModel was passed into HFLM, we forgo distributed setup.\n    eval_logger.warning(\n        \"Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\"\n    )\n    self._rank = 0\n    self._world_size = 1\n\n    self.custom_prefix_token_id = prefix_token_id\n    if prefix_token_id is not None:\n        eval_logger.info(\n            f\"Loglikelihood prefix token id used in evaluation: {self.prefix_token_id}\"\n        )\n</code></pre>"},{"location":"api/whittle/eval/whittle_llms/#whittle.eval.whittle_llms.WhittleLM.apply_chat_template","title":"apply_chat_template","text":"<pre><code>apply_chat_template(\n    chat_history: list[dict[str, str]]\n) -&gt; str\n</code></pre> <p>Method to apply a chat template to a list of chat history between user and model.</p> Source code in <code>whittle/eval/whittle_llms.py</code> <pre><code>def apply_chat_template(self, chat_history: list[dict[str, str]]) -&gt; str:\n    \"\"\"\n    Method to apply a chat template to a list of chat history between user and model.\n    \"\"\"\n    return self.tokenizer.apply_chat_template(\n        chat_history, tokenize=False, add_generation_prompt=True\n    )\n</code></pre>"},{"location":"api/whittle/eval/whittle_llms/#whittle.eval.whittle_llms.WhittleLM.tok_encode","title":"tok_encode","text":"<pre><code>tok_encode(\n    string: str,\n    left_truncate_len=None,\n    add_special_tokens=None,\n) -&gt; list[int]\n</code></pre> Source code in <code>whittle/eval/whittle_llms.py</code> <pre><code>def tok_encode(\n    self, string: str, left_truncate_len=None, add_special_tokens=None\n) -&gt; list[int]:\n    \"\"\" \"\"\"\n    # default for None - empty dict, use predefined tokenizer param\n    # used for all models except for CausalLM or predefined value\n    special_tokens_kwargs = {}\n\n    # by default for CausalLM - false or self.add_bos_token is set\n    if add_special_tokens is None:\n        if self.AUTO_MODEL_CLASS == GPT:\n            special_tokens_kwargs = {\n                \"add_special_tokens\": False or self.add_bos_token\n            }\n    # otherwise the method explicitly defines the value\n    else:\n        special_tokens_kwargs = {\"add_special_tokens\": add_special_tokens}\n\n    encoding = self.tokenizer.encode(string, **special_tokens_kwargs)\n\n    # left-truncate the encoded context to be at most `left_truncate_len` tokens long\n    if left_truncate_len:\n        encoding = encoding[-left_truncate_len:]\n\n    return encoding\n</code></pre>"},{"location":"api/whittle/eval/whittle_llms/#whittle.eval.whittle_llms.configure_pad_token","title":"configure_pad_token","text":"<pre><code>configure_pad_token(\n    tokenizer: PreTrainedTokenizerBase,\n    model_config: PretrainedConfig | None = None,\n) -&gt; PreTrainedTokenizerBase\n</code></pre> <p>This function checks if the (Hugging Face) tokenizer has a padding token and sets it if not present. Some tokenizers require special handling.</p> PARAMETER DESCRIPTION <code>tokenizer</code> <p>The tokenizer for which the padding token is to be handled.</p> <p> TYPE: <code>PreTrainedTokenizerBase</code> </p> <code>model_config</code> <p>The configuration of the model. Default is None.</p> <p> TYPE: <code>PretrainedConfig | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>PreTrainedTokenizerBase</code> <p>The tokenizer after the padding token has been handled.</p> RAISES DESCRIPTION <code>AssertionError</code> <p>If the tokenizer is of type RWKVWorldTokenizer or Rwkv5Tokenizer and the padding token id is not 0.</p> Source code in <code>whittle/eval/whittle_llms.py</code> <pre><code>def configure_pad_token(\n    tokenizer: PreTrainedTokenizerBase,\n    model_config: PretrainedConfig | None = None,\n) -&gt; PreTrainedTokenizerBase:\n    \"\"\"\n    This function checks if the (Hugging Face) tokenizer has a padding token and sets it if not present.\n    Some tokenizers require special handling.\n\n    Args:\n        tokenizer: The tokenizer for which the padding token is to be handled.\n        model_config: The configuration of the model. Default is None.\n\n    Returns:\n        The tokenizer after the padding token has been handled.\n\n    Raises:\n        AssertionError: If the tokenizer is of type RWKVWorldTokenizer or Rwkv5Tokenizer and the padding token id is not 0.\n    \"\"\"\n    if tokenizer.pad_token:\n        pass\n    elif tokenizer.unk_token:\n        tokenizer.pad_token_id = tokenizer.unk_token_id\n    elif tokenizer.eos_token:\n        tokenizer.pad_token_id = tokenizer.eos_token_id\n    else:\n        # handle special cases\n        if model_config and getattr(model_config, \"model_type\", None) == \"qwen\":\n            # Qwen's trust_remote_code tokenizer does not allow for adding special tokens\n            tokenizer.pad_token = \"&lt;|endoftext|&gt;\"\n        elif (\n            tokenizer.__class__.__name__ == \"RWKVWorldTokenizer\"\n            or tokenizer.__class__.__name__ == \"Rwkv5Tokenizer\"\n        ):\n            # The RWKV world tokenizer, does not allow for adding special tokens / setting the pad token (which is set as 0)\n            # The additional tokenizer name check is needed, as there exists rwkv4 models with neox tokenizer\n            # ---\n            # Note that the world tokenizer class name, might change in the future for the final huggingface merge\n            # https://github.com/huggingface/transformers/pull/26963\n            assert tokenizer.pad_token_id == 0\n        else:\n            tokenizer.add_special_tokens({\"pad_token\": \"&lt;|pad|&gt;\"})\n\n    return tokenizer\n</code></pre>"},{"location":"api/whittle/loss/kd_loss/","title":"Kd loss","text":""},{"location":"api/whittle/loss/kd_loss/#whittle.loss.kd_loss","title":"whittle.loss.kd_loss","text":""},{"location":"api/whittle/loss/kd_loss/#whittle.loss.kd_loss.DistillLoss","title":"DistillLoss","text":"<pre><code>DistillLoss(temperature, distillation_weight)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Custom loss function for knowledge distillation</p> <p>This loss function combines the standard cross-entropy loss with the KL divergence between the soft targets produced by a teacher model and a student model.</p> ATTRIBUTE DESCRIPTION <code>temperature</code> <p>The temperature used for distillation. Higher temperatures                  produce softer probability distributions.</p> <p> TYPE: <code>float</code> </p> <code>distillation_weight</code> <p>The weight factor that balances the importance of                          the distillation loss and the hard target loss.</p> <p> TYPE: <code>float</code> </p> <code>kldiv</code> <p>The KL divergence loss function.</p> <p> TYPE: <code>KLDivLoss</code> </p> PARAMETER DESCRIPTION <code>temperature</code> <p>The temperature for distillation.</p> <p> TYPE: <code>float</code> </p> <code>distillation_weight</code> <p>The weight factor for the distillation loss.</p> <p> TYPE: <code>float</code> </p> Source code in <code>whittle/loss/kd_loss.py</code> <pre><code>def __init__(self, temperature, distillation_weight):\n    \"\"\"\n    Initializes the DistillLoss module.\n\n    Args:\n        temperature (float): The temperature for distillation.\n        distillation_weight (float): The weight factor for the distillation loss.\n    \"\"\"\n    super().__init__()\n\n    self.temperature = temperature\n    self.distillation_weight = distillation_weight\n    self.kldiv = nn.KLDivLoss(reduction=\"batchmean\")\n</code></pre>"},{"location":"api/whittle/loss/kd_loss/#whittle.loss.kd_loss.DistillLoss.forward","title":"forward","text":"<pre><code>forward(outputs, labels, outputs_teacher) -&gt; Tensor\n</code></pre> <p>Compute the distillation loss.</p> <p>This method computes the loss as a weighted sum of the soft target loss (KL divergence between student and teacher outputs) and the hard target loss (cross-entropy loss between student outputs and ground truth labels).</p> PARAMETER DESCRIPTION <code>outputs</code> <p>The logits output by the student model. Shape (batch_size, num_classes).</p> <p> TYPE: <code>Tensor</code> </p> <code>labels</code> <p>The ground truth labels. Shape (batch_size).</p> <p> TYPE: <code>Tensor</code> </p> <code>outputs_teacher</code> <p>The logits output by the teacher model. Shape (batch_size, num_classes).</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>torch.Tensor: The combined loss.</p> Source code in <code>whittle/loss/kd_loss.py</code> <pre><code>def forward(self, outputs, labels, outputs_teacher) -&gt; nn.Tensor:\n    \"\"\"\n    Compute the distillation loss.\n\n    This method computes the loss as a weighted sum of the soft target loss (KL divergence\n    between student and teacher outputs) and the hard target loss (cross-entropy loss\n    between student outputs and ground truth labels).\n\n    Args:\n        outputs (torch.Tensor): The logits output by the student model. Shape (batch_size, num_classes).\n        labels (torch.Tensor): The ground truth labels. Shape (batch_size).\n        outputs_teacher (torch.Tensor): The logits output by the teacher model. Shape (batch_size, num_classes).\n\n    Returns:\n        torch.Tensor: The combined loss.\n    \"\"\"\n    soft_target_loss = 0\n    outputs_teacher = outputs_teacher.detach()\n    if outputs_teacher is not None and self.distillation_weight &gt; 0:\n        soft_target_loss = self.kldiv(\n            F.log_softmax(outputs / self.temperature, dim=1),\n            F.softmax(outputs_teacher / self.temperature, dim=1),\n        ) * (self.temperature**2)\n\n    hard_target_loss = F.cross_entropy(outputs, labels, reduction=\"mean\")\n\n    total_loss = soft_target_loss * self.distillation_weight + hard_target_loss * (\n        1 - self.distillation_weight\n    )\n\n    return total_loss\n</code></pre>"},{"location":"api/whittle/metrics/flops/","title":"Flops","text":""},{"location":"api/whittle/metrics/flops/#whittle.metrics.flops","title":"whittle.metrics.flops","text":""},{"location":"api/whittle/metrics/flops/#whittle.metrics.flops.compute_flops","title":"compute_flops","text":"<pre><code>compute_flops(\n    model: GPT,\n    batch_size: int = 1,\n    sequence_length: int = 512,\n    metric: Literal[\"flops\", \"macs\"] = \"flops\",\n) -&gt; float\n</code></pre> <p>Estimates the number of floating-point operations (FLOPs) or multiply-accumulate operations (MACs) for a GPT model.</p> <p>This function uses DeepSpeed's FlopsProfiler to estimate the FLOPs or MACs of the model's forward pass.</p> PARAMETER DESCRIPTION <code>model</code> <p>The GPT model to profile.</p> <p> TYPE: <code>GPT</code> </p> <code>batch_size</code> <p>The batch size for the input tensor. Defaults to 1.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>sequence_length</code> <p>The sequence length for the input tensor. Defaults to 512.</p> <p> TYPE: <code>int</code> DEFAULT: <code>512</code> </p> <code>metric</code> <p>The metric to return. Either \"flops\" for floating-point operations or \"macs\" for multiply-accumulate operations. Defaults to \"flops\".</p> <p> TYPE: <code>Literal['flops', 'macs']</code> DEFAULT: <code>'flops'</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The estimated number of floating-point operations (FLOPs) or multiply-accumulate operations (MACs) for the model's forward pass, depending on the specified metric.</p> Source code in <code>whittle/metrics/flops.py</code> <pre><code>def compute_flops(\n    model: GPT,\n    batch_size: int = 1,\n    sequence_length: int = 512,\n    metric: Literal[\"flops\", \"macs\"] = \"flops\",\n) -&gt; float:\n    \"\"\"\n    Estimates the number of floating-point operations (FLOPs) or multiply-accumulate operations (MACs) for a GPT model.\n\n    This function uses DeepSpeed's FlopsProfiler to estimate the FLOPs or MACs of the model's forward pass.\n\n    Args:\n        model: The GPT model to profile.\n        batch_size: The batch size for the input tensor. Defaults to 1.\n        sequence_length: The sequence length for the input tensor. Defaults to 512.\n        metric: The metric to return. Either \"flops\" for floating-point operations or \"macs\" for multiply-accumulate operations. Defaults to \"flops\".\n\n    Returns:\n        The estimated number of floating-point operations (FLOPs) or multiply-accumulate operations (MACs) for the model's forward pass, depending on the specified metric.\n    \"\"\"\n\n    input_tensor = torch.randint(\n        0, model.config.padded_vocab_size, (batch_size, sequence_length)\n    )\n\n    model.eval()\n\n    os.environ[\"DS_ACCELERATOR\"] = \"CPU\"\n    deepspeed.accelerator.set_accelerator(CPU_Accelerator())\n\n    flops, macs, _ = get_model_profile(\n        model=model,\n        args=(input_tensor,),\n        print_profile=False,\n        detailed=False,\n        warm_up=1,\n        as_string=False,\n    )\n\n    if metric == \"flops\":\n        return flops\n    else:\n        return macs\n</code></pre>"},{"location":"api/whittle/metrics/latency/","title":"Latency","text":""},{"location":"api/whittle/metrics/latency/#whittle.metrics.latency","title":"whittle.metrics.latency","text":""},{"location":"api/whittle/metrics/latency/#whittle.metrics.latency.compute_latency","title":"compute_latency","text":"<pre><code>compute_latency(\n    model: Module,\n    use_cuda: bool = False,\n    batch_size: int = 8,\n    n_samples: int = 10,\n) -&gt; float\n</code></pre> <p>Profiles the latency of a PyTorch model for inference.</p> <p>This function measures the average latency of the model's forward pass over a specified number of samples using PyTorch's profiler. It supports both CPU and CUDA profiling.</p> PARAMETER DESCRIPTION <code>model</code> <p>the LitGPT profiled.</p> <p> TYPE: <code>Module</code> </p> <code>use_cuda</code> <p>If True and CUDA is available, the model will be moved to the GPU for profiling. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>batch_size</code> <p>The batch size for the input tensor. Defaults to 8.</p> <p> TYPE: <code>int</code> DEFAULT: <code>8</code> </p> <code>n_samples</code> <p>The number of samples to profile after the warm-up phase. Defaults to 10.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The average inference time per sample in milliseconds.</p> <p> TYPE: <code>float</code> </p> Source code in <code>whittle/metrics/latency.py</code> <pre><code>def compute_latency(\n    model: torch.nn.Module,\n    use_cuda: bool = False,\n    batch_size: int = 8,\n    n_samples: int = 10,\n) -&gt; float:\n    \"\"\"\n    Profiles the latency of a PyTorch model for inference.\n\n    This function measures the average latency of the model's forward pass over a specified number of samples\n    using PyTorch's profiler. It supports both CPU and CUDA profiling.\n\n    Args:\n        model (torch.nn.Module): the LitGPT profiled.\n        use_cuda (bool, optional): If True and CUDA is available, the model will be moved to the GPU for profiling. Defaults to False.\n        batch_size (int, optional): The batch size for the input tensor. Defaults to 8.\n        n_samples (int, optional): The number of samples to profile after the warm-up phase. Defaults to 10.\n\n    Returns:\n        float: The average inference time per sample in milliseconds.\n    \"\"\"\n    input_tensor = torch.randint(\n        0, model.config.padded_vocab_size, (batch_size, model.max_seq_length)\n    )\n    if use_cuda and torch.cuda.is_available():\n        model = model.cuda()\n        input_tensor = input_tensor.cuda()\n\n    # Use PyTorch profiler to record compute_latency\n    model.eval()\n    with torch.profiler.profile(\n        activities=[\n            torch.profiler.ProfilerActivity.CPU,\n            torch.profiler.ProfilerActivity.CUDA,\n        ],\n        schedule=torch.profiler.schedule(wait=1, warmup=10, active=n_samples),\n    ) as profiler:\n        # Actual forward pass inside the profiler\n        with record_function(\"model_inference\"):\n            for i in range(11 + n_samples):\n                _ = model(input_tensor)\n                profiler.step()\n\n    # Summing up CPU and CUDA times from the profiler using the correct methods\n    cuda_time_us, cpu_time_us = get_total_cpu_gpu_runtime(profiler)\n\n    # Convert time to milliseconds\n    total_time_ms = (cpu_time_us + cuda_time_us) / 1000\n    model = model.cpu()\n    return total_time_ms / n_samples\n</code></pre>"},{"location":"api/whittle/metrics/latency/#whittle.metrics.latency.get_total_cpu_gpu_runtime","title":"get_total_cpu_gpu_runtime","text":"<pre><code>get_total_cpu_gpu_runtime(prof: profile) -&gt; tuple[int, int]\n</code></pre> <p>Calculates the total runtime for CPU and GPU (CUDA) from the profiler events.</p> <p>This function extracts and sums the self CPU and CUDA times from the PyTorch profiler events. It handles both legacy and Kineto profiler events for accurate CPU and CUDA profiling.</p> PARAMETER DESCRIPTION <code>prof</code> <p>A PyTorch profiler object containing profiling events.</p> <p> TYPE: <code>profile</code> </p> RETURNS DESCRIPTION <code>tuple[int, int]</code> <p>tuple[int, int]: A tuple where the first value is the total CPU time (in microseconds),                  and the second value is the total CUDA time (in microseconds).</p> Source code in <code>whittle/metrics/latency.py</code> <pre><code>def get_total_cpu_gpu_runtime(prof: torch.profiler.profile) -&gt; tuple[int, int]:\n    \"\"\"\n    Calculates the total runtime for CPU and GPU (CUDA) from the profiler events.\n\n    This function extracts and sums the self CPU and CUDA times from the PyTorch profiler events.\n    It handles both legacy and Kineto profiler events for accurate CPU and CUDA profiling.\n\n    Args:\n        prof (torch.profiler.profile): A PyTorch profiler object containing profiling events.\n\n    Returns:\n        tuple[int, int]: A tuple where the first value is the total CPU time (in microseconds),\n                         and the second value is the total CUDA time (in microseconds).\n    \"\"\"\n    events = prof.events()\n    sum_self_cpu_time_total = sum([event.self_cpu_time_total for event in events])\n\n    sum_self_cuda_time_total = 0\n    for evt in events:\n        if evt.device_type == torch.device(\"cpu\").type:\n            # In legacy profiler, kernel info is stored in CPU events\n            if evt.is_legacy:\n                sum_self_cuda_time_total += evt.self_cuda_time_total\n        elif evt.device_type == torch.device(\"cuda\").type:\n            # In Kineto profiler, there are events with the correct device type (e.g., CUDA)\n            sum_self_cuda_time_total += evt.self_cuda_time_total\n\n    return sum_self_cpu_time_total, sum_self_cuda_time_total\n</code></pre>"},{"location":"api/whittle/metrics/mag/","title":"Mag","text":""},{"location":"api/whittle/metrics/mag/#whittle.metrics.mag","title":"whittle.metrics.mag","text":""},{"location":"api/whittle/metrics/mag/#whittle.metrics.mag.compute_weight_magnitude","title":"compute_weight_magnitude","text":"<pre><code>compute_weight_magnitude(model: GPT) -&gt; float\n</code></pre> <p>Computes the sum of the weight magnitudes of the current sub-network of a GPT model. Make sure to set the sub-network before calling this function.</p> PARAMETER DESCRIPTION <code>model</code> <p>GPT model</p> <p> TYPE: <code>GPT</code> </p> RETURNS DESCRIPTION <code>float</code> <p>magnitude of the weights of the activated sub-network</p> <p> TYPE: <code>float</code> </p> Source code in <code>whittle/metrics/mag.py</code> <pre><code>def compute_weight_magnitude(model: GPT) -&gt; float:\n    \"\"\"\n    Computes the sum of the weight magnitudes of the current sub-network of a GPT model. Make sure to set the\n    sub-network before calling this function.\n\n    Args:\n        model: GPT model\n\n    Returns:\n        float: magnitude of the weights of the activated sub-network\n    \"\"\"\n    magnitude = 0\n    magnitude += compute_weight_magnitude_linear_layer(model.lm_head)\n    magnitude += compute_weight_magnitude_embedding(model.transformer.wte)\n    for i in range(model.sub_network_n_layers):\n        block = model.transformer.h[i]\n        magnitude += compute_weight_magnitude_layer_norm(block.norm_1)\n        magnitude += compute_weight_magnitude_attention(block.attn)\n        magnitude += compute_weight_magnitude_mlp(block.mlp)\n        magnitude += compute_weight_magnitude_layer_norm(block.norm_2)\n    magnitude += compute_weight_magnitude_layer_norm(model.transformer.ln_f)\n    return magnitude\n</code></pre>"},{"location":"api/whittle/metrics/parameters/","title":"Parameters","text":""},{"location":"api/whittle/metrics/parameters/#whittle.metrics.parameters","title":"whittle.metrics.parameters","text":""},{"location":"api/whittle/metrics/parameters/#whittle.metrics.parameters.compute_parameters","title":"compute_parameters","text":"<pre><code>compute_parameters(model: GPT) -&gt; float\n</code></pre> <p>Computes parameters of the current sub-network of a GPT mmodel. Make sure to set the sub-network before calling this function.</p> Refs <p>towardsdatascience.com/how-to-estimate-the-number-of-parameters-in-transformer-models-ca0f57d8dff0</p> PARAMETER DESCRIPTION <code>model</code> <p>GPT model</p> <p> TYPE: <code>GPT</code> </p> RETURNS DESCRIPTION <code>float</code> <p>number of parameters of the activated sub-network</p> <p> TYPE: <code>float</code> </p> Source code in <code>whittle/metrics/parameters.py</code> <pre><code>def compute_parameters(model: GPT) -&gt; float:\n    \"\"\"\n    Computes parameters of the current sub-network of a GPT mmodel. Make sure to set the sub-network before\n    calling this function.\n\n    Refs:\n        https://towardsdatascience.com/how-to-estimate-the-number-of-parameters-in-transformer-models-ca0f57d8dff0\n\n    Args:\n        model: GPT model\n\n    Returns:\n        float: number of parameters of the activated sub-network\n    \"\"\"\n\n    num_params = 0\n    num_params += params_linear_layer(model.lm_head)\n    num_params += params_embedding_layer(model.transformer.wte)\n    for i in range(model.sub_network_n_layers):\n        block = model.transformer.h[i]\n        num_params += params_mlp(block.mlp)\n        num_params += params_attention_layer(block.attn)\n\n        num_params += params_layer_normalization(block.norm_1)\n        num_params += params_layer_normalization(block.norm_2)\n    num_params += params_layer_normalization(model.transformer.ln_f)\n    return num_params\n</code></pre>"},{"location":"api/whittle/models/gpt/extract/","title":"Extract","text":""},{"location":"api/whittle/models/gpt/extract/#whittle.models.gpt.extract","title":"whittle.models.gpt.extract","text":""},{"location":"api/whittle/models/gpt/extract/#whittle.models.gpt.extract.extract_sub_network","title":"extract_sub_network","text":"<pre><code>extract_sub_network(\n    model: GPT, sub_network_config: Config\n) -&gt; GPT\n</code></pre> <p>Extracts a sub-network from a given model based on the specified sub-network configuration. Copies relevant layers, weights, and configurations from the full model into a sub-network model.</p> PARAMETER DESCRIPTION <code>model</code> <p>The original, full GPT model from which the sub-network is extracted.</p> <p> TYPE: <code>GPT</code> </p> <code>sub_network_config</code> <p>Configuration object for the sub-network, containing the necessary architecture specifications such as embedding size, number of heads, and number of layers.</p> <p> TYPE: <code>Config</code> </p> RETURNS DESCRIPTION <code>GPT</code> <p>A new sub-network model instance, initialized with parameters extracted from the original model.</p> Source code in <code>whittle/models/gpt/extract.py</code> <pre><code>def extract_sub_network(model: GPT, sub_network_config: Config) -&gt; GPT:\n    \"\"\"\n    Extracts a sub-network from a given model based on the specified sub-network configuration.\n    Copies relevant layers, weights, and configurations from the full model into a sub-network model.\n\n    Args:\n        model: The original, full GPT model from which the sub-network is extracted.\n        sub_network_config: Configuration object for the sub-network, containing the necessary\n            architecture specifications such as embedding size, number of heads, and number of layers.\n\n    Returns:\n        A new sub-network model instance, initialized with parameters extracted from the original model.\n    \"\"\"\n\n    sub_network = GPT(sub_network_config)\n\n    state_dict = extract_linear(model.lm_head)\n    sub_network.lm_head.load_state_dict(state_dict)\n\n    state_dict = extract_embedding(model.transformer.wte)\n    sub_network.transformer.wte.load_state_dict(state_dict)\n\n    extract_norm(model.transformer.ln_f, sub_network.transformer.ln_f)\n\n    for i in range(sub_network_config.n_layer):\n        block = model.transformer.h[i]\n        sub_network_block = sub_network.transformer.h[i]\n\n        # Attention\n        state_dict = extract_linear(block.attn.attn)\n        sub_network_block.attn.attn.load_state_dict(state_dict)\n        state_dict = extract_linear(block.attn.proj)\n        sub_network_block.attn.proj.load_state_dict(state_dict)\n\n        # MLP\n        extract_mlp(block.mlp, sub_network_block.mlp)\n\n        # norm\n        extract_norm(block.norm_1, sub_network_block.norm_1)\n        extract_norm(block.post_attention_norm, sub_network_block.post_attention_norm)\n        extract_norm(block.norm_2, sub_network_block.norm_2)\n        extract_norm(block.post_mlp_norm, sub_network_block.post_mlp_norm)\n\n    return sub_network\n</code></pre>"},{"location":"api/whittle/models/gpt/model/","title":"Model","text":""},{"location":"api/whittle/models/gpt/model/#whittle.models.gpt.model","title":"whittle.models.gpt.model","text":"<p>Full definition of a decoder-only transformer-based language model, all of it in this single file.</p> <p>Based on the nanoGPT implementation: karpathy/nanoGPT and github.com/EleutherAI/gpt-neox/tree/main/megatron/model.</p>"},{"location":"api/whittle/models/gpt/model/#whittle.models.gpt.model.GPT","title":"GPT","text":"<pre><code>GPT(config: Config)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>An extension of litgpt's GPT model with support to adapt to sub-network dimensionality.</p> Source code in <code>whittle/models/gpt/model.py</code> <pre><code>def __init__(self, config: Config) -&gt; None:\n    super().__init__()\n    assert config.padded_vocab_size is not None\n    self.config = config\n    self.lm_head = Linear(\n        config.n_embd, config.padded_vocab_size, bias=config.lm_head_bias\n    )\n    self.transformer = nn.ModuleDict(\n        dict(\n            wte=Embedding(config.padded_vocab_size, config.n_embd),\n            h=nn.ModuleList(\n                Block(config, block_idx) for block_idx in range(config.n_layer)\n            ),\n            ln_f=self.norm_class(config.n_embd, eps=config.norm_eps),\n        )\n    )\n    self.max_layer = config.n_layer\n    self.max_seq_length = self.config.block_size\n    self.mask_cache: torch.Tensor | None = None\n\n    # Set current sub-network to super-network\n    self.sub_network_n_embd = self.config.n_embd\n    self.sub_network_intermediate_size = self.config.intermediate_size\n    self.sub_network_num_heads = self.config.n_head\n    self.sub_network_n_layers = self.config.n_layer\n    self.sub_network_head_size: int | None = self.config.head_size\n    self.sub_network_query_groups: int | None = self.config.n_query_groups\n    self.sub_network_rope_n_elem = self.config.rope_n_elem\n    self.cos: torch.Tensor\n    self.sin: torch.Tensor\n    self.config.is_encoder_decoder = False\n    self.main_input_name = \"input_pos\"\n    self._supports_cache_class = True\n</code></pre>"},{"location":"api/whittle/models/gpt/model/#whittle.models.gpt.model.GPT.reset_super_network","title":"reset_super_network","text":"<pre><code>reset_super_network()\n</code></pre> <p>Resets the GPT model to the original super-network dimensionality.</p> Source code in <code>whittle/models/gpt/model.py</code> <pre><code>def reset_super_network(self):\n    \"\"\"\n    Resets the GPT model to the original super-network dimensionality.\n    \"\"\"\n    self.sub_network_n_embd = self.config.n_embd\n    self.sub_network_intermediate_size = self.config.intermediate_size\n    self.sub_network_num_heads = self.config.n_head\n    self.sub_network_n_layers = self.config.n_layer\n    self.sub_network_head_size: int | None = self.config.head_size\n    self.sub_network_query_groups: int | None = self.config.n_query_groups\n    self.sub_network_rope_n_elem = self.config.rope_n_elem\n    self.transformer.wte.reset_super_network()\n    self.transformer.ln_f.reset_super_network()\n    for i in range(self.config.n_layer):\n        block = self.transformer.h[i]\n        block.reset_super_network()\n    self.lm_head.reset_super_network()\n</code></pre>"},{"location":"api/whittle/models/gpt/model/#whittle.models.gpt.model.GPT.select_sub_network","title":"select_sub_network","text":"<pre><code>select_sub_network(config: dict[str, Any]) -&gt; None\n</code></pre> <p>Selects and sets the sub-network configuration based on the provided configuration.</p> Source code in <code>whittle/models/gpt/model.py</code> <pre><code>def select_sub_network(self, config: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Selects and sets the sub-network configuration based on the provided configuration.\n    \"\"\"\n    self.set_sub_network(\n        config[\"embed_dim\"],\n        config[\"mlp_ratio\"] * config[\"embed_dim\"],\n        config[\"num_heads\"],\n        config[\"depth\"],\n    )\n</code></pre>"},{"location":"api/whittle/models/gpt/model/#whittle.models.gpt.model.GPT.set_sub_network","title":"set_sub_network","text":"<pre><code>set_sub_network(\n    sub_network_n_embd: int,\n    sub_network_intermediate_size: int,\n    sub_network_num_heads: int,\n    sub_network_n_layers: int,\n    sub_network_query_groups: int | None = None,\n    sub_network_head_size: int | None = None,\n) -&gt; None\n</code></pre> <p>Sets the GPT model to the specified sub-network dimensionality. Input arguments are set to the specified sub-network dimensionality.</p> PARAMETER DESCRIPTION <code>sub_network_n_embd</code> <p>Embedding dimension of the sub-network.</p> <p> TYPE: <code>int</code> </p> <code>sub_network_intermediate_size</code> <p>Intermediate size of the sub-network.</p> <p> TYPE: <code>int</code> </p> <code>sub_network_num_heads</code> <p>Number of attention heads in the sub-network.</p> <p> TYPE: <code>int</code> </p> <code>sub_network_n_layers</code> <p>Number of layers in the sub-network.</p> <p> TYPE: <code>int</code> </p> <code>sub_network_query_groups</code> <p>Number of query groups in the sub-network. Defaults to None.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>sub_network_head_size</code> <p>Size of each attention head in the sub-network. Defaults to None.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>whittle/models/gpt/model.py</code> <pre><code>def set_sub_network(\n    self,\n    sub_network_n_embd: int,\n    sub_network_intermediate_size: int,\n    sub_network_num_heads: int,\n    sub_network_n_layers: int,\n    sub_network_query_groups: int | None = None,\n    sub_network_head_size: int | None = None,\n) -&gt; None:\n    \"\"\"\n    Sets the GPT model to the specified sub-network dimensionality.\n    Input arguments are set to the specified sub-network dimensionality.\n\n    Args:\n        sub_network_n_embd: Embedding dimension of the sub-network.\n        sub_network_intermediate_size: Intermediate size of the sub-network.\n        sub_network_num_heads: Number of attention heads in the sub-network.\n        sub_network_n_layers: Number of layers in the sub-network.\n        sub_network_query_groups: Number of query groups in the sub-network. Defaults to None.\n        sub_network_head_size: Size of each attention head in the sub-network. Defaults to None.\n    \"\"\"\n    self.sub_network_n_embd = sub_network_n_embd\n    self.sub_network_intermediate_size = sub_network_intermediate_size\n    self.sub_network_num_heads = sub_network_num_heads\n    self.sub_network_n_layers = sub_network_n_layers\n    self.transformer.wte.set_sub_network(self.sub_network_n_embd)\n    self.transformer.ln_f.set_sub_network(self.sub_network_n_embd)\n    if sub_network_query_groups is None:\n        if self.config.n_query_groups == 1:\n            self.sub_network_query_groups = 1\n        elif self.sub_network_num_heads % self.config.n_query_groups == 0:\n            self.sub_network_query_groups = self.config.n_query_groups\n        else:\n            self.sub_network_query_groups = self.sub_network_num_heads // (\n                self.config.n_head // self.config.n_query_groups\n            )\n    else:\n        self.sub_network_query_groups = sub_network_query_groups\n    if self.config.fix_head_size:\n        if sub_network_head_size is None:\n            self.sub_network_head_size = self.config.head_size\n        else:\n            self.sub_network_head_size = sub_network_head_size\n    else:\n        self.sub_network_head_size = (\n            self.sub_network_n_embd // self.sub_network_num_heads\n        )\n    for i in range(self.sub_network_n_layers):\n        block = self.transformer.h[i]\n        block.set_sub_network(\n            self.sub_network_n_embd,\n            self.sub_network_intermediate_size,\n            self.sub_network_num_heads,\n            self.sub_network_query_groups,\n            self.sub_network_head_size,\n        )\n    self.lm_head.set_sub_network(\n        self.sub_network_n_embd, self.config.padded_vocab_size\n    )\n\n    # change the rope cache to match n_elem induced by subnet head size\n    self.sub_network_rope_n_elem = int(\n        self.config.rotary_percentage * self.sub_network_head_size\n    )\n    self.cos, self.sin = self.rope_cache(\n        seq_len=self._max_seq_length,\n        n_elem=self.sub_network_rope_n_elem,\n        device=self.cos.device,\n    )\n</code></pre>"},{"location":"api/whittle/models/gpt/utils/","title":"Utils","text":""},{"location":"api/whittle/models/gpt/utils/#whittle.models.gpt.utils","title":"whittle.models.gpt.utils","text":"<p>Utility functions for training and inference.</p>"},{"location":"api/whittle/models/gpt/utils/#whittle.models.gpt.utils.CycleIterator","title":"CycleIterator","text":"<pre><code>CycleIterator(iterable: Iterable)\n</code></pre> <p>An iterator that cycles through an iterable indefinitely.</p> Example <p>iterator = CycleIterator([1, 2, 3]) [next(iterator) for _ in range(5)]</p> <p>[1, 2, 3, 1, 2]</p> Note <p>Unlike <code>itertools.cycle</code>, this iterator does not cache the values of the iterable.</p> Source code in <code>whittle/models/gpt/utils.py</code> <pre><code>def __init__(self, iterable: Iterable) -&gt; None:\n    self.iterable = iterable\n    self.epoch = 0\n    self._iterator: Iterator | None = None\n</code></pre>"},{"location":"api/whittle/models/gpt/utils/#whittle.models.gpt.utils.estimate_flops","title":"estimate_flops","text":"<pre><code>estimate_flops(model: GPT, training: bool) -&gt; int\n</code></pre> <p>Measures estimated FLOPs for MFU.</p> Refs <ul> <li>ar5iv.labs.arxiv.org/html/2205.05198#A1</li> <li>ar5iv.labs.arxiv.org/html/2204.02311#A2</li> </ul> Source code in <code>whittle/models/gpt/utils.py</code> <pre><code>def estimate_flops(model: GPT, training: bool) -&gt; int:\n    \"\"\"Measures estimated FLOPs for MFU.\n\n    Refs:\n        * https://ar5iv.labs.arxiv.org/html/2205.05198#A1\n        * https://ar5iv.labs.arxiv.org/html/2204.02311#A2\n    \"\"\"\n    # using all parameters for this is a naive over estimation because not all model parameters actually contribute to\n    # this FLOP computation (e.g. embedding, norm). For this reason, the result will be higher by a fixed percentage\n    # (~10%) compared to the measured FLOPs, making those lower but more realistic.\n    # For a proper estimate, this needs a more fine-grained calculation as in Appendix A of the paper.\n    n_trainable_params = num_parameters(model, requires_grad=True)\n    trainable_flops = flops_per_param(\n        model.max_seq_length,\n        model.config.n_layer,\n        model.config.n_embd,\n        n_trainable_params,\n    )\n    # forward + backward + gradients (assumes no gradient accumulation)\n    ops_per_step = 3 if training else 1\n    n_frozen_params = num_parameters(model, requires_grad=False)\n    frozen_flops = flops_per_param(\n        model.max_seq_length, model.config.n_layer, model.config.n_embd, n_frozen_params\n    )\n    # forward + backward\n    frozen_ops_per_step = 2 if training else 1\n    return ops_per_step * trainable_flops + frozen_ops_per_step * frozen_flops\n</code></pre>"},{"location":"api/whittle/models/gpt/utils/#whittle.models.gpt.utils.get_default_supported_precision","title":"get_default_supported_precision","text":"<pre><code>get_default_supported_precision(training: bool) -&gt; str\n</code></pre> <p>Return default precision that is supported by the hardware: either <code>bf16</code> or <code>16</code>.</p> PARAMETER DESCRIPTION <code>training</code> <p><code>-mixed</code> or <code>-true</code> version of the precision to use</p> <p> TYPE: <code>bool</code> </p> RETURNS DESCRIPTION <code>str</code> <p>default precision that is suitable for the task and is supported by the hardware</p> Source code in <code>whittle/models/gpt/utils.py</code> <pre><code>def get_default_supported_precision(training: bool) -&gt; str:\n    \"\"\"\n    Return default precision that is supported by the hardware: either `bf16` or `16`.\n\n    Args:\n        training: `-mixed` or `-true` version of the precision to use\n\n    Returns:\n        default precision that is suitable for the task and is supported by the hardware\n    \"\"\"\n    from lightning.fabric.accelerators import MPSAccelerator\n\n    if MPSAccelerator.is_available() or (\n        torch.cuda.is_available() and not torch.cuda.is_bf16_supported()\n    ):\n        return \"16-mixed\" if training else \"16-true\"\n    return \"bf16-mixed\" if training else \"bf16-true\"\n</code></pre>"},{"location":"api/whittle/models/gpt/utils/#whittle.models.gpt.utils.sample_config","title":"sample_config","text":"<pre><code>sample_config(\n    choices_dict: dict,\n    layer_sampling_scheme: str = \"normal\",\n    seed: int = 0,\n) -&gt; dict\n</code></pre> <p>Sample a configuration from a dictionary of choices.</p> PARAMETER DESCRIPTION <code>choices_dict</code> <p>a dictionary of choices for each architectural dimension</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>a dictionary with the same keys as <code>choices_dict</code> and the sampled choices</p> Source code in <code>whittle/models/gpt/utils.py</code> <pre><code>def sample_config(\n    choices_dict: dict, layer_sampling_scheme: str = \"normal\", seed: int = 0\n) -&gt; dict:\n    \"\"\"Sample a configuration from a dictionary of choices.\n\n    Args:\n        choices_dict: a dictionary of choices for each architectural dimension\n\n    Returns:\n        a dictionary with the same keys as `choices_dict` and the sampled choices\n    \"\"\"\n    sampled_dict = {}\n    r = random.Random(seed)\n    # sample embed dim -&gt; held constant throughout transformer\n    sampled_dict[\"sample_embed_dim\"] = r.choice(choices_dict[\"embed_dim_choices\"])\n    # sample number of layers\n    sampled_dict[\"sample_n_layer\"] = r.choice(choices_dict[\"n_layer_choices\"])\n    # sample layer indices\n    max_layer = max(choices_dict[\"n_layer_choices\"])\n    sampled_dict[\"sample_layer_indices\"] = []\n    if layer_sampling_scheme == \"normal\":\n        sampled_dict[\"sample_layer_indices\"] = list(\n            range(sampled_dict[\"sample_n_layer\"])\n        )\n    elif layer_sampling_scheme == \"strided\":\n        if sampled_dict[\"sample_n_layer\"] == max_layer:\n            sampled_dict[\"sample_layer_indices\"] = list(\n                range(sampled_dict[\"sample_n_layer\"])\n            )\n        elif sampled_dict[\"sample_n_layer\"] == max_layer - 1:\n            sampled_dict[\"sample_layer_indices\"] = list(\n                range(sampled_dict[\"sample_n_layer\"])\n            )\n            sampled_dict[\"sample_layer_indices\"][-1] = max_layer - 1\n        else:\n            increment_floor = max_layer // sampled_dict[\"sample_n_layer\"]\n            increment_ceil = int(np.ceil(max_layer / sampled_dict[\"sample_n_layer\"]))\n            sampled_dict[\"sample_layer_indices\"] = [\n                0 for _ in range(sampled_dict[\"sample_n_layer\"])\n            ]\n            counter_layer = 0\n            for i in range(sampled_dict[\"sample_n_layer\"]):\n                if counter_layer &lt; (max_layer // 2):\n                    sampled_dict[\"sample_layer_indices\"][i] = counter_layer\n                    counter_layer += increment_ceil\n                else:\n                    sampled_dict[\"sample_layer_indices\"][i] = counter_layer\n                    counter_layer += increment_floor\n\n            sampled_dict[\"sample_layer_indices\"][0] = 0\n            sampled_dict[\"sample_layer_indices\"][-1] = max_layer - 1\n\n    # sample number of heads\n    sampled_dict[\"sample_n_head\"] = [\n        r.choice(choices_dict[\"n_head_choices\"]) for _ in range(max_layer)\n    ]\n    # sample mlp ratio\n    sampled_dict[\"sample_mlp_ratio\"] = [\n        r.choice(choices_dict[\"mlp_ratio_choices\"]) for _ in range(max_layer)\n    ]\n    # sample bias\n    sampled_dict[\"sample_bias\"] = r.choice(choices_dict[\"bias_choices\"])\n\n    return sampled_dict\n</code></pre>"},{"location":"api/whittle/models/gpt/utils/#whittle.models.gpt.utils.sample_config_max","title":"sample_config_max","text":"<pre><code>sample_config_max(\n    choices_dict: dict,\n    layer_sampling_scheme: str = \"normal\",\n) -&gt; dict\n</code></pre> <p>Sample a configuration from a dictionary of choices.</p> PARAMETER DESCRIPTION <code>choices_dict</code> <p>a dictionary of choices for each architectural dimension</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>a dictionary with the same keys as <code>choices_dict</code> and the sampled choices</p> Source code in <code>whittle/models/gpt/utils.py</code> <pre><code>def sample_config_max(\n    choices_dict: dict, layer_sampling_scheme: str = \"normal\"\n) -&gt; dict:\n    \"\"\"Sample a configuration from a dictionary of choices.\n\n    Args:\n        choices_dict: a dictionary of choices for each architectural dimension\n\n    Returns:\n        a dictionary with the same keys as `choices_dict` and the sampled choices\n    \"\"\"\n    sampled_dict = {}\n    # sample embed dim -&gt; held constant throughout transformer\n    sampled_dict[\"sample_embed_dim\"] = max(choices_dict[\"embed_dim_choices\"])\n    # sample number of layers\n    sampled_dict[\"sample_n_layer\"] = max(choices_dict[\"n_layer_choices\"])\n    # sample layer indices\n    max_layer = max(choices_dict[\"n_layer_choices\"])\n    sampled_dict[\"sample_layer_indices\"] = []\n    sampled_dict[\"sample_layer_indices\"] = list(range(sampled_dict[\"sample_n_layer\"]))\n\n    # sample number of heads\n    sampled_dict[\"sample_n_head\"] = [\n        max(choices_dict[\"n_head_choices\"]) for _ in range(max_layer)\n    ]\n    # sample mlp ratio\n    sampled_dict[\"sample_mlp_ratio\"] = [\n        max(choices_dict[\"mlp_ratio_choices\"]) for _ in range(max_layer)\n    ]\n    # sample bias\n    sampled_dict[\"sample_bias\"] = True\n\n    return sampled_dict\n</code></pre>"},{"location":"api/whittle/models/gpt/utils/#whittle.models.gpt.utils.sample_config_mid","title":"sample_config_mid","text":"<pre><code>sample_config_mid(\n    choices_dict: dict,\n    layer_sampling_scheme: str = \"normal\",\n) -&gt; dict\n</code></pre> <p>Sample a configuration from a dictionary of choices.</p> PARAMETER DESCRIPTION <code>choices_dict</code> <p>a dictionary of choices for each architectural dimension</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>a dictionary with the same keys as <code>choices_dict</code> and the sampled choices</p> Source code in <code>whittle/models/gpt/utils.py</code> <pre><code>def sample_config_mid(\n    choices_dict: dict, layer_sampling_scheme: str = \"normal\"\n) -&gt; dict:\n    \"\"\"Sample a configuration from a dictionary of choices.\n\n    Args:\n        choices_dict: a dictionary of choices for each architectural dimension\n\n    Returns:\n        a dictionary with the same keys as `choices_dict` and the sampled choices\n    \"\"\"\n    sampled_dict = {}\n    # sample embed dim -&gt; held constant throughout transformer\n    sampled_dict[\"sample_embed_dim\"] = choices_dict[\"embed_dim_choices\"][1]\n    # sample number of layers\n    sampled_dict[\"sample_n_layer\"] = choices_dict[\"n_layer_choices\"][1]\n    # sample layer indices\n    max_layer = choices_dict[\"n_layer_choices\"][1]\n    sampled_dict[\"sample_layer_indices\"] = []\n    sampled_dict[\"sample_layer_indices\"] = list(range(sampled_dict[\"sample_n_layer\"]))\n\n    # sample number of heads\n    sampled_dict[\"sample_n_head\"] = [\n        choices_dict[\"n_head_choices\"][1] for _ in range(max_layer)\n    ]\n    # sample mlp ratio\n    sampled_dict[\"sample_mlp_ratio\"] = [\n        choices_dict[\"mlp_ratio_choices\"][1] for _ in range(max_layer)\n    ]\n    # sample bias\n    sampled_dict[\"sample_bias\"] = True\n\n    return sampled_dict\n</code></pre>"},{"location":"api/whittle/models/gpt/utils/#whittle.models.gpt.utils.sample_config_min","title":"sample_config_min","text":"<pre><code>sample_config_min(\n    choices_dict: dict,\n    layer_sampling_scheme: str = \"normal\",\n) -&gt; dict\n</code></pre> <p>Sample a configuration from a dictionary of choices.</p> PARAMETER DESCRIPTION <code>choices_dict</code> <p>a dictionary of choices for each architectural dimension</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>a dictionary with the same keys as <code>choices_dict</code> and the sampled choices</p> Source code in <code>whittle/models/gpt/utils.py</code> <pre><code>def sample_config_min(\n    choices_dict: dict, layer_sampling_scheme: str = \"normal\"\n) -&gt; dict:\n    \"\"\"Sample a configuration from a dictionary of choices.\n\n    Args:\n        choices_dict: a dictionary of choices for each architectural dimension\n\n    Returns:\n        a dictionary with the same keys as `choices_dict` and the sampled choices\n    \"\"\"\n    sampled_dict = {}\n    # sample embed dim -&gt; held constant throughout transformer\n    sampled_dict[\"sample_embed_dim\"] = min(choices_dict[\"embed_dim_choices\"])\n    # sample number of layers\n    sampled_dict[\"sample_n_layer\"] = min(choices_dict[\"n_layer_choices\"])\n    # sample layer indices\n    max_layer = min(choices_dict[\"n_layer_choices\"])\n    sampled_dict[\"sample_layer_indices\"] = []\n    sampled_dict[\"sample_layer_indices\"] = list(range(sampled_dict[\"sample_n_layer\"]))\n\n    # sample number of heads\n    sampled_dict[\"sample_n_head\"] = [\n        min(choices_dict[\"n_head_choices\"]) for _ in range(max_layer)\n    ]\n    # sample mlp ratio\n    sampled_dict[\"sample_mlp_ratio\"] = [\n        min(choices_dict[\"mlp_ratio_choices\"]) for _ in range(max_layer)\n    ]\n    # sample bias\n    sampled_dict[\"sample_bias\"] = True\n\n    return sampled_dict\n</code></pre>"},{"location":"api/whittle/models/gpt/blocks/causal_self_attention/","title":"Causal self attention","text":""},{"location":"api/whittle/models/gpt/blocks/causal_self_attention/#whittle.models.gpt.blocks.causal_self_attention","title":"whittle.models.gpt.blocks.causal_self_attention","text":""},{"location":"api/whittle/models/gpt/blocks/causal_self_attention/#whittle.models.gpt.blocks.causal_self_attention.CausalSelfAttention","title":"CausalSelfAttention","text":"<pre><code>CausalSelfAttention(config: Config, block_idx: int)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Extension of litgpt's <code>litgpt.model.CausalSelfAttention</code> with support to adapt to sub-network dimensionality.</p> Source code in <code>whittle/models/gpt/blocks/causal_self_attention.py</code> <pre><code>def __init__(self, config: Config, block_idx: int) -&gt; None:\n    super().__init__()\n    shape = (config.n_head + 2 * config.n_query_groups) * config.head_size\n    # key, query, value projections for all heads, but in a batch\n    self.attn = Linear(config.n_embd, shape, bias=config.bias)\n    # output projection\n    # if `head_size` is explicitly specified in the config, `n_emd` might not be equal to `head_size * n_head`\n    self.proj = Linear(\n        config.head_size * config.n_head, config.n_embd, bias=config.bias\n    )\n    # disabled by default\n    self.kv_cache: KVCache | None = None\n    self.apply_sliding_window_attention = (\n        config.sliding_window_size is not None\n        and block_idx % config.sliding_window_layer_placing == 0\n    )\n    self.config = config\n    # Set current sub-network to super-network\n    self.sub_network_n_embd = self.config.n_embd\n    self.sub_network_n_head = self.config.n_head\n    self.sub_network_head_size = self.config.head_size\n    self.sub_network_qkv_shape = (\n        self.config.n_head + 2 * self.config.n_query_groups\n    ) * self.config.head_size\n    self.sub_network_query_groups = self.config.n_query_groups\n    self.sub_network_q_per_kv = (\n        self.sub_network_n_head // self.sub_network_query_groups\n    )\n    self.sub_attention_scaler = self.config.attention_scores_scalar\n</code></pre>"},{"location":"api/whittle/models/gpt/blocks/causal_self_attention/#whittle.models.gpt.blocks.causal_self_attention.CausalSelfAttention.reset_super_network","title":"reset_super_network","text":"<pre><code>reset_super_network()\n</code></pre> <p>Resets the dimensionality of the current sub-network to the super-network dimensionality.</p> Source code in <code>whittle/models/gpt/blocks/causal_self_attention.py</code> <pre><code>def reset_super_network(self):\n    \"\"\"Resets the dimensionality of the current sub-network to the super-network dimensionality.\"\"\"\n    self.sub_network_n_embd = self.config.n_embd\n    self.sub_network_n_head = self.config.n_head\n    self.sub_network_head_size = self.config.head_size\n    self.sub_network_qkv_shape = (\n        self.config.n_head + 2 * self.config.n_query_groups\n    ) * self.config.head_size\n    self.sub_network_query_groups = self.config.n_query_groups\n    self.sub_network_q_per_kv = (\n        self.sub_network_n_head // self.sub_network_query_groups\n    )\n    self.attn.reset_super_network()\n    self.proj.reset_super_network()\n    self.sub_attention_scaler = self.config.attention_scores_scalar\n</code></pre>"},{"location":"api/whittle/models/gpt/blocks/causal_self_attention/#whittle.models.gpt.blocks.causal_self_attention.CausalSelfAttention.set_sub_network","title":"set_sub_network","text":"<pre><code>set_sub_network(\n    sub_network_n_embd: int,\n    sub_network_n_head: int,\n    sub_network_query_groups: int,\n    sub_network_head_size: int,\n)\n</code></pre> <p>Sets the CausalSelfAttention block to the specified sub-network dimensionality.</p> PARAMETER DESCRIPTION <code>sub_network_n_embd</code> <p>Embedding dimension of the sub-network</p> <p> TYPE: <code>int</code> </p> <code>sub_network_n_head</code> <p>Number of attention heads in the sub-network</p> <p> TYPE: <code>int</code> </p> <code>sub_network_query_groups</code> <p>Number of query groups for grouped-query attention (GQA).</p> <p> TYPE: <code>int</code> </p> <code>sub_network_head_size</code> <p>Size of each attention head in the sub-network.</p> <p> TYPE: <code>int</code> </p> Source code in <code>whittle/models/gpt/blocks/causal_self_attention.py</code> <pre><code>def set_sub_network(\n    self,\n    sub_network_n_embd: int,\n    sub_network_n_head: int,\n    sub_network_query_groups: int,\n    sub_network_head_size: int,\n):\n    \"\"\"\n    Sets the CausalSelfAttention block to the specified sub-network dimensionality.\n\n    Args:\n        sub_network_n_embd: Embedding dimension of the sub-network\n        sub_network_n_head: Number of attention heads in the sub-network\n        sub_network_query_groups: Number of query groups for grouped-query attention (GQA).\n        sub_network_head_size: Size of each attention head in the sub-network.\n    \"\"\"\n    self.sub_network_n_embd = sub_network_n_embd\n    self.sub_network_n_head = sub_network_n_head\n    self.sub_network_query_groups = sub_network_query_groups\n    self.sub_network_head_size = sub_network_head_size\n\n    self.sub_network_qkv_shape = (\n        self.sub_network_n_head + 2 * self.sub_network_query_groups\n    ) * self.sub_network_head_size\n\n    self.attn.set_sub_network(self.sub_network_n_embd, self.sub_network_qkv_shape)\n    self.proj.set_sub_network(\n        self.sub_network_head_size * self.sub_network_n_head,\n        self.sub_network_n_embd,\n    )\n    self.sub_network_q_per_kv = self.sub_network_n_head // float(\n        self.sub_network_query_groups\n    )\n    if self.config.attention_scores_scalar:\n        self.sub_attention_scaler = (\n            self.sub_network_n_embd // self.sub_network_n_head\n        )\n    else:\n        self.sub_attention_scaler = self.config.attention_scores_scalar\n</code></pre>"},{"location":"api/whittle/models/gpt/blocks/mlp/","title":"Mlp","text":""},{"location":"api/whittle/models/gpt/blocks/mlp/#whittle.models.gpt.blocks.mlp","title":"whittle.models.gpt.blocks.mlp","text":""},{"location":"api/whittle/models/gpt/blocks/mlp/#whittle.models.gpt.blocks.mlp.GemmaMLP","title":"GemmaMLP","text":"<pre><code>GemmaMLP(config: Config)\n</code></pre> <p>               Bases: <code>LLaMAMLP</code></p> <p>Implementation of the forward pass of LLaMAMLP network.</p> Source code in <code>whittle/models/gpt/blocks/mlp.py</code> <pre><code>def __init__(self, config: Config) -&gt; None:\n    super().__init__(config)\n</code></pre>"},{"location":"api/whittle/models/gpt/blocks/mlp/#whittle.models.gpt.blocks.mlp.GemmaMLP.reset_super_network","title":"reset_super_network","text":"<pre><code>reset_super_network()\n</code></pre> <p>Reset the input dimensionality of the current sub-network to the super-network dimensionality.</p> Source code in <code>whittle/models/gpt/blocks/mlp.py</code> <pre><code>def reset_super_network(self):\n    \"\"\"Reset the input dimensionality of the current sub-network to the super-network dimensionality.\"\"\"\n    self.sub_network_n_embd = self.in_features\n    self.sub_network_intermediate_size = self.intermediate_size\n\n    self.fc_1.reset_super_network()\n    self.fc_2.reset_super_network()\n    self.proj.reset_super_network()\n</code></pre>"},{"location":"api/whittle/models/gpt/blocks/mlp/#whittle.models.gpt.blocks.mlp.GemmaMLP.set_sub_network","title":"set_sub_network","text":"<pre><code>set_sub_network(\n    sub_network_n_embd: int,\n    sub_network_intermediate_size: int,\n)\n</code></pre> <p>Sets the dimensionality of the current sub-network MLP layers.</p> PARAMETER DESCRIPTION <code>sub_network_n_embd</code> <p>Input and output embedding dimension of the sub-network.</p> <p> TYPE: <code>int</code> </p> <code>sub_network_intermediate_size</code> <p>Hidden layer dimension of the sub-network MLP.</p> <p> TYPE: <code>int</code> </p> Source code in <code>whittle/models/gpt/blocks/mlp.py</code> <pre><code>def set_sub_network(\n    self, sub_network_n_embd: int, sub_network_intermediate_size: int\n):\n    \"\"\"\n    Sets the dimensionality of the current sub-network MLP layers.\n\n    Args:\n        sub_network_n_embd: Input and output embedding dimension of the sub-network.\n        sub_network_intermediate_size: Hidden layer dimension of the sub-network MLP.\n    \"\"\"\n    self.sub_network_n_embd = sub_network_n_embd\n    self.sub_network_intermediate_size = sub_network_intermediate_size\n\n    self.fc_1.set_sub_network(\n        self.sub_network_n_embd, self.sub_network_intermediate_size\n    )\n    self.fc_2.set_sub_network(\n        self.sub_network_n_embd, self.sub_network_intermediate_size\n    )\n    self.proj.set_sub_network(\n        self.sub_network_intermediate_size, self.sub_network_n_embd\n    )\n</code></pre>"},{"location":"api/whittle/models/gpt/blocks/mlp/#whittle.models.gpt.blocks.mlp.GptNeoxMLP","title":"GptNeoxMLP","text":"<pre><code>GptNeoxMLP(config: Config)\n</code></pre> <p>               Bases: <code>GptNeoxMLP</code></p> <p>An extension of litgp's <code>litgpt.model.GptNeoxMLP</code> with support to adapt to sub-network dimensionality.</p> Source code in <code>whittle/models/gpt/blocks/mlp.py</code> <pre><code>def __init__(self, config: Config) -&gt; None:\n    super().__init__(config)\n    self.fc = Linear(config.n_embd, config.intermediate_size, bias=config.bias)\n    self.proj = Linear(config.intermediate_size, config.n_embd, bias=config.bias)\n    self.config = config\n    self.in_features = config.n_embd\n    self.intermediate_size = config.intermediate_size\n\n    # Set current sub-network to super-network\n    self.sub_network_n_embd = self.in_features\n    self.sub_network_intermediate_size = self.intermediate_size\n</code></pre>"},{"location":"api/whittle/models/gpt/blocks/mlp/#whittle.models.gpt.blocks.mlp.GptNeoxMLP.reset_super_network","title":"reset_super_network","text":"<pre><code>reset_super_network()\n</code></pre> <p>Resets the MLP dimensions to the original super-network dimensionality.</p> Source code in <code>whittle/models/gpt/blocks/mlp.py</code> <pre><code>def reset_super_network(self):\n    \"\"\"Resets the MLP dimensions to the original super-network dimensionality.\"\"\"\n    self.sub_network_n_embd = self.in_features\n    self.sub_network_intermediate_size = self.intermediate_size\n\n    self.fc.reset_super_network()\n    self.proj.reset_super_network()\n</code></pre>"},{"location":"api/whittle/models/gpt/blocks/mlp/#whittle.models.gpt.blocks.mlp.GptNeoxMLP.set_sub_network","title":"set_sub_network","text":"<pre><code>set_sub_network(\n    sub_network_n_embd: int,\n    sub_network_intermediate_size: int,\n)\n</code></pre> <p>Sets the dimensionality of the current sub-network MLP layers.</p> PARAMETER DESCRIPTION <code>sub_network_n_embd</code> <p>Input and output embedding dimension of the sub-network.</p> <p> TYPE: <code>int</code> </p> <code>sub_network_intermediate_size</code> <p>Hidden layer dimension of the sub-network MLP.</p> <p> TYPE: <code>int</code> </p> Source code in <code>whittle/models/gpt/blocks/mlp.py</code> <pre><code>def set_sub_network(\n    self, sub_network_n_embd: int, sub_network_intermediate_size: int\n):\n    \"\"\"\n    Sets the dimensionality of the current sub-network MLP layers.\n\n    Args:\n       sub_network_n_embd: Input and output embedding dimension of the sub-network.\n       sub_network_intermediate_size: Hidden layer dimension of the sub-network MLP.\n    \"\"\"\n    self.sub_network_n_embd = sub_network_n_embd\n    self.sub_network_intermediate_size = sub_network_intermediate_size\n\n    self.fc.set_sub_network(\n        self.sub_network_n_embd, self.sub_network_intermediate_size\n    )\n    self.proj.set_sub_network(\n        self.sub_network_intermediate_size, self.sub_network_n_embd\n    )\n</code></pre>"},{"location":"api/whittle/models/gpt/blocks/mlp/#whittle.models.gpt.blocks.mlp.LLaMAMLP","title":"LLaMAMLP","text":"<pre><code>LLaMAMLP(config: Config)\n</code></pre> <p>               Bases: <code>LLaMAMLP</code></p> <p>An extension of litgp's <code>litgpt.model.LLaMAMLP</code> with support to adapt to sub-network dimensionality.</p> Source code in <code>whittle/models/gpt/blocks/mlp.py</code> <pre><code>def __init__(self, config: Config) -&gt; None:\n    super().__init__(config)\n    self.fc_1 = Linear(config.n_embd, config.intermediate_size, bias=config.bias)\n    self.fc_2 = Linear(config.n_embd, config.intermediate_size, bias=config.bias)\n    self.proj = Linear(config.intermediate_size, config.n_embd, bias=config.bias)\n    self.in_features = config.n_embd\n    self.intermediate_size = config.intermediate_size\n    self.sub_network_n_embd: int | None = None\n    self.sub_network_intermediate_size: int | None = None\n    self.config = config\n</code></pre>"},{"location":"api/whittle/models/gpt/blocks/mlp/#whittle.models.gpt.blocks.mlp.LLaMAMLP.reset_super_network","title":"reset_super_network","text":"<pre><code>reset_super_network()\n</code></pre> <p>Reset the input dimensionality of the current sub-network to the super-network dimensionality.</p> Source code in <code>whittle/models/gpt/blocks/mlp.py</code> <pre><code>def reset_super_network(self):\n    \"\"\"Reset the input dimensionality of the current sub-network to the super-network dimensionality.\"\"\"\n    self.sub_network_n_embd = self.in_features\n    self.sub_network_intermediate_size = self.intermediate_size\n\n    self.fc_1.reset_super_network()\n    self.fc_2.reset_super_network()\n    self.proj.reset_super_network()\n</code></pre>"},{"location":"api/whittle/models/gpt/blocks/mlp/#whittle.models.gpt.blocks.mlp.LLaMAMLP.set_sub_network","title":"set_sub_network","text":"<pre><code>set_sub_network(\n    sub_network_n_embd: int,\n    sub_network_intermediate_size: int,\n)\n</code></pre> <p>Sets the dimensionality of the current sub-network MLP layers.</p> PARAMETER DESCRIPTION <code>sub_network_n_embd</code> <p>Input and output embedding dimension of the sub-network.</p> <p> TYPE: <code>int</code> </p> <code>sub_network_intermediate_size</code> <p>Hidden layer dimension of the sub-network MLP.</p> <p> TYPE: <code>int</code> </p> Source code in <code>whittle/models/gpt/blocks/mlp.py</code> <pre><code>def set_sub_network(\n    self, sub_network_n_embd: int, sub_network_intermediate_size: int\n):\n    \"\"\"\n    Sets the dimensionality of the current sub-network MLP layers.\n\n    Args:\n        sub_network_n_embd: Input and output embedding dimension of the sub-network.\n        sub_network_intermediate_size: Hidden layer dimension of the sub-network MLP.\n    \"\"\"\n    self.sub_network_n_embd = sub_network_n_embd\n    self.sub_network_intermediate_size = sub_network_intermediate_size\n\n    self.fc_1.set_sub_network(\n        self.sub_network_n_embd, self.sub_network_intermediate_size\n    )\n    self.fc_2.set_sub_network(\n        self.sub_network_n_embd, self.sub_network_intermediate_size\n    )\n    self.proj.set_sub_network(\n        self.sub_network_intermediate_size, self.sub_network_n_embd\n    )\n</code></pre>"},{"location":"api/whittle/models/gpt/blocks/transformer_block/","title":"Transformer block","text":""},{"location":"api/whittle/models/gpt/blocks/transformer_block/#whittle.models.gpt.blocks.transformer_block","title":"whittle.models.gpt.blocks.transformer_block","text":""},{"location":"api/whittle/models/gpt/blocks/transformer_block/#whittle.models.gpt.blocks.transformer_block.Block","title":"Block","text":"<pre><code>Block(config: Config, block_idx: int)\n</code></pre> <p>               Bases: <code>Block</code></p> <p>An extension of litgpt's Transformer Block with support to adapt to sub-network dimensionality.</p> Source code in <code>whittle/models/gpt/blocks/transformer_block.py</code> <pre><code>def __init__(self, config: Config, block_idx: int) -&gt; None:\n    super().__init__(config, block_idx)\n    self.config = config\n    if not config.parallel_residual and config.shared_attention_norm:\n        raise NotImplementedError(\n            \"No checkpoint amongst the ones we support uses this configuration\"\n            \" (non-parallel residual and shared attention norm).\"\n        )\n\n    self.norm_1 = self.norm_class()(config.n_embd, eps=config.norm_eps)\n    self.attn = CausalSelfAttention(config, block_idx)\n    self.post_attention_norm = (\n        self.norm_class()(config.n_embd, eps=config.norm_eps)\n        if config.post_attention_norm\n        else nn.Identity()\n    )\n    self.norm_2: LayerNorm | RMSNorm | None = (\n        None\n        if config.shared_attention_norm\n        else self.norm_class()(config.n_embd, eps=config.norm_eps)\n    )\n    self.mlp = self.mlp_class()(config)\n    self.post_mlp_norm = (\n        self.norm_class()(config.n_embd, eps=config.norm_eps)\n        if config.post_mlp_norm\n        else nn.Identity()\n    )\n    # Set current sub-network to super-network\n    self.sub_network_n_embd = self.config.n_embd\n    self.sub_network_intermediate_size = self.config.intermediate_size\n    self.sub_network_num_heads = self.config.n_head\n</code></pre>"},{"location":"api/whittle/models/gpt/blocks/transformer_block/#whittle.models.gpt.blocks.transformer_block.Block.reset_super_network","title":"reset_super_network","text":"<pre><code>reset_super_network()\n</code></pre> <p>Resets the layers in the Block to it's original super-network dimensionality.</p> Source code in <code>whittle/models/gpt/blocks/transformer_block.py</code> <pre><code>def reset_super_network(self):\n    \"\"\"\n    Resets the layers in the Block to it's original super-network dimensionality.\n    \"\"\"\n    self.sub_network_n_embd = self.config.n_embd\n    self.sub_network_intermediate_size = self.config.intermediate_size\n    self.sub_network_num_heads = self.config.n_head\n    self.norm_1.reset_super_network()\n    self.attn.reset_super_network()\n    if not self.config.shared_attention_norm:\n        self.norm_2.reset_super_network()\n    self.mlp.reset_super_network()\n    if isinstance(self.post_attention_norm, LayerNorm) or isinstance(\n        self.post_attention_norm, RMSNorm\n    ):\n        self.post_attention_norm.reset_super_network()\n    if isinstance(self.post_mlp_norm, LayerNorm) or isinstance(\n        self.post_mlp_norm, RMSNorm\n    ):\n        self.post_mlp_norm.reset_super_network()\n</code></pre>"},{"location":"api/whittle/models/gpt/blocks/transformer_block/#whittle.models.gpt.blocks.transformer_block.Block.set_sub_network","title":"set_sub_network","text":"<pre><code>set_sub_network(\n    sub_network_n_embd: int,\n    sub_network_intermediate_size: int,\n    sub_network_num_heads: int,\n    sub_network_query_groups: int,\n    sub_network_head_size: int,\n) -&gt; None\n</code></pre> <p>Set the Block to the specified sub-network dimensionality.</p> PARAMETER DESCRIPTION <code>sub_network_n_embd</code> <p>Embedding dimension of the sub-network.</p> <p> TYPE: <code>int</code> </p> <code>sub_network_intermediate_size</code> <p>Intermediate size of the sub-network.</p> <p> TYPE: <code>int</code> </p> <code>sub_network_num_heads</code> <p>Number of attention heads in the sub-network.</p> <p> TYPE: <code>int</code> </p> <code>sub_network_query_groups</code> <p>Number of query groups in the sub-network.</p> <p> TYPE: <code>int</code> </p> <code>sub_network_head_size</code> <p>Size of each attention head in the sub-network.</p> <p> TYPE: <code>int</code> </p> Source code in <code>whittle/models/gpt/blocks/transformer_block.py</code> <pre><code>def set_sub_network(\n    self,\n    sub_network_n_embd: int,\n    sub_network_intermediate_size: int,\n    sub_network_num_heads: int,\n    sub_network_query_groups: int,\n    sub_network_head_size: int,\n) -&gt; None:\n    \"\"\"\n    Set the Block to the specified sub-network dimensionality.\n\n    Args:\n        sub_network_n_embd: Embedding dimension of the sub-network.\n        sub_network_intermediate_size: Intermediate size of the sub-network.\n        sub_network_num_heads: Number of attention heads in the sub-network.\n        sub_network_query_groups: Number of query groups in the sub-network.\n        sub_network_head_size: Size of each attention head in the sub-network.\n    \"\"\"\n    self.sub_network_n_embd = sub_network_n_embd\n    self.sub_network_intermediate_size = sub_network_intermediate_size\n    self.sub_network_num_heads = sub_network_num_heads\n    self.norm_1.set_sub_network(self.sub_network_n_embd)\n    self.attn.set_sub_network(\n        self.sub_network_n_embd,\n        self.sub_network_num_heads,\n        sub_network_query_groups,\n        sub_network_head_size,\n    )\n    if isinstance(self.post_attention_norm, LayerNorm) or isinstance(\n        self.post_attention_norm, RMSNorm\n    ):\n        self.post_attention_norm.set_sub_network(self.sub_network_n_embd)\n    if not self.config.shared_attention_norm and self.norm_2 is not None:\n        self.norm_2.set_sub_network(self.sub_network_n_embd)\n    self.mlp.set_sub_network(\n        self.sub_network_n_embd, self.sub_network_intermediate_size\n    )\n    if isinstance(self.post_mlp_norm, LayerNorm) or isinstance(\n        self.post_mlp_norm, RMSNorm\n    ):\n        self.post_mlp_norm.set_sub_network(self.sub_network_n_embd)\n</code></pre>"},{"location":"api/whittle/modules/embedding/","title":"Embedding","text":""},{"location":"api/whittle/modules/embedding/#whittle.modules.embedding","title":"whittle.modules.embedding","text":""},{"location":"api/whittle/modules/embedding/#whittle.modules.embedding.Embedding","title":"Embedding","text":"<pre><code>Embedding(\n    num_embeddings: int,\n    embedding_dim: int,\n    padding_idx: int | None = None,\n    max_norm: float | None = None,\n    norm_type: float = 2.0,\n    scale_grad_by_freq: bool = False,\n    sparse: bool = False,\n    device=None,\n    dtype=None,\n)\n</code></pre> <p>               Bases: <code>Embedding</code></p> <p>An extension of PyTorch's torch.nn.Embedding with support to sub-sample weights corresponding to the sub-network dimensionality</p> Source code in <code>whittle/modules/embedding.py</code> <pre><code>def __init__(\n    self,\n    num_embeddings: int,\n    embedding_dim: int,\n    padding_idx: int | None = None,\n    max_norm: float | None = None,\n    norm_type: float = 2.0,\n    scale_grad_by_freq: bool = False,\n    sparse: bool = False,\n    device=None,\n    dtype=None,\n) -&gt; None:\n    super().__init__(\n        num_embeddings,\n        embedding_dim,\n        padding_idx,\n        max_norm,\n        norm_type,\n        scale_grad_by_freq,\n        sparse,\n        device,\n        dtype,\n    )\n\n    # the embedding dimensionality of the current sub-network\n    self.sub_network_embedding_dim: int | None = embedding_dim\n</code></pre>"},{"location":"api/whittle/modules/embedding/#whittle.modules.embedding.Embedding.reset_super_network","title":"reset_super_network","text":"<pre><code>reset_super_network()\n</code></pre> <p>Reset the embedding dimensionality of the current sub-network to the super-network dimensionality</p> Source code in <code>whittle/modules/embedding.py</code> <pre><code>def reset_super_network(self):\n    \"\"\"Reset the embedding dimensionality of the current sub-network to the super-network dimensionality\"\"\"\n    self.sub_network_embedding_dim = self.embedding_dim\n</code></pre>"},{"location":"api/whittle/modules/embedding/#whittle.modules.embedding.Embedding.set_sub_network","title":"set_sub_network","text":"<pre><code>set_sub_network(sub_network_embedding_dim: int)\n</code></pre> <p>Set the embedding dimensionality of the current sub-network.</p> Source code in <code>whittle/modules/embedding.py</code> <pre><code>def set_sub_network(self, sub_network_embedding_dim: int):\n    \"\"\"Set the embedding dimensionality of the current sub-network.\"\"\"\n    self.sub_network_embedding_dim = sub_network_embedding_dim\n</code></pre>"},{"location":"api/whittle/modules/layernorm/","title":"Layernorm","text":""},{"location":"api/whittle/modules/layernorm/#whittle.modules.layernorm","title":"whittle.modules.layernorm","text":""},{"location":"api/whittle/modules/layernorm/#whittle.modules.layernorm.LayerNorm","title":"LayerNorm","text":"<pre><code>LayerNorm(in_features: int, eps: float = 1e-05)\n</code></pre> <p>               Bases: <code>LayerNorm</code></p> <p>An extension of PyTorch's <code>torch.nn.LayerNorm</code> with support  with support to sub-sample weights corresponding to the sub-network dimensionality.</p> Source code in <code>whittle/modules/layernorm.py</code> <pre><code>def __init__(self, in_features: int, eps: float = 1e-5):\n    super().__init__(in_features, eps)\n    self.in_features = in_features\n\n    # Set current sub-network to super-network\n    self.sub_network_in_features = self.in_features\n</code></pre>"},{"location":"api/whittle/modules/layernorm/#whittle.modules.layernorm.LayerNorm.reset_super_network","title":"reset_super_network","text":"<pre><code>reset_super_network()\n</code></pre> <p>Reset the input dimensionality of the current sub-network to the super-network dimensionality.</p> Source code in <code>whittle/modules/layernorm.py</code> <pre><code>def reset_super_network(self):\n    \"\"\"Reset the input dimensionality of the current sub-network to the super-network dimensionality.\"\"\"\n    self.sub_network_in_features = self.in_features\n</code></pre>"},{"location":"api/whittle/modules/layernorm/#whittle.modules.layernorm.LayerNorm.set_sub_network","title":"set_sub_network","text":"<pre><code>set_sub_network(sub_network_in_features: int)\n</code></pre> <p>Set the input dimensionality of the current sub-network.</p> Source code in <code>whittle/modules/layernorm.py</code> <pre><code>def set_sub_network(self, sub_network_in_features: int):\n    \"\"\"Set the input dimensionality of the current sub-network.\"\"\"\n    self.sub_network_in_features = sub_network_in_features\n</code></pre>"},{"location":"api/whittle/modules/linear/","title":"Linear","text":""},{"location":"api/whittle/modules/linear/#whittle.modules.linear","title":"whittle.modules.linear","text":""},{"location":"api/whittle/modules/linear/#whittle.modules.linear.Linear","title":"Linear","text":"<pre><code>Linear(\n    in_features: int,\n    out_features: int,\n    bias: bool = True,\n    device=None,\n    dtype=None,\n)\n</code></pre> <p>               Bases: <code>Linear</code></p> <p>An extension of PyTorch's torch.nn.Linear with flexible input and output dimensionality corresponding to sub-network</p> Source code in <code>whittle/modules/linear.py</code> <pre><code>def __init__(\n    self,\n    in_features: int,\n    out_features: int,\n    bias: bool = True,\n    device=None,\n    dtype=None,\n):\n    super().__init__(in_features, out_features, bias, device, dtype)\n\n    # Set the current sub-network dimensions equal to super-network\n    self.sub_network_in_features = in_features\n    self.sub_network_out_features = out_features\n    self.use_bias = bias\n</code></pre>"},{"location":"api/whittle/modules/linear/#whittle.modules.linear.Linear.reset_super_network","title":"reset_super_network","text":"<pre><code>reset_super_network()\n</code></pre> <p>Reset the linear transformation dimensions of the current sub-network to the super-network dimensionality.</p> Source code in <code>whittle/modules/linear.py</code> <pre><code>def reset_super_network(self):\n    \"\"\"Reset the linear transformation dimensions of the current sub-network to the super-network dimensionality.\"\"\"\n    self.sub_network_in_features = self.in_features\n    self.sub_network_out_features = self.out_features\n</code></pre>"},{"location":"api/whittle/modules/linear/#whittle.modules.linear.Linear.set_sub_network","title":"set_sub_network","text":"<pre><code>set_sub_network(\n    sub_network_in_features: int,\n    sub_network_out_features: int,\n)\n</code></pre> <p>Set the linear transformation dimensions of the current sub-network.</p> Source code in <code>whittle/modules/linear.py</code> <pre><code>def set_sub_network(\n    self, sub_network_in_features: int, sub_network_out_features: int\n):\n    \"\"\"Set the linear transformation dimensions of the current sub-network.\"\"\"\n    self.sub_network_in_features = sub_network_in_features\n    self.sub_network_out_features = sub_network_out_features\n</code></pre>"},{"location":"api/whittle/modules/rmsnorm/","title":"Rmsnorm","text":""},{"location":"api/whittle/modules/rmsnorm/#whittle.modules.rmsnorm","title":"whittle.modules.rmsnorm","text":""},{"location":"api/whittle/modules/rmsnorm/#whittle.modules.rmsnorm.RMSNorm","title":"RMSNorm","text":"<pre><code>RMSNorm(\n    in_features: int,\n    dim: int = -1,\n    eps: float = 1e-06,\n    add_unit_offset: bool = False,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Root Mean Square Layer Normalization.</p> <p>Derived from github.com/bzhangGo/rmsnorm/blob/master/rmsnorm_torch.py. BSD 3-Clause License: github.com/bzhangGo/rmsnorm/blob/master/LICENSE.</p> Source code in <code>whittle/modules/rmsnorm.py</code> <pre><code>def __init__(\n    self,\n    in_features: int,\n    dim: int = -1,\n    eps: float = 1e-6,\n    add_unit_offset: bool = False,\n) -&gt; None:\n    super().__init__()\n    self.in_features = in_features\n    self.weight = torch.nn.Parameter(torch.ones(in_features))\n    self.eps = eps\n    self.dim = dim\n    self.add_unit_offset = add_unit_offset\n    self.sub_network_in_features: int | None = in_features\n</code></pre>"},{"location":"api/whittle/sampling/random_sampler/","title":"Random sampler","text":""},{"location":"api/whittle/sampling/random_sampler/#whittle.sampling.random_sampler","title":"whittle.sampling.random_sampler","text":""},{"location":"api/whittle/sampling/random_sampler/#whittle.sampling.random_sampler.RandomSampler","title":"RandomSampler","text":"<pre><code>RandomSampler(config_space: dict, seed: int | None = None)\n</code></pre> <p>RandomSampler samples configurations from a given search space using a random state.</p> PARAMETER DESCRIPTION <code>config_space</code> <p>The search space from which to sample.</p> <p> TYPE: <code>dict</code> </p> <code>seed</code> <p>Seed for the random number generator. Defaults to None.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>whittle/sampling/random_sampler.py</code> <pre><code>def __init__(self, config_space: dict, seed: int | None = None):\n    self.config_space = config_space\n    self.rng = np.random.RandomState(seed)\n</code></pre>"},{"location":"api/whittle/sampling/random_sampler/#whittle.sampling.random_sampler.RandomSampler.get_largest_sub_network","title":"get_largest_sub_network","text":"<pre><code>get_largest_sub_network() -&gt; dict[str, Any]\n</code></pre> <p>gets the largest sub-network configuration from the search space.</p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>The largest sub-network configuration.</p> Source code in <code>whittle/sampling/random_sampler.py</code> <pre><code>def get_largest_sub_network(self) -&gt; dict[str, Any]:\n    \"\"\"\n    gets the largest sub-network configuration from the search space.\n\n    Returns:\n        The largest sub-network configuration.\n    \"\"\"\n\n    config = {}\n    for k, v in self.config_space.items():\n        if isinstance(v, Domain):\n            if isinstance(v, Categorical):\n                if all(isinstance(e, (int, float)) for e in v.categories):\n                    config[k] = max(v.categories)\n                else:\n                    warnings.warn(\n                        \"Warning: Categoricals are non-integers, check if largest network is as intended\"\n                    )\n                    config[k] = v.categories[-1]\n            else:\n                config[k] = v.upper\n    return config\n</code></pre>"},{"location":"api/whittle/sampling/random_sampler/#whittle.sampling.random_sampler.RandomSampler.get_smallest_sub_network","title":"get_smallest_sub_network","text":"<pre><code>get_smallest_sub_network() -&gt; dict[str, Any]\n</code></pre> <p>Gets the smallest sub-network configuration from the search space.</p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>The smallest sub-network configuration.</p> Source code in <code>whittle/sampling/random_sampler.py</code> <pre><code>def get_smallest_sub_network(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Gets the smallest sub-network configuration from the search space.\n\n    Returns:\n        The smallest sub-network configuration.\n    \"\"\"\n    config = {}\n    for k, v in self.config_space.items():\n        if isinstance(v, Domain):\n            if isinstance(v, Categorical):\n                if all(isinstance(e, (int, float)) for e in v.categories):\n                    config[k] = min(v.categories)\n                else:\n                    warnings.warn(\n                        \"Warning: Categoricals are non-integers, check if smallest network is as intended\"\n                    )\n                    config[k] = v.categories[0]\n            else:\n                config[k] = v.lower\n    return config\n</code></pre>"},{"location":"api/whittle/sampling/random_sampler/#whittle.sampling.random_sampler.RandomSampler.sample","title":"sample","text":"<pre><code>sample() -&gt; dict[str, Any]\n</code></pre> <p>Gets the smallest sub-network configuration from the search space.</p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>The smallest sub-network configuration.</p> Source code in <code>whittle/sampling/random_sampler.py</code> <pre><code>def sample(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Gets the smallest sub-network configuration from the search space.\n\n    Returns:\n        The smallest sub-network configuration.\n    \"\"\"\n    config = {}\n    for hp_name, hparam in self.config_space.items():\n        if isinstance(hparam, Domain):\n            config[hp_name] = hparam.sample(random_state=self.rng)\n    return config\n</code></pre>"},{"location":"api/whittle/search/ask_tell_scheduler/","title":"Ask tell scheduler","text":""},{"location":"api/whittle/search/ask_tell_scheduler/#whittle.search.ask_tell_scheduler","title":"whittle.search.ask_tell_scheduler","text":""},{"location":"api/whittle/search/ask_tell_scheduler/#whittle.search.ask_tell_scheduler.AskTellScheduler","title":"AskTellScheduler","text":"<pre><code>AskTellScheduler(base_scheduler: TrialScheduler)\n</code></pre> Source code in <code>whittle/search/ask_tell_scheduler.py</code> <pre><code>def __init__(self, base_scheduler: TrialScheduler):\n    self.bscheduler = base_scheduler\n    self.trial_counter = 0\n    self.completed_experiments = {}\n</code></pre>"},{"location":"api/whittle/search/ask_tell_scheduler/#whittle.search.ask_tell_scheduler.AskTellScheduler.ask","title":"ask","text":"<pre><code>ask() -&gt; Trial\n</code></pre> <p>Ask the scheduler for new trial to run</p> RETURNS DESCRIPTION <code>Trial</code> <p>Trial to run</p> Source code in <code>whittle/search/ask_tell_scheduler.py</code> <pre><code>def ask(self) -&gt; Trial:\n    \"\"\"\n    Ask the scheduler for new trial to run\n\n    Returns:\n        Trial to run\n    \"\"\"\n    trial_suggestion = self.bscheduler.suggest(self.trial_counter)\n    trial = Trial(\n        trial_id=self.trial_counter,\n        config=trial_suggestion.config,\n        creation_time=datetime.datetime.now(),\n    )\n    self.trial_counter += 1\n    return trial\n</code></pre>"},{"location":"api/whittle/search/ask_tell_scheduler/#whittle.search.ask_tell_scheduler.AskTellScheduler.best_trial","title":"best_trial","text":"<pre><code>best_trial(metris: str) -&gt; TrialResult\n</code></pre> RETURNS DESCRIPTION <code>TrialResult</code> <p>the best trial according to the provided metric.</p> Source code in <code>whittle/search/ask_tell_scheduler.py</code> <pre><code>def best_trial(self, metris: str) -&gt; TrialResult:\n    \"\"\"\n    Returns:\n        the best trial according to the provided metric.\n    \"\"\"\n    if self.bscheduler.mode == \"max\":\n        sign = 1.0\n    else:\n        sign = -1.0\n\n    return max(\n        [value for key, value in self.completed_experiments.items()],\n        key=lambda trial: sign * trial.metrics[metris],\n    )\n</code></pre>"},{"location":"api/whittle/search/ask_tell_scheduler/#whittle.search.ask_tell_scheduler.AskTellScheduler.tell","title":"tell","text":"<pre><code>tell(trial: Trial, experiment_result: dict[str, float])\n</code></pre> <p>Feed experiment results back to the Scheduler.</p> PARAMETER DESCRIPTION <code>trial</code> <p>Trial that was run.</p> <p> TYPE: <code>Trial</code> </p> <code>experiment_result</code> <p>{metric: value} dictionary with experiment results.</p> <p> TYPE: <code>dict[str, float]</code> </p> Source code in <code>whittle/search/ask_tell_scheduler.py</code> <pre><code>def tell(self, trial: Trial, experiment_result: dict[str, float]):\n    \"\"\"\n    Feed experiment results back to the Scheduler.\n\n    Args:\n        trial: Trial that was run.\n        experiment_result: {metric: value} dictionary with experiment results.\n\n    \"\"\"\n    trial_result = trial.add_results(\n        metrics=experiment_result,\n        status=Status.completed,\n        training_end_time=datetime.datetime.now(),\n    )\n    self.bscheduler.on_trial_complete(trial=trial, result=experiment_result)\n    self.completed_experiments[trial_result.trial_id] = trial_result\n</code></pre>"},{"location":"api/whittle/search/baselines/","title":"Baselines","text":""},{"location":"api/whittle/search/baselines/#whittle.search.baselines","title":"whittle.search.baselines","text":""},{"location":"api/whittle/search/local_search/","title":"Local search","text":""},{"location":"api/whittle/search/local_search/#whittle.search.local_search","title":"whittle.search.local_search","text":""},{"location":"api/whittle/search/local_search/#whittle.search.local_search.LS","title":"LS","text":"<pre><code>LS(\n    config_space: dict[str, Any],\n    metric: list[str],\n    mode: list[str] | str = \"min\",\n    start_point: dict[str, Any] | None = None,\n    random_seed: int | None = None,\n    points_to_evaluate: list[dict] | None = None,\n    **kwargs: Any\n)\n</code></pre> <p>               Bases: <code>FIFOScheduler</code></p> <p>Local Search Scheduler for hyperparameter optimization.</p> <p>This scheduler uses a local search strategy to explore the configuration space. It extends the FIFOScheduler and integrates with the LocalSearch searcher.</p> PARAMETER DESCRIPTION <code>config_space</code> <p>Configuration space for the evaluation function.</p> <p> TYPE: <code>dict[str, Any]</code> </p> <code>metric</code> <p>List of metrics to optimize.</p> <p> TYPE: <code>list[str]</code> </p> <code>mode</code> <p>Optimization mode, either \"min\" or \"max\". Defaults to \"min\".</p> <p> TYPE: <code>list[str] | str</code> DEFAULT: <code>'min'</code> </p> <code>start_point</code> <p>Starting point for the search. Defaults to None.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>random_seed</code> <p>Random seed for reproducibility. Defaults to None.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>points_to_evaluate</code> <p>Initial points to evaluate. Defaults to None.</p> <p> TYPE: <code>list[dict] | None</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <p>Additional arguments for the FIFOScheduler.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>whittle/search/local_search.py</code> <pre><code>def __init__(\n    self,\n    config_space: dict[str, Any],\n    metric: list[str],\n    mode: list[str] | str = \"min\",\n    start_point: dict[str, Any] | None = None,\n    random_seed: int | None = None,\n    points_to_evaluate: list[dict] | None = None,\n    **kwargs: Any,\n):\n    super().__init__(\n        config_space=config_space,\n        metric=metric,\n        mode=mode,\n        searcher=LocalSearch(\n            config_space=config_space,\n            metric=metric,\n            start_point=start_point,\n            mode=mode,\n            random_seed=random_seed,\n            points_to_evaluate=points_to_evaluate,\n        ),\n        random_seed=random_seed,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/whittle/search/local_search/#whittle.search.local_search.LocalSearch","title":"LocalSearch","text":"<pre><code>LocalSearch(\n    config_space: dict[str, Any],\n    metric: list[str] | str,\n    points_to_evaluate: list[dict] | None = None,\n    start_point: dict | None = None,\n    mode: list[str] | str = \"min\",\n    **kwargs: Any\n)\n</code></pre> <p>               Bases: <code>StochasticSearcher</code></p> <p>Local Search algorithm for hyperparameter optimization.</p> <p>This searcher uses a local search strategy to explore the configuration space. It extends the StochasticSearcher and used searcher input parameter in LS scheduler.</p> PARAMETER DESCRIPTION <code>config_space</code> <p>Configuration space for the evaluation function.</p> <p> TYPE: <code>dict[str, Any]</code> </p> <code>metric</code> <p>List of metrics to optimize.</p> <p> TYPE: <code>list[str] | str</code> </p> <code>points_to_evaluate</code> <p>Initial points to evaluate. Defaults to None.</p> <p> TYPE: <code>list[dict] | None</code> DEFAULT: <code>None</code> </p> <code>start_point</code> <p>Starting point for the search. Defaults to None.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>Optimization mode, either \"min\" or \"max\". Defaults to \"min\".</p> <p> TYPE: <code>list[str] | str</code> DEFAULT: <code>'min'</code> </p> <code>**kwargs</code> <p>Additional arguments for the StochasticSearcher.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>whittle/search/local_search.py</code> <pre><code>def __init__(\n    self,\n    config_space: dict[str, Any],\n    metric: list[str] | str,\n    points_to_evaluate: list[dict] | None = None,\n    start_point: dict | None = None,\n    mode: list[str] | str = \"min\",\n    **kwargs: Any,\n):\n    if start_point is None:\n        self.start_point = {\n            k: v.sample() if isinstance(v, Domain) else v\n            for k, v in config_space.items()\n        }\n    else:\n        self.start_point = start_point\n\n    self._pareto_front: list[PopulationElement] = []\n\n    if points_to_evaluate is None:\n        points_to_evaluate = [self.start_point]\n    else:\n        points_to_evaluate.append(self.start_point)\n\n    super().__init__(\n        config_space,\n        metric,\n        mode=mode,\n        points_to_evaluate=points_to_evaluate,\n        **kwargs,\n    )\n    if isinstance(self._mode, list):\n        self._metric_op: dict[str, Any] = {\n            metric: 1 if mode == \"min\" else -1\n            for metric, mode in zip(metric, self._mode)\n        }\n    else:\n        if self._mode == \"min\":\n            self._metric_op = dict(zip(self._metric, [1.0] * len(self._metric)))\n        elif self._mode == \"max\":\n            self._metric_op = dict(zip(self._metric, [-1.0] * len(self._metric)))\n</code></pre>"},{"location":"api/whittle/search/local_search/#whittle.search.local_search.PopulationElement","title":"PopulationElement  <code>dataclass</code>","text":"<pre><code>PopulationElement(\n    trial_id: int, config: dict, result: dict\n)\n</code></pre> <p>Internal PBT state tracked per-trial.</p>"},{"location":"api/whittle/search/multi_objective/","title":"Multi objective","text":""},{"location":"api/whittle/search/multi_objective/#whittle.search.multi_objective","title":"whittle.search.multi_objective","text":""},{"location":"api/whittle/search/multi_objective/#whittle.search.multi_objective.get_pareto_optimal","title":"get_pareto_optimal","text":"<pre><code>get_pareto_optimal(costs: ndarray) -&gt; NDArray[bool_]\n</code></pre> <p>Find the pareto-optimal point.</p> PARAMETER DESCRIPTION <code>costs</code> <p>array containing the costs for each objective asscoiated with each point (n_points, 2).</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>NDArray[bool_]</code> <p>(n_points, 1) indicator if point is on pareto front or not.</p> Source code in <code>whittle/search/multi_objective.py</code> <pre><code>def get_pareto_optimal(costs: np.ndarray) -&gt; npt.NDArray[np.bool_]:\n    \"\"\"\n    Find the pareto-optimal point.\n\n    Args:\n        costs: array containing the costs for each objective asscoiated with each point (n_points, 2).\n\n    Returns:\n        (n_points, 1) indicator if point is on pareto front or not.\n\n    \"\"\"\n    assert isinstance(costs, np.ndarray)\n    assert costs.ndim == 2\n\n    # first assume all points are pareto optimal\n    is_pareto = np.ones(costs.shape[0], dtype=bool)\n    for i, c in enumerate(costs):\n        if is_pareto[i]:\n            # determine all points that have a smaller cost\n            all_with_lower_costs = np.any(costs &lt; c, axis=1)\n            keep_on_front = np.logical_and(all_with_lower_costs, is_pareto)\n            is_pareto = keep_on_front\n            is_pareto[i] = True  # keep self\n    return is_pareto\n</code></pre>"},{"location":"api/whittle/search/search/","title":"Search","text":""},{"location":"api/whittle/search/search/#whittle.search.search","title":"whittle.search.search","text":""},{"location":"api/whittle/search/search/#whittle.search.search.multi_objective_search","title":"multi_objective_search","text":"<pre><code>multi_objective_search(\n    objective: Callable[..., Any],\n    search_space: dict,\n    search_strategy: str = \"random_search\",\n    num_samples: int = 100,\n    objective_kwargs: dict[str, Any] | None = None,\n    seed: int | None = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Search for the Pareto-optimal sub-networks using the specified strategy.</p> PARAMETER DESCRIPTION <code>objective</code> <p>The objective function to optimize.</p> <p> TYPE: <code>Callable[..., Any]</code> </p> <code>search_space</code> <p>The search space for the optimization.</p> <p> TYPE: <code>dict</code> </p> <code>search_strategy</code> <p>The search strategy to use. Defaults to \"random_search\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'random_search'</code> </p> <code>num_samples</code> <p>The number of samples to evaluate. Defaults to 100.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>objective_kwargs</code> <p>Keyword arguments for the objective function. Defaults to None.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The random seed for reproducibility. Defaults to None.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>The results of the search, including Pareto-optimal solutions.</p> Source code in <code>whittle/search/search.py</code> <pre><code>def multi_objective_search(\n    objective: Callable[..., Any],\n    search_space: dict,\n    search_strategy: str = \"random_search\",\n    num_samples: int = 100,\n    objective_kwargs: dict[str, Any] | None = None,\n    seed: int | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Search for the Pareto-optimal sub-networks using the specified strategy.\n\n    Args:\n        objective: The objective function to optimize.\n        search_space: The search space for the optimization.\n        search_strategy: The search strategy to use.\n            Defaults to \"random_search\".\n        num_samples: The number of samples to evaluate.\n            Defaults to 100.\n        objective_kwargs: Keyword arguments for the objective function.\n            Defaults to None.\n        seed: The random seed for reproducibility.\n            Defaults to None.\n\n    Returns:\n        The results of the search, including Pareto-optimal solutions.\n\n    \"\"\"\n    metrics = [\"objective_1\", \"objective_2\"]\n    if seed is None:\n        seed = np.random.randint(0, 1000000)\n\n    assert search_strategy in methods\n\n    base_scheduler = methods[search_strategy](\n        MethodArguments(\n            config_space=search_space,\n            metrics=metrics,\n            mode=[\"min\", \"min\"],\n            random_seed=seed,\n        )\n    )\n\n    scheduler = AskTellScheduler(base_scheduler=base_scheduler)\n\n    costs = np.empty((num_samples, 2))\n    runtime = []\n    configs = []\n    start_time = time.time()\n    for i in range(num_samples):\n        trial_suggestion = scheduler.ask()\n        objective_1, objective_2 = objective(\n            trial_suggestion.config, **(objective_kwargs or {})\n        )\n\n        scheduler.tell(\n            trial_suggestion, {\"objective_1\": objective_1, \"objective_2\": objective_2}\n        )\n\n        # bookkeeping\n        costs[i][0] = float(objective_1)\n        costs[i][1] = float(objective_2)\n        configs.append(trial_suggestion.config)\n\n        runtime.append(time.time() - start_time)\n        print(\n            f\"iteration {i}: objective_1={objective_1} ; objective_2={objective_2}; runtime = {runtime[-1]}\"\n        )\n    idx = get_pareto_optimal(costs)\n\n    results = {\n        \"costs\": costs,\n        \"configs\": configs,\n        \"runtime\": runtime,\n        \"is_pareto_optimal\": [bool(i) for i in idx],\n    }\n    return results\n</code></pre>"},{"location":"api/whittle/training_strategies/ats/","title":"Ats","text":""},{"location":"api/whittle/training_strategies/ats/#whittle.training_strategies.ats","title":"whittle.training_strategies.ats","text":""},{"location":"api/whittle/training_strategies/ats/#whittle.training_strategies.ats.ATS","title":"ATS","text":"<pre><code>ATS(random_samples: int = 1, **kwargs: Any)\n</code></pre> <p>               Bases: <code>BaseTrainingStrategy</code></p> <p>ATS strategy.</p> <p>Follows the approach by Mohtashami et al. and updates a set of randomly sampled sub-networks if if the current step is even, otherwise it updates the super-network.</p> refs <p>Masked Training of Neural Networks with Partial Gradients Amirkeivan Mohtashami, Martin Jaggi, Sebastian Stich Proceedings of The 25th International Conference on Artificial Intelligence and Statistics arxiv.org/abs/2106.08895</p> PARAMETER DESCRIPTION <code>random_samples</code> <p>the number of randomly sampled sub-networks to sample and update in each step</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>**kwargs</code> <p>kwargs of <code>BaseTrainingStrategy</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>whittle/training_strategies/ats.py</code> <pre><code>def __init__(self, random_samples: int = 1, **kwargs: Any):\n    \"\"\"\n    Initialises an `ATS` strategy.\n\n    Args:\n        random_samples: the number of randomly sampled sub-networks to sample and update in each step\n        **kwargs: kwargs of `BaseTrainingStrategy`\n    \"\"\"\n    super().__init__(**kwargs)\n    self.random_samples = random_samples\n    self.current_step = 0\n</code></pre>"},{"location":"api/whittle/training_strategies/ats/#whittle.training_strategies.ats.ATS.__call__","title":"__call__","text":"<pre><code>__call__(model, inputs, outputs, scale_loss=1, **kwargs)\n</code></pre> <p>Updates a set of randomly sampled sub-networks if the current step is odd. Else, it updates the super-network.</p> Source code in <code>whittle/training_strategies/ats.py</code> <pre><code>def __call__(self, model, inputs, outputs, scale_loss=1, **kwargs):\n    \"\"\"\n    Updates a set of randomly sampled sub-networks if the current step is odd. Else, it updates the\n    super-network.\n    \"\"\"\n    total_loss = 0\n    y_supernet = model(inputs)\n    if self.current_step % 2 == 0:\n        # update random sub-networks\n        for i in range(self.random_samples):\n            config = self.sampler.sample()\n            model.select_sub_network(config)\n            y_hat = model(inputs)\n            if self.kd_loss is not None:\n                loss = self.kd_loss(y_hat, outputs, y_supernet)\n            else:\n                loss = self.loss_function(y_hat, outputs)\n            loss *= scale_loss\n            loss.backward() if self.fabric is None else self.fabric.backward(loss)\n            model.reset_super_network()\n\n            total_loss += loss.item()\n    else:\n        y_hat = model(inputs)\n        if self.kd_loss is not None:\n            loss = self.kd_loss(y_hat, outputs, y_supernet)\n        else:\n            loss = self.loss_function(y_hat, outputs)\n        loss *= scale_loss\n        loss.backward() if self.fabric is None else self.fabric.backward(loss)\n        total_loss = loss.item()\n    self.current_step += 1\n    return total_loss\n</code></pre>"},{"location":"api/whittle/training_strategies/base_strategy/","title":"Base strategy","text":""},{"location":"api/whittle/training_strategies/base_strategy/#whittle.training_strategies.base_strategy","title":"whittle.training_strategies.base_strategy","text":""},{"location":"api/whittle/training_strategies/base_strategy/#whittle.training_strategies.base_strategy.BaseTrainingStrategy","title":"BaseTrainingStrategy","text":"<pre><code>BaseTrainingStrategy(\n    sampler: RandomSampler,\n    loss_function: Callable,\n    kd_loss: Callable | None = None,\n    device: str = \"cuda\",\n    fabric: Fabric = None,\n    **kwargs\n)\n</code></pre> <p>Base Training Strategy.</p> <p>Base class that all training strategies inherit from.</p> <pre><code>sampler: sampler that returns a sub-network when called\nloss_function: loss function to compute the loss of a sub-network\ndevice: device to run the model on\n**kwargs:\n</code></pre> Source code in <code>whittle/training_strategies/base_strategy.py</code> <pre><code>def __init__(\n    self,\n    sampler: RandomSampler,\n    loss_function: Callable,\n    kd_loss: Callable | None = None,\n    device: str = \"cuda\",\n    fabric: Fabric = None,\n    **kwargs,\n):\n    \"\"\"\n    Initialises a `BaseTrainingStrategy`\n    Args:\n        sampler: sampler that returns a sub-network when called\n        loss_function: loss function to compute the loss of a sub-network\n        device: device to run the model on\n        **kwargs:\n    \"\"\"\n    self.sampler = sampler\n    self.loss_function = loss_function\n    self.device = device\n    self.kd_loss = kd_loss\n    self.fabric = fabric\n    if isinstance(self.kd_loss, DistillLoss):\n        if not isinstance(loss_function, torch.nn.CrossEntropyLoss):\n            raise TypeError(\n                \"KD Loss not yet supported: Expected torch.nn.CrossEntropyLoss\"\n            )\n</code></pre>"},{"location":"api/whittle/training_strategies/random/","title":"Random","text":""},{"location":"api/whittle/training_strategies/random/#whittle.training_strategies.random","title":"whittle.training_strategies.random","text":""},{"location":"api/whittle/training_strategies/random/#whittle.training_strategies.random.RandomStrategy","title":"RandomStrategy","text":"<pre><code>RandomStrategy(random_samples: int = 1, **kwargs: Any)\n</code></pre> <p>               Bases: <code>BaseTrainingStrategy</code></p> <p>Random strategy.</p> <p>Randomly samples and updates <code>random_samples</code> sub-networks in each step.</p> PARAMETER DESCRIPTION <code>random_samples</code> <p>the number of randomly sampled sub-networks to sample and update in each step</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>**kwargs</code> <p>kwargs of <code>BaseTrainingStrategy</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>whittle/training_strategies/random.py</code> <pre><code>def __init__(self, random_samples: int = 1, **kwargs: Any):\n    \"\"\"\n    Initialises a `RandomStrategy`\n\n    Args:\n        random_samples: the number of randomly sampled sub-networks to sample and update in each step\n        **kwargs: kwargs of `BaseTrainingStrategy`\n    \"\"\"\n    super().__init__(**kwargs)\n    self.random_samples = random_samples\n</code></pre>"},{"location":"api/whittle/training_strategies/random/#whittle.training_strategies.random.RandomStrategy.__call__","title":"__call__","text":"<pre><code>__call__(model, inputs, outputs, scale_loss=1, **kwargs)\n</code></pre> <p>Updates randomly sampled sub-networks in each step.</p> Source code in <code>whittle/training_strategies/random.py</code> <pre><code>def __call__(self, model, inputs, outputs, scale_loss=1, **kwargs):\n    \"\"\"Updates randomly sampled sub-networks in each step.\"\"\"\n    total_loss = 0\n    y_supernet = model(inputs)\n    for i in range(self.random_samples):\n        config = self.sampler.sample()\n        model.select_sub_network(config)\n        y_hat = model(inputs)\n        if self.kd_loss is not None:\n            loss = self.kd_loss(y_hat, outputs, y_supernet)\n        else:\n            loss = self.loss_function(y_hat, outputs)\n        loss *= scale_loss\n        loss.backward() if self.fabric is None else self.fabric.backward(loss)\n        model.reset_super_network()\n\n        total_loss += loss.item()\n    return total_loss\n</code></pre>"},{"location":"api/whittle/training_strategies/random_linear/","title":"Random linear","text":""},{"location":"api/whittle/training_strategies/random_linear/#whittle.training_strategies.random_linear","title":"whittle.training_strategies.random_linear","text":""},{"location":"api/whittle/training_strategies/random_linear/#whittle.training_strategies.random_linear.RandomLinearStrategy","title":"RandomLinearStrategy","text":"<pre><code>RandomLinearStrategy(\n    total_number_of_steps: int,\n    random_samples: int = 1,\n    **kwargs: Any\n)\n</code></pre> <p>               Bases: <code>BaseTrainingStrategy</code></p> <p>Random linear strategy.</p> <p>Updates <code>random_samples</code> randomly sampled sub-network with probability <code>p</code> or the super-network with <code>1 - p</code>. <code>p</code> linearly increases with the step count.</p> refs <p>Structural Pruning of Pre-trained Language Models via Neural Architecture Search Aaron Klein, Jacek Golebiowski, Xingchen Ma, Valerio Perrone, Cedric Archambeau arxiv.org/abs/2405.02267</p> <p>Understanding and Simplifying One-Shot Architecture Search Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, Quoc Le International Conference on Machine Learning (ICML) 2018 proceedings.mlr.press/v80/bender18a/bender18a.pdf</p> PARAMETER DESCRIPTION <code>total_number_of_steps</code> <p>the number of steps the optimization runs for</p> <p> TYPE: <code>int</code> </p> <code>random_samples</code> <p>the number of randomly sampled sub-networks to sample and update in each step</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>**kwargs</code> <p>kwargs of <code>BaseTrainingStrategy</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>whittle/training_strategies/random_linear.py</code> <pre><code>def __init__(\n    self, total_number_of_steps: int, random_samples: int = 1, **kwargs: Any\n):\n    \"\"\"\n    Initialises a `RandomLinearStrategy`\n\n    Args:\n        total_number_of_steps: the number of steps the optimization runs for\n        random_samples: the number of randomly sampled sub-networks to sample and update in each step\n        **kwargs: kwargs of `BaseTrainingStrategy`\n    \"\"\"\n    super().__init__(**kwargs)\n    self.random_samples = random_samples\n    self.total_number_of_steps = total_number_of_steps\n    self.current_step = 0\n    self.rate = np.linspace(0.0, 1, total_number_of_steps)\n</code></pre>"},{"location":"api/whittle/training_strategies/sandwich/","title":"Sandwich","text":""},{"location":"api/whittle/training_strategies/sandwich/#whittle.training_strategies.sandwich","title":"whittle.training_strategies.sandwich","text":""},{"location":"api/whittle/training_strategies/sandwich/#whittle.training_strategies.sandwich.SandwichStrategy","title":"SandwichStrategy","text":"<pre><code>SandwichStrategy(random_samples: int = 2, **kwargs: Any)\n</code></pre> <p>               Bases: <code>BaseTrainingStrategy</code></p> <p>Sandwich strategy.</p> <p>In each step, the sandwich strategy updates the super-network, the smallest, and a set of randomly sampled sub-networks.</p> refs <p>Universally Slimmable Networks and Improved Training Techniques Jiahui Yu, Thomas Huang International Conference on Computer Vision 2019 arxiv.org/abs/1903.05134</p> PARAMETER DESCRIPTION <code>random_samples</code> <p>the number of randomly sampled sub-networks to sample and update in each step</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>**kwargs</code> <p>kwargs of <code>BaseTrainingStrategy</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>whittle/training_strategies/sandwich.py</code> <pre><code>def __init__(self, random_samples: int = 2, **kwargs: Any):\n    \"\"\"\n    Initialises a `SandwichStrategy`\n\n    Args:\n        random_samples: the number of randomly sampled sub-networks to sample and update in each step\n        **kwargs: kwargs of `BaseTrainingStrategy`\n    \"\"\"\n    super().__init__(**kwargs)\n    self.random_samples = random_samples\n</code></pre>"},{"location":"api/whittle/training_strategies/standard/","title":"Standard","text":""},{"location":"api/whittle/training_strategies/standard/#whittle.training_strategies.standard","title":"whittle.training_strategies.standard","text":""},{"location":"api/whittle/training_strategies/standard/#whittle.training_strategies.standard.StandardStrategy","title":"StandardStrategy","text":"<pre><code>StandardStrategy(**kwargs: Any)\n</code></pre> <p>               Bases: <code>BaseTrainingStrategy</code></p> <p>Standard strategy.</p> <p>Implements the standard update rule and updates all weights of the super-network.</p> PARAMETER DESCRIPTION <code>**kwargs</code> <p>kwargs of <code>BaseTrainingStrategy</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>whittle/training_strategies/standard.py</code> <pre><code>def __init__(self, **kwargs: Any):\n    \"\"\"\n    Initialises a `StandardStrategy`\n\n    Args:\n        **kwargs: kwargs of `BaseTrainingStrategy`\n    \"\"\"\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/whittle/tutorials/gpt_utils/","title":"Gpt utils","text":""},{"location":"api/whittle/tutorials/gpt_utils/#whittle.tutorials.gpt_utils","title":"whittle.tutorials.gpt_utils","text":""},{"location":"api/whittle/tutorials/gpt_utils/#whittle.tutorials.gpt_utils.estimate_loss","title":"estimate_loss","text":"<pre><code>estimate_loss(model: Module, eval_iters: int)\n</code></pre> <p>Function to evaluate the model on train &amp; valid splits.</p> Source code in <code>whittle/tutorials/gpt_utils.py</code> <pre><code>@torch.no_grad()\ndef estimate_loss(model: nn.Module, eval_iters: int):\n    \"\"\"Function to evaluate the model on train &amp; valid splits.\"\"\"\n    out = {}\n    model.eval()\n    for split in [\"train\", \"valid\"]:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            B, T = X.shape\n            # evaluate loss on the batch\n            logits = model(X)\n            logits = logits.view(B * T, -1)\n            targets = Y.view(B * T)\n            loss = F.cross_entropy(logits, targets)\n            # logits = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n</code></pre>"},{"location":"api/whittle/tutorials/gpt_utils/#whittle.tutorials.gpt_utils.get_batch","title":"get_batch","text":"<pre><code>get_batch(\n    split: str,\n    block_size: int = 8,\n    batch_size: int = 4,\n    device: str = \"cuda\",\n)\n</code></pre> <p>Gets a randomized batch from the split of data chosen.</p>"},{"location":"api/whittle/tutorials/gpt_utils/#whittle.tutorials.gpt_utils.get_batch--arguments","title":"Arguments","text":"<p>split : str, {\"train\", \"valid\"} block_size : int     The context length for predictions, that is, a sentence length batch_size : int     The batch size, that is, the number of sentences</p> Source code in <code>whittle/tutorials/gpt_utils.py</code> <pre><code>def get_batch(\n    split: str, block_size: int = 8, batch_size: int = 4, device: str = \"cuda\"\n):\n    \"\"\"Gets a randomized batch from the split of data chosen.\n\n    Arguments\n    ---------\n    split : str, {\"train\", \"valid\"}\n    block_size : int\n        The context length for predictions, that is, a sentence length\n    batch_size : int\n        The batch size, that is, the number of sentences\n    \"\"\"\n    # generate a small batch of data of inputs x and targets y\n    assert split in [\"train\", \"valid\"]\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    data = train_data if split == \"train\" else valid_data\n    # generating random indices as markers in the full text document\n    # such that they are a starting point to the sentence of length\n    # `block_size` that will be a data point in the batch\n    ix = torch.randint(low=0, high=len(data) - block_size, size=(batch_size,))\n    # extracting a sentence of length `block_size` for every\n    # random starting point in `ix`\n    x = torch.stack([data[i : i + block_size] for i in ix])\n    # extracting a sentence of length `block_size` for every\n    # random starting point in `ix` + 1 (shifted to right)\n    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n</code></pre>"},{"location":"api/whittle/tutorials/gpt_utils/#whittle.tutorials.gpt_utils.to_tokens","title":"to_tokens","text":"<pre><code>to_tokens(example)\n</code></pre> <p>Function to tokenize a string using BPE.</p> Source code in <code>whittle/tutorials/gpt_utils.py</code> <pre><code>def to_tokens(example):\n    \"\"\"Function to tokenize a string using BPE.\"\"\"\n    enc = tiktoken.get_encoding(\"gpt2\")\n    ids = enc.encode_ordinary(\n        example[\"text\"]\n    )  # encode_ordinary ignores any special tokens\n    ids.append(enc.eot_token)  # add the end of text token, e.g. 50256 for gpt2 bpe\n    # note: I think eot should be prepended not appended... hmm. it's called \"eot\" though...\n    out = {\"ids\": ids, \"len\": len(ids)}\n    return out\n</code></pre>"}]}