{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from litgpt.model import GPT\n",
    "\n",
    "\n",
    "from whittle.baselines.magnitude_structured import prune_magnitude\n",
    "from whittle.baselines.prune_sparsegpt import prune_sparsegpt\n",
    "from whittle.baselines.wanda_structured import prune_wanda\n",
    "\n",
    "\n",
    "class Pruner:\n",
    "    def __init__(self, args=None):\n",
    "        self.args = args\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        model: GPT,\n",
    "        prune_n=2,\n",
    "        prune_m=4,\n",
    "        prune_methode=\"magnitude\",\n",
    "        tokenizer=None,\n",
    "    ) -> float:\n",
    "        total_parameters = self.compute_parameters(model)\n",
    "        self.prune(\n",
    "            model, prune_n, prune_m, prune_methode=prune_methode, tokenizer=tokenizer\n",
    "        )\n",
    "        return self.count_sparse_parameters(model) / total_parameters\n",
    "\n",
    "    def prune(\n",
    "        self,\n",
    "        model: GPT,\n",
    "        prune_n=2,\n",
    "        prune_m=4,\n",
    "        prune_methode=\"magnitude\",\n",
    "        tokenizer=None,\n",
    "        dev=\"cpu\",\n",
    "    ):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def count_sparse_parameters(model: GPT):\n",
    "        params = 0\n",
    "        for _, p in model.named_parameters():\n",
    "            params += torch.sum(p.data != 0).item()\n",
    "        return float(params)\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_parameters(model: GPT):\n",
    "        return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "class NMPruner(Pruner):\n",
    "    def prune(\n",
    "        self,\n",
    "        model: GPT,\n",
    "        prune_n=2,\n",
    "        prune_m=4,\n",
    "        prune_methode=\"magnitude\",\n",
    "        tokenizer=None,\n",
    "        dev=\"cpu\",\n",
    "    ):\n",
    "        print(f\"Pruning {prune_n} out of {prune_m} weights per group\")\n",
    "        print(f\"Pruning method: {prune_methode}\")\n",
    "\n",
    "        if prune_methode == \"magnitude\":\n",
    "            prune_magnitude(model, prune_n, prune_m)\n",
    "\n",
    "        elif prune_methode == \"wanda\":\n",
    "            prune_wanda(self.args, model, tokenizer, device=dev, prune_n=2, prune_m=4)\n",
    "\n",
    "        elif prune_methode == \"sparse\":\n",
    "            prune_sparsegpt(self.args, model, tokenizer, dev=dev, prune_n=2, prune_m=4)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                f\"Pruning method {prune_methode} not implemented!\"\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whittle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
