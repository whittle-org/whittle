============================= test session starts ==============================
platform linux -- Python 3.11.11, pytest-8.3.4, pluggy-1.5.0
rootdir: /work/dlclarge1/sukthank-dense-lth/whittle_lora_attention/whittle
configfile: pyproject.toml
collected 24 items

test/test_lora_qkv_linear.py .FF...FF..FFFFFFFFFFFFFF                    [100%]

=================================== FAILURES ===================================
_________________________ test_zero_pad[mha_enable_qv] _________________________

qkv_config = 'mha_enable_qv'

    @pytest.mark.parametrize("qkv_config", lora_qkv_configs.keys())
    def test_zero_pad(qkv_config):
        config = lora_qkv_configs[qkv_config]
        seq_length, batch_size = 4, 2
        in_features, n_head, head_size, r = 128, 16, 2, 8
        enable_lora = config["enable_lora"]
        n_query_groups = config["n_query_groups"]
        out_features = (n_head + 2 * n_query_groups) * head_size
    
        lora_config = LoRAConfig(
            n_embd=out_features,
            n_head=n_head,
            n_query_groups=n_query_groups,
            head_size=head_size,
        )
        lora_config.fix_head_size = True
    
        qkv = LoRAQKVLinear(
            lora_config,
            in_features,
            out_features,
            head_size,
            n_head,
            n_query_groups,
            True,
            r=r,
            enable_lora=enable_lora,
        )
    
        q_shape = (batch_size, seq_length, n_head * head_size)
        k_shape = (batch_size, seq_length, n_query_groups * head_size)
        v_shape = (batch_size, seq_length, n_query_groups * head_size)
        # Init weights so that we know a) what's the original position in the pre-shuffle matrix
        # b) which one is from q, k or v
        qkv_weights = []
        for j, (i, shape) in enumerate(zip([1, 1000, 10000], [q_shape, k_shape, v_shape])):
            if enable_lora[j]:
                qkv_weights.append((i + torch.arange(torch.prod(torch.tensor(shape)).item())).reshape(shape))
        result = qkv.zero_pad(qkv_weights)
        qkv_group_size = qkv.sub_network_q_per_kv + 2
    
        # **Fix: Compute correct q_id indices**
        flat_result = result.flatten()
        q_ids = torch.arange(flat_result.shape[0])  # Corrected to match `flat_result` size
    
        group_ids = (q_ids // head_size) % qkv_group_size
        cond_q = group_ids < qkv_group_size - 2
        cond_k = group_ids == qkv_group_size - 2
        cond_v = group_ids > qkv_group_size - 2
    
        # Extract values based on conditions
        q_values, k_values, v_values = (
            flat_result[cond_q],
            flat_result[cond_k],
            flat_result[cond_v],
        )
        # now we check back the order
        qkv_idx = 0
        if enable_lora[0]:
            assert torch.all(q_values < 1000)
            assert q_values.numel() == qkv_weights[qkv_idx].numel()
            assert torch.allclose(q_values, qkv_weights[qkv_idx].flatten(), atol=1e-6)
            qkv_idx += 1
        else:
            assert torch.allclose(q_values, torch.zeros_like(q_values), atol=1e-6)
        if enable_lora[1]:
            assert torch.all((1000 <= k_values) & (k_values < 10000))
            assert k_values.numel() == qkv_weights[qkv_idx].numel()
            assert torch.allclose(k_values, qkv_weights[qkv_idx].flatten(), atol=1e-6)
            qkv_idx += 1
        else:
            assert torch.allclose(k_values, torch.zeros_like(k_values), atol=1e-6)
        if enable_lora[2]:
            assert torch.all(v_values >= 10000)
            assert v_values.numel() == qkv_weights[qkv_idx].numel()
            assert torch.allclose(v_values, qkv_weights[qkv_idx].flatten(), atol=1e-6)
        else:
            assert torch.allclose(v_values, torch.zeros_like(v_values), atol=1e-6)
    
        # Reshape and permute for final check
        result = result.view(
            batch_size, seq_length, n_query_groups, qkv_group_size, head_size
        )
        result = result.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
    
        q, k, v = result.split((qkv.sub_network_q_per_kv, 1, 1), dim=2)
    
        def check_qkv(post_split, weights):
            assert torch.allclose(
                weights, post_split.permute(0, 3, 1, 2, 4).flatten(), atol=1e-6
            )
    
        if enable_lora[0]:
            check_qkv(q, qkv_weights[0].flatten())
        if enable_lora[1]:
            check_qkv(k, qkv_weights[1].flatten())
        if enable_lora[2]:
>           check_qkv(v, qkv_weights[2].flatten())
E           IndexError: list index out of range

test/test_lora_qkv_linear.py:159: IndexError
_________________________ test_zero_pad[mha_enable_kv] _________________________

qkv_config = 'mha_enable_kv'

    @pytest.mark.parametrize("qkv_config", lora_qkv_configs.keys())
    def test_zero_pad(qkv_config):
        config = lora_qkv_configs[qkv_config]
        seq_length, batch_size = 4, 2
        in_features, n_head, head_size, r = 128, 16, 2, 8
        enable_lora = config["enable_lora"]
        n_query_groups = config["n_query_groups"]
        out_features = (n_head + 2 * n_query_groups) * head_size
    
        lora_config = LoRAConfig(
            n_embd=out_features,
            n_head=n_head,
            n_query_groups=n_query_groups,
            head_size=head_size,
        )
        lora_config.fix_head_size = True
    
        qkv = LoRAQKVLinear(
            lora_config,
            in_features,
            out_features,
            head_size,
            n_head,
            n_query_groups,
            True,
            r=r,
            enable_lora=enable_lora,
        )
    
        q_shape = (batch_size, seq_length, n_head * head_size)
        k_shape = (batch_size, seq_length, n_query_groups * head_size)
        v_shape = (batch_size, seq_length, n_query_groups * head_size)
        # Init weights so that we know a) what's the original position in the pre-shuffle matrix
        # b) which one is from q, k or v
        qkv_weights = []
        for j, (i, shape) in enumerate(zip([1, 1000, 10000], [q_shape, k_shape, v_shape])):
            if enable_lora[j]:
                qkv_weights.append((i + torch.arange(torch.prod(torch.tensor(shape)).item())).reshape(shape))
        result = qkv.zero_pad(qkv_weights)
        qkv_group_size = qkv.sub_network_q_per_kv + 2
    
        # **Fix: Compute correct q_id indices**
        flat_result = result.flatten()
        q_ids = torch.arange(flat_result.shape[0])  # Corrected to match `flat_result` size
    
        group_ids = (q_ids // head_size) % qkv_group_size
        cond_q = group_ids < qkv_group_size - 2
        cond_k = group_ids == qkv_group_size - 2
        cond_v = group_ids > qkv_group_size - 2
    
        # Extract values based on conditions
        q_values, k_values, v_values = (
            flat_result[cond_q],
            flat_result[cond_k],
            flat_result[cond_v],
        )
        # now we check back the order
        qkv_idx = 0
        if enable_lora[0]:
            assert torch.all(q_values < 1000)
            assert q_values.numel() == qkv_weights[qkv_idx].numel()
            assert torch.allclose(q_values, qkv_weights[qkv_idx].flatten(), atol=1e-6)
            qkv_idx += 1
        else:
            assert torch.allclose(q_values, torch.zeros_like(q_values), atol=1e-6)
        if enable_lora[1]:
            assert torch.all((1000 <= k_values) & (k_values < 10000))
            assert k_values.numel() == qkv_weights[qkv_idx].numel()
            assert torch.allclose(k_values, qkv_weights[qkv_idx].flatten(), atol=1e-6)
            qkv_idx += 1
        else:
            assert torch.allclose(k_values, torch.zeros_like(k_values), atol=1e-6)
        if enable_lora[2]:
            assert torch.all(v_values >= 10000)
            assert v_values.numel() == qkv_weights[qkv_idx].numel()
            assert torch.allclose(v_values, qkv_weights[qkv_idx].flatten(), atol=1e-6)
        else:
            assert torch.allclose(v_values, torch.zeros_like(v_values), atol=1e-6)
    
        # Reshape and permute for final check
        result = result.view(
            batch_size, seq_length, n_query_groups, qkv_group_size, head_size
        )
        result = result.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
    
        q, k, v = result.split((qkv.sub_network_q_per_kv, 1, 1), dim=2)
    
        def check_qkv(post_split, weights):
            assert torch.allclose(
                weights, post_split.permute(0, 3, 1, 2, 4).flatten(), atol=1e-6
            )
    
        if enable_lora[0]:
            check_qkv(q, qkv_weights[0].flatten())
        if enable_lora[1]:
>           check_qkv(k, qkv_weights[1].flatten())

test/test_lora_qkv_linear.py:157: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

post_split = tensor([[[[[1000, 1001],
           [1032, 1033],
           [1064, 1065],
           [1096, 1097]]],


         [[[10...252, 1253]]],


         [[[1158, 1159],
           [1190, 1191],
           [1222, 1223],
           [1254, 1255]]]]])
weights = tensor([10000, 10001, 10002, 10003, 10004, 10005, 10006, 10007, 10008, 10009,
        10010, 10011, 10012, 10013, 1001...0240, 10241, 10242, 10243, 10244, 10245, 10246, 10247, 10248, 10249,
        10250, 10251, 10252, 10253, 10254, 10255])

    def check_qkv(post_split, weights):
>       assert torch.allclose(
            weights, post_split.permute(0, 3, 1, 2, 4).flatten(), atol=1e-6
        )
E       assert False
E        +  where False = <built-in method allclose of type object at 0x7fafa1c5f240>(tensor([10000, 10001, 10002, 10003, 10004, 10005, 10006, 10007, 10008, 10009,\n        10010, 10011, 10012, 10013, 1001...0240, 10241, 10242, 10243, 10244, 10245, 10246, 10247, 10248, 10249,\n        10250, 10251, 10252, 10253, 10254, 10255]), tensor([1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011,\n        1012, 1013, 1014, 1015, 1016, ... 1239,\n        1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251,\n        1252, 1253, 1254, 1255]), atol=1e-06)
E        +    where <built-in method allclose of type object at 0x7fafa1c5f240> = torch.allclose
E        +    and   tensor([1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011,\n        1012, 1013, 1014, 1015, 1016, ... 1239,\n        1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251,\n        1252, 1253, 1254, 1255]) = <built-in method flatten of Tensor object at 0x7fae6f2a2750>()
E        +      where <built-in method flatten of Tensor object at 0x7fae6f2a2750> = tensor([[[[[1000, 1001]],\n\n          [[1002, 1003]],\n\n          [[1004, 1005]],\n\n          [[1006, 1007]],\n\n          ... 1247]],\n\n          [[1248, 1249]],\n\n          [[1250, 1251]],\n\n          [[1252, 1253]],\n\n          [[1254, 1255]]]]]).flatten
E        +        where tensor([[[[[1000, 1001]],\n\n          [[1002, 1003]],\n\n          [[1004, 1005]],\n\n          [[1006, 1007]],\n\n          ... 1247]],\n\n          [[1248, 1249]],\n\n          [[1250, 1251]],\n\n          [[1252, 1253]],\n\n          [[1254, 1255]]]]]) = <built-in method permute of Tensor object at 0x7fae6f2a2450>(0, 3, 1, 2, 4)
E        +          where <built-in method permute of Tensor object at 0x7fae6f2a2450> = tensor([[[[[1000, 1001],\n           [1032, 1033],\n           [1064, 1065],\n           [1096, 1097]]],\n\n\n         [[[10...252, 1253]]],\n\n\n         [[[1158, 1159],\n           [1190, 1191],\n           [1222, 1223],\n           [1254, 1255]]]]]).permute

test/test_lora_qkv_linear.py:150: AssertionError
_________________________ test_zero_pad[gqa_enable_qv] _________________________

qkv_config = 'gqa_enable_qv'

    @pytest.mark.parametrize("qkv_config", lora_qkv_configs.keys())
    def test_zero_pad(qkv_config):
        config = lora_qkv_configs[qkv_config]
        seq_length, batch_size = 4, 2
        in_features, n_head, head_size, r = 128, 16, 2, 8
        enable_lora = config["enable_lora"]
        n_query_groups = config["n_query_groups"]
        out_features = (n_head + 2 * n_query_groups) * head_size
    
        lora_config = LoRAConfig(
            n_embd=out_features,
            n_head=n_head,
            n_query_groups=n_query_groups,
            head_size=head_size,
        )
        lora_config.fix_head_size = True
    
        qkv = LoRAQKVLinear(
            lora_config,
            in_features,
            out_features,
            head_size,
            n_head,
            n_query_groups,
            True,
            r=r,
            enable_lora=enable_lora,
        )
    
        q_shape = (batch_size, seq_length, n_head * head_size)
        k_shape = (batch_size, seq_length, n_query_groups * head_size)
        v_shape = (batch_size, seq_length, n_query_groups * head_size)
        # Init weights so that we know a) what's the original position in the pre-shuffle matrix
        # b) which one is from q, k or v
        qkv_weights = []
        for j, (i, shape) in enumerate(zip([1, 1000, 10000], [q_shape, k_shape, v_shape])):
            if enable_lora[j]:
                qkv_weights.append((i + torch.arange(torch.prod(torch.tensor(shape)).item())).reshape(shape))
        result = qkv.zero_pad(qkv_weights)
        qkv_group_size = qkv.sub_network_q_per_kv + 2
    
        # **Fix: Compute correct q_id indices**
        flat_result = result.flatten()
        q_ids = torch.arange(flat_result.shape[0])  # Corrected to match `flat_result` size
    
        group_ids = (q_ids // head_size) % qkv_group_size
        cond_q = group_ids < qkv_group_size - 2
        cond_k = group_ids == qkv_group_size - 2
        cond_v = group_ids > qkv_group_size - 2
    
        # Extract values based on conditions
        q_values, k_values, v_values = (
            flat_result[cond_q],
            flat_result[cond_k],
            flat_result[cond_v],
        )
        # now we check back the order
        qkv_idx = 0
        if enable_lora[0]:
            assert torch.all(q_values < 1000)
            assert q_values.numel() == qkv_weights[qkv_idx].numel()
            assert torch.allclose(q_values, qkv_weights[qkv_idx].flatten(), atol=1e-6)
            qkv_idx += 1
        else:
            assert torch.allclose(q_values, torch.zeros_like(q_values), atol=1e-6)
        if enable_lora[1]:
            assert torch.all((1000 <= k_values) & (k_values < 10000))
            assert k_values.numel() == qkv_weights[qkv_idx].numel()
            assert torch.allclose(k_values, qkv_weights[qkv_idx].flatten(), atol=1e-6)
            qkv_idx += 1
        else:
            assert torch.allclose(k_values, torch.zeros_like(k_values), atol=1e-6)
        if enable_lora[2]:
            assert torch.all(v_values >= 10000)
            assert v_values.numel() == qkv_weights[qkv_idx].numel()
            assert torch.allclose(v_values, qkv_weights[qkv_idx].flatten(), atol=1e-6)
        else:
            assert torch.allclose(v_values, torch.zeros_like(v_values), atol=1e-6)
    
        # Reshape and permute for final check
        result = result.view(
            batch_size, seq_length, n_query_groups, qkv_group_size, head_size
        )
        result = result.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
    
        q, k, v = result.split((qkv.sub_network_q_per_kv, 1, 1), dim=2)
    
        def check_qkv(post_split, weights):
            assert torch.allclose(
                weights, post_split.permute(0, 3, 1, 2, 4).flatten(), atol=1e-6
            )
    
        if enable_lora[0]:
            check_qkv(q, qkv_weights[0].flatten())
        if enable_lora[1]:
            check_qkv(k, qkv_weights[1].flatten())
        if enable_lora[2]:
>           check_qkv(v, qkv_weights[2].flatten())
E           IndexError: list index out of range

test/test_lora_qkv_linear.py:159: IndexError
_________________________ test_zero_pad[gqa_enable_kv] _________________________

qkv_config = 'gqa_enable_kv'

    @pytest.mark.parametrize("qkv_config", lora_qkv_configs.keys())
    def test_zero_pad(qkv_config):
        config = lora_qkv_configs[qkv_config]
        seq_length, batch_size = 4, 2
        in_features, n_head, head_size, r = 128, 16, 2, 8
        enable_lora = config["enable_lora"]
        n_query_groups = config["n_query_groups"]
        out_features = (n_head + 2 * n_query_groups) * head_size
    
        lora_config = LoRAConfig(
            n_embd=out_features,
            n_head=n_head,
            n_query_groups=n_query_groups,
            head_size=head_size,
        )
        lora_config.fix_head_size = True
    
        qkv = LoRAQKVLinear(
            lora_config,
            in_features,
            out_features,
            head_size,
            n_head,
            n_query_groups,
            True,
            r=r,
            enable_lora=enable_lora,
        )
    
        q_shape = (batch_size, seq_length, n_head * head_size)
        k_shape = (batch_size, seq_length, n_query_groups * head_size)
        v_shape = (batch_size, seq_length, n_query_groups * head_size)
        # Init weights so that we know a) what's the original position in the pre-shuffle matrix
        # b) which one is from q, k or v
        qkv_weights = []
        for j, (i, shape) in enumerate(zip([1, 1000, 10000], [q_shape, k_shape, v_shape])):
            if enable_lora[j]:
                qkv_weights.append((i + torch.arange(torch.prod(torch.tensor(shape)).item())).reshape(shape))
        result = qkv.zero_pad(qkv_weights)
        qkv_group_size = qkv.sub_network_q_per_kv + 2
    
        # **Fix: Compute correct q_id indices**
        flat_result = result.flatten()
        q_ids = torch.arange(flat_result.shape[0])  # Corrected to match `flat_result` size
    
        group_ids = (q_ids // head_size) % qkv_group_size
        cond_q = group_ids < qkv_group_size - 2
        cond_k = group_ids == qkv_group_size - 2
        cond_v = group_ids > qkv_group_size - 2
    
        # Extract values based on conditions
        q_values, k_values, v_values = (
            flat_result[cond_q],
            flat_result[cond_k],
            flat_result[cond_v],
        )
        # now we check back the order
        qkv_idx = 0
        if enable_lora[0]:
            assert torch.all(q_values < 1000)
            assert q_values.numel() == qkv_weights[qkv_idx].numel()
            assert torch.allclose(q_values, qkv_weights[qkv_idx].flatten(), atol=1e-6)
            qkv_idx += 1
        else:
            assert torch.allclose(q_values, torch.zeros_like(q_values), atol=1e-6)
        if enable_lora[1]:
            assert torch.all((1000 <= k_values) & (k_values < 10000))
            assert k_values.numel() == qkv_weights[qkv_idx].numel()
            assert torch.allclose(k_values, qkv_weights[qkv_idx].flatten(), atol=1e-6)
            qkv_idx += 1
        else:
            assert torch.allclose(k_values, torch.zeros_like(k_values), atol=1e-6)
        if enable_lora[2]:
            assert torch.all(v_values >= 10000)
            assert v_values.numel() == qkv_weights[qkv_idx].numel()
            assert torch.allclose(v_values, qkv_weights[qkv_idx].flatten(), atol=1e-6)
        else:
            assert torch.allclose(v_values, torch.zeros_like(v_values), atol=1e-6)
    
        # Reshape and permute for final check
        result = result.view(
            batch_size, seq_length, n_query_groups, qkv_group_size, head_size
        )
        result = result.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
    
        q, k, v = result.split((qkv.sub_network_q_per_kv, 1, 1), dim=2)
    
        def check_qkv(post_split, weights):
            assert torch.allclose(
                weights, post_split.permute(0, 3, 1, 2, 4).flatten(), atol=1e-6
            )
    
        if enable_lora[0]:
            check_qkv(q, qkv_weights[0].flatten())
        if enable_lora[1]:
>           check_qkv(k, qkv_weights[1].flatten())

test/test_lora_qkv_linear.py:157: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

post_split = tensor([[[[[1000, 1001],
           [1008, 1009],
           [1016, 1017],
           [1024, 1025]]],


         [[[10...060, 1061]]],


         [[[1038, 1039],
           [1046, 1047],
           [1054, 1055],
           [1062, 1063]]]]])
weights = tensor([10000, 10001, 10002, 10003, 10004, 10005, 10006, 10007, 10008, 10009,
        10010, 10011, 10012, 10013, 1001...049,
        10050, 10051, 10052, 10053, 10054, 10055, 10056, 10057, 10058, 10059,
        10060, 10061, 10062, 10063])

    def check_qkv(post_split, weights):
>       assert torch.allclose(
            weights, post_split.permute(0, 3, 1, 2, 4).flatten(), atol=1e-6
        )
E       assert False
E        +  where False = <built-in method allclose of type object at 0x7fafa1c5f240>(tensor([10000, 10001, 10002, 10003, 10004, 10005, 10006, 10007, 10008, 10009,\n        10010, 10011, 10012, 10013, 1001...049,\n        10050, 10051, 10052, 10053, 10054, 10055, 10056, 10057, 10058, 10059,\n        10060, 10061, 10062, 10063]), tensor([1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011,\n        1012, 1013, 1014, 1015, 1016, ... 1047,\n        1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059,\n        1060, 1061, 1062, 1063]), atol=1e-06)
E        +    where <built-in method allclose of type object at 0x7fafa1c5f240> = torch.allclose
E        +    and   tensor([1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011,\n        1012, 1013, 1014, 1015, 1016, ... 1047,\n        1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059,\n        1060, 1061, 1062, 1063]) = <built-in method flatten of Tensor object at 0x7fae6f0bd250>()
E        +      where <built-in method flatten of Tensor object at 0x7fae6f0bd250> = tensor([[[[[1000, 1001]],\n\n          [[1002, 1003]],\n\n          [[1004, 1005]],\n\n          [[1006, 1007]]],\n\n\n        ...055]]],\n\n\n         [[[1056, 1057]],\n\n          [[1058, 1059]],\n\n          [[1060, 1061]],\n\n          [[1062, 1063]]]]]).flatten
E        +        where tensor([[[[[1000, 1001]],\n\n          [[1002, 1003]],\n\n          [[1004, 1005]],\n\n          [[1006, 1007]]],\n\n\n        ...055]]],\n\n\n         [[[1056, 1057]],\n\n          [[1058, 1059]],\n\n          [[1060, 1061]],\n\n          [[1062, 1063]]]]]) = <built-in method permute of Tensor object at 0x7fae6f0bdf10>(0, 3, 1, 2, 4)
E        +          where <built-in method permute of Tensor object at 0x7fae6f0bdf10> = tensor([[[[[1000, 1001],\n           [1008, 1009],\n           [1016, 1017],\n           [1024, 1025]]],\n\n\n         [[[10...060, 1061]]],\n\n\n         [[[1038, 1039],\n           [1046, 1047],\n           [1054, 1055],\n           [1062, 1063]]]]]).permute

test/test_lora_qkv_linear.py:150: AssertionError
_________________________ test_zero_pad[mqa_enable_qv] _________________________

qkv_config = 'mqa_enable_qv'

    @pytest.mark.parametrize("qkv_config", lora_qkv_configs.keys())
    def test_zero_pad(qkv_config):
        config = lora_qkv_configs[qkv_config]
        seq_length, batch_size = 4, 2
        in_features, n_head, head_size, r = 128, 16, 2, 8
        enable_lora = config["enable_lora"]
        n_query_groups = config["n_query_groups"]
        out_features = (n_head + 2 * n_query_groups) * head_size
    
        lora_config = LoRAConfig(
            n_embd=out_features,
            n_head=n_head,
            n_query_groups=n_query_groups,
            head_size=head_size,
        )
        lora_config.fix_head_size = True
    
        qkv = LoRAQKVLinear(
            lora_config,
            in_features,
            out_features,
            head_size,
            n_head,
            n_query_groups,
            True,
            r=r,
            enable_lora=enable_lora,
        )
    
        q_shape = (batch_size, seq_length, n_head * head_size)
        k_shape = (batch_size, seq_length, n_query_groups * head_size)
        v_shape = (batch_size, seq_length, n_query_groups * head_size)
        # Init weights so that we know a) what's the original position in the pre-shuffle matrix
        # b) which one is from q, k or v
        qkv_weights = []
        for j, (i, shape) in enumerate(zip([1, 1000, 10000], [q_shape, k_shape, v_shape])):
            if enable_lora[j]:
                qkv_weights.append((i + torch.arange(torch.prod(torch.tensor(shape)).item())).reshape(shape))
        result = qkv.zero_pad(qkv_weights)
        qkv_group_size = qkv.sub_network_q_per_kv + 2
    
        # **Fix: Compute correct q_id indices**
        flat_result = result.flatten()
        q_ids = torch.arange(flat_result.shape[0])  # Corrected to match `flat_result` size
    
        group_ids = (q_ids // head_size) % qkv_group_size
        cond_q = group_ids < qkv_group_size - 2
        cond_k = group_ids == qkv_group_size - 2
        cond_v = group_ids > qkv_group_size - 2
    
        # Extract values based on conditions
        q_values, k_values, v_values = (
            flat_result[cond_q],
            flat_result[cond_k],
            flat_result[cond_v],
        )
        # now we check back the order
        qkv_idx = 0
        if enable_lora[0]:
            assert torch.all(q_values < 1000)
            assert q_values.numel() == qkv_weights[qkv_idx].numel()
            assert torch.allclose(q_values, qkv_weights[qkv_idx].flatten(), atol=1e-6)
            qkv_idx += 1
        else:
            assert torch.allclose(q_values, torch.zeros_like(q_values), atol=1e-6)
        if enable_lora[1]:
            assert torch.all((1000 <= k_values) & (k_values < 10000))
            assert k_values.numel() == qkv_weights[qkv_idx].numel()
            assert torch.allclose(k_values, qkv_weights[qkv_idx].flatten(), atol=1e-6)
            qkv_idx += 1
        else:
            assert torch.allclose(k_values, torch.zeros_like(k_values), atol=1e-6)
        if enable_lora[2]:
            assert torch.all(v_values >= 10000)
            assert v_values.numel() == qkv_weights[qkv_idx].numel()
            assert torch.allclose(v_values, qkv_weights[qkv_idx].flatten(), atol=1e-6)
        else:
            assert torch.allclose(v_values, torch.zeros_like(v_values), atol=1e-6)
    
        # Reshape and permute for final check
        result = result.view(
            batch_size, seq_length, n_query_groups, qkv_group_size, head_size
        )
        result = result.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
    
        q, k, v = result.split((qkv.sub_network_q_per_kv, 1, 1), dim=2)
    
        def check_qkv(post_split, weights):
            assert torch.allclose(
                weights, post_split.permute(0, 3, 1, 2, 4).flatten(), atol=1e-6
            )
    
        if enable_lora[0]:
            check_qkv(q, qkv_weights[0].flatten())
        if enable_lora[1]:
            check_qkv(k, qkv_weights[1].flatten())
        if enable_lora[2]:
>           check_qkv(v, qkv_weights[2].flatten())
E           IndexError: list index out of range

test/test_lora_qkv_linear.py:159: IndexError
_________________________ test_zero_pad[mqa_enable_kv] _________________________

qkv_config = 'mqa_enable_kv'

    @pytest.mark.parametrize("qkv_config", lora_qkv_configs.keys())
    def test_zero_pad(qkv_config):
        config = lora_qkv_configs[qkv_config]
        seq_length, batch_size = 4, 2
        in_features, n_head, head_size, r = 128, 16, 2, 8
        enable_lora = config["enable_lora"]
        n_query_groups = config["n_query_groups"]
        out_features = (n_head + 2 * n_query_groups) * head_size
    
        lora_config = LoRAConfig(
            n_embd=out_features,
            n_head=n_head,
            n_query_groups=n_query_groups,
            head_size=head_size,
        )
        lora_config.fix_head_size = True
    
        qkv = LoRAQKVLinear(
            lora_config,
            in_features,
            out_features,
            head_size,
            n_head,
            n_query_groups,
            True,
            r=r,
            enable_lora=enable_lora,
        )
    
        q_shape = (batch_size, seq_length, n_head * head_size)
        k_shape = (batch_size, seq_length, n_query_groups * head_size)
        v_shape = (batch_size, seq_length, n_query_groups * head_size)
        # Init weights so that we know a) what's the original position in the pre-shuffle matrix
        # b) which one is from q, k or v
        qkv_weights = []
        for j, (i, shape) in enumerate(zip([1, 1000, 10000], [q_shape, k_shape, v_shape])):
            if enable_lora[j]:
                qkv_weights.append((i + torch.arange(torch.prod(torch.tensor(shape)).item())).reshape(shape))
        result = qkv.zero_pad(qkv_weights)
        qkv_group_size = qkv.sub_network_q_per_kv + 2
    
        # **Fix: Compute correct q_id indices**
        flat_result = result.flatten()
        q_ids = torch.arange(flat_result.shape[0])  # Corrected to match `flat_result` size
    
        group_ids = (q_ids // head_size) % qkv_group_size
        cond_q = group_ids < qkv_group_size - 2
        cond_k = group_ids == qkv_group_size - 2
        cond_v = group_ids > qkv_group_size - 2
    
        # Extract values based on conditions
        q_values, k_values, v_values = (
            flat_result[cond_q],
            flat_result[cond_k],
            flat_result[cond_v],
        )
        # now we check back the order
        qkv_idx = 0
        if enable_lora[0]:
            assert torch.all(q_values < 1000)
            assert q_values.numel() == qkv_weights[qkv_idx].numel()
            assert torch.allclose(q_values, qkv_weights[qkv_idx].flatten(), atol=1e-6)
            qkv_idx += 1
        else:
            assert torch.allclose(q_values, torch.zeros_like(q_values), atol=1e-6)
        if enable_lora[1]:
            assert torch.all((1000 <= k_values) & (k_values < 10000))
            assert k_values.numel() == qkv_weights[qkv_idx].numel()
            assert torch.allclose(k_values, qkv_weights[qkv_idx].flatten(), atol=1e-6)
            qkv_idx += 1
        else:
            assert torch.allclose(k_values, torch.zeros_like(k_values), atol=1e-6)
        if enable_lora[2]:
            assert torch.all(v_values >= 10000)
            assert v_values.numel() == qkv_weights[qkv_idx].numel()
            assert torch.allclose(v_values, qkv_weights[qkv_idx].flatten(), atol=1e-6)
        else:
            assert torch.allclose(v_values, torch.zeros_like(v_values), atol=1e-6)
    
        # Reshape and permute for final check
        result = result.view(
            batch_size, seq_length, n_query_groups, qkv_group_size, head_size
        )
        result = result.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)
    
        q, k, v = result.split((qkv.sub_network_q_per_kv, 1, 1), dim=2)
    
        def check_qkv(post_split, weights):
            assert torch.allclose(
                weights, post_split.permute(0, 3, 1, 2, 4).flatten(), atol=1e-6
            )
    
        if enable_lora[0]:
            check_qkv(q, qkv_weights[0].flatten())
        if enable_lora[1]:
>           check_qkv(k, qkv_weights[1].flatten())

test/test_lora_qkv_linear.py:157: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

post_split = tensor([[[[[1000, 1001],
           [1002, 1003],
           [1004, 1005],
           [1006, 1007]]]],



        [[[[1008, 1009],
           [1010, 1011],
           [1012, 1013],
           [1014, 1015]]]]])
weights = tensor([10000, 10001, 10002, 10003, 10004, 10005, 10006, 10007, 10008, 10009,
        10010, 10011, 10012, 10013, 10014, 10015])

    def check_qkv(post_split, weights):
>       assert torch.allclose(
            weights, post_split.permute(0, 3, 1, 2, 4).flatten(), atol=1e-6
        )
E       assert False
E        +  where False = <built-in method allclose of type object at 0x7fafa1c5f240>(tensor([10000, 10001, 10002, 10003, 10004, 10005, 10006, 10007, 10008, 10009,\n        10010, 10011, 10012, 10013, 10014, 10015]), tensor([1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011,\n        1012, 1013, 1014, 1015]), atol=1e-06)
E        +    where <built-in method allclose of type object at 0x7fafa1c5f240> = torch.allclose
E        +    and   tensor([1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011,\n        1012, 1013, 1014, 1015]) = <built-in method flatten of Tensor object at 0x7fae6f0bfb30>()
E        +      where <built-in method flatten of Tensor object at 0x7fae6f0bfb30> = tensor([[[[[1000, 1001]]],\n\n\n         [[[1002, 1003]]],\n\n\n         [[[1004, 1005]]],\n\n\n         [[[1006, 1007]]]],\n\n\n\n        [[[[1008, 1009]]],\n\n\n         [[[1010, 1011]]],\n\n\n         [[[1012, 1013]]],\n\n\n         [[[1014, 1015]]]]]).flatten
E        +        where tensor([[[[[1000, 1001]]],\n\n\n         [[[1002, 1003]]],\n\n\n         [[[1004, 1005]]],\n\n\n         [[[1006, 1007]]]],\n\n\n\n        [[[[1008, 1009]]],\n\n\n         [[[1010, 1011]]],\n\n\n         [[[1012, 1013]]],\n\n\n         [[[1014, 1015]]]]]) = <built-in method permute of Tensor object at 0x7fae6f0bfad0>(0, 3, 1, 2, 4)
E        +          where <built-in method permute of Tensor object at 0x7fae6f0bfad0> = tensor([[[[[1000, 1001],\n           [1002, 1003],\n           [1004, 1005],\n           [1006, 1007]]]],\n\n\n\n        [[[[1008, 1009],\n           [1010, 1011],\n           [1012, 1013],\n           [1014, 1015]]]]]).permute

test/test_lora_qkv_linear.py:150: AssertionError
___________________ test_zero_pad_sub_network[mha_enable_qk] ___________________

qkv_config = 'mha_enable_qk'

    @pytest.mark.parametrize("qkv_config", lora_qkv_configs.keys())
    def test_zero_pad_sub_network(qkv_config):
        config = lora_qkv_configs[qkv_config]
        seq_length, batch_size = 4, 2
        in_features, n_head, head_size, r = 128, 16, 2, 8
        enable_lora = config["enable_lora"]
        n_query_groups = config["n_query_groups"]
    
        out_features = (n_head + 2 * n_query_groups) * head_size
    
        lora_config = LoRAConfig(
            n_embd=out_features,
            n_head=n_head,
            n_query_groups=n_query_groups,
            head_size=head_size,
        )
        lora_config.fix_head_size = True
    
        qkv = LoRAQKVLinear(
            lora_config,
            in_features,
            out_features,
            head_size,
            n_head,
            n_query_groups,
            True,
            r=r,
            enable_lora=enable_lora,
        )
        qkv.reset_super_network()
    
        # determine sub-network configuration
        if n_query_groups == 1:  # MQA
            # super-network has 16 heads and 1 group
            sub_n_query_groups = 1
            sub_n_head = 8
        elif n_query_groups == n_head:  # MHA
            # super-network has 16 heads in 16 groups
            sub_n_query_groups = 8
            sub_n_head = 8
        else:  # GQA
            # super-network has 16 heads in 4 groups (4 heads per group)
            sub_n_query_groups = 2
            sub_n_head = 8  # 8/16 heads, 2/4 active groups (resulting into 2 heads per group)
    
        attn = CausalSelfAttention(lora_config, 0)
        attn.set_sub_network(lora_config.n_embd, sub_n_head, sub_n_query_groups, head_size)
        qkv_indices = attn.qkv_indices
        qkv.set_sub_network(
            in_features,
            (attn.sub_network_q_per_kv + 2) * head_size * sub_n_query_groups,
            qkv_indices,
            sub_network_query_groups=sub_n_query_groups,
            sub_network_n_head=sub_n_head,
            sub_network_head_size=head_size,
            sub_network_q_per_kv=attn.sub_network_q_per_kv,
        )
    
        q_shape = (
            batch_size,
            seq_length,
            qkv.sub_network_q_per_kv * sub_n_query_groups * head_size,
        )
        k_shape = (batch_size, seq_length, sub_n_query_groups * head_size)
        v_shape = (batch_size, seq_length, sub_n_query_groups * head_size)
    
        # **Generate sequential weights for easy tracking**
        qkv_weights = [
            (i + torch.arange(torch.prod(torch.tensor(shape)).item())).reshape(shape)
            for j, (i, shape) in enumerate(zip([1, 1000, 10000], [q_shape, k_shape, v_shape])) if enable_lora[j]
        ]
    
        result = qkv.zero_pad(qkv_weights)
    
        # now we check back the order
        qkv_weights = []
        if enable_lora[0]:
>           qkv_weights.append(qkv_weights[0].flatten())
E           IndexError: list index out of range

test/test_lora_qkv_linear.py:239: IndexError
___________________ test_zero_pad_sub_network[mha_enable_qv] ___________________

qkv_config = 'mha_enable_qv'

    @pytest.mark.parametrize("qkv_config", lora_qkv_configs.keys())
    def test_zero_pad_sub_network(qkv_config):
        config = lora_qkv_configs[qkv_config]
        seq_length, batch_size = 4, 2
        in_features, n_head, head_size, r = 128, 16, 2, 8
        enable_lora = config["enable_lora"]
        n_query_groups = config["n_query_groups"]
    
        out_features = (n_head + 2 * n_query_groups) * head_size
    
        lora_config = LoRAConfig(
            n_embd=out_features,
            n_head=n_head,
            n_query_groups=n_query_groups,
            head_size=head_size,
        )
        lora_config.fix_head_size = True
    
        qkv = LoRAQKVLinear(
            lora_config,
            in_features,
            out_features,
            head_size,
            n_head,
            n_query_groups,
            True,
            r=r,
            enable_lora=enable_lora,
        )
        qkv.reset_super_network()
    
        # determine sub-network configuration
        if n_query_groups == 1:  # MQA
            # super-network has 16 heads and 1 group
            sub_n_query_groups = 1
            sub_n_head = 8
        elif n_query_groups == n_head:  # MHA
            # super-network has 16 heads in 16 groups
            sub_n_query_groups = 8
            sub_n_head = 8
        else:  # GQA
            # super-network has 16 heads in 4 groups (4 heads per group)
            sub_n_query_groups = 2
            sub_n_head = 8  # 8/16 heads, 2/4 active groups (resulting into 2 heads per group)
    
        attn = CausalSelfAttention(lora_config, 0)
        attn.set_sub_network(lora_config.n_embd, sub_n_head, sub_n_query_groups, head_size)
        qkv_indices = attn.qkv_indices
        qkv.set_sub_network(
            in_features,
            (attn.sub_network_q_per_kv + 2) * head_size * sub_n_query_groups,
            qkv_indices,
            sub_network_query_groups=sub_n_query_groups,
            sub_network_n_head=sub_n_head,
            sub_network_head_size=head_size,
            sub_network_q_per_kv=attn.sub_network_q_per_kv,
        )
    
        q_shape = (
            batch_size,
            seq_length,
            qkv.sub_network_q_per_kv * sub_n_query_groups * head_size,
        )
        k_shape = (batch_size, seq_length, sub_n_query_groups * head_size)
        v_shape = (batch_size, seq_length, sub_n_query_groups * head_size)
    
        # **Generate sequential weights for easy tracking**
        qkv_weights = [
            (i + torch.arange(torch.prod(torch.tensor(shape)).item())).reshape(shape)
            for j, (i, shape) in enumerate(zip([1, 1000, 10000], [q_shape, k_shape, v_shape])) if enable_lora[j]
        ]
    
        result = qkv.zero_pad(qkv_weights)
    
        # now we check back the order
        qkv_weights = []
        if enable_lora[0]:
>           qkv_weights.append(qkv_weights[0].flatten())
E           IndexError: list index out of range

test/test_lora_qkv_linear.py:239: IndexError
___________________ test_zero_pad_sub_network[mha_enable_kv] ___________________

qkv_config = 'mha_enable_kv'

    @pytest.mark.parametrize("qkv_config", lora_qkv_configs.keys())
    def test_zero_pad_sub_network(qkv_config):
        config = lora_qkv_configs[qkv_config]
        seq_length, batch_size = 4, 2
        in_features, n_head, head_size, r = 128, 16, 2, 8
        enable_lora = config["enable_lora"]
        n_query_groups = config["n_query_groups"]
    
        out_features = (n_head + 2 * n_query_groups) * head_size
    
        lora_config = LoRAConfig(
            n_embd=out_features,
            n_head=n_head,
            n_query_groups=n_query_groups,
            head_size=head_size,
        )
        lora_config.fix_head_size = True
    
        qkv = LoRAQKVLinear(
            lora_config,
            in_features,
            out_features,
            head_size,
            n_head,
            n_query_groups,
            True,
            r=r,
            enable_lora=enable_lora,
        )
        qkv.reset_super_network()
    
        # determine sub-network configuration
        if n_query_groups == 1:  # MQA
            # super-network has 16 heads and 1 group
            sub_n_query_groups = 1
            sub_n_head = 8
        elif n_query_groups == n_head:  # MHA
            # super-network has 16 heads in 16 groups
            sub_n_query_groups = 8
            sub_n_head = 8
        else:  # GQA
            # super-network has 16 heads in 4 groups (4 heads per group)
            sub_n_query_groups = 2
            sub_n_head = 8  # 8/16 heads, 2/4 active groups (resulting into 2 heads per group)
    
        attn = CausalSelfAttention(lora_config, 0)
        attn.set_sub_network(lora_config.n_embd, sub_n_head, sub_n_query_groups, head_size)
        qkv_indices = attn.qkv_indices
        qkv.set_sub_network(
            in_features,
            (attn.sub_network_q_per_kv + 2) * head_size * sub_n_query_groups,
            qkv_indices,
            sub_network_query_groups=sub_n_query_groups,
            sub_network_n_head=sub_n_head,
            sub_network_head_size=head_size,
            sub_network_q_per_kv=attn.sub_network_q_per_kv,
        )
    
        q_shape = (
            batch_size,
            seq_length,
            qkv.sub_network_q_per_kv * sub_n_query_groups * head_size,
        )
        k_shape = (batch_size, seq_length, sub_n_query_groups * head_size)
        v_shape = (batch_size, seq_length, sub_n_query_groups * head_size)
    
        # **Generate sequential weights for easy tracking**
        qkv_weights = [
            (i + torch.arange(torch.prod(torch.tensor(shape)).item())).reshape(shape)
            for j, (i, shape) in enumerate(zip([1, 1000, 10000], [q_shape, k_shape, v_shape])) if enable_lora[j]
        ]
    
        result = qkv.zero_pad(qkv_weights)
    
        # now we check back the order
        qkv_weights = []
        if enable_lora[0]:
            qkv_weights.append(qkv_weights[0].flatten())
        if enable_lora[1]:
>           qkv_weights.append(qkv_weights[1].flatten())
E           IndexError: list index out of range

test/test_lora_qkv_linear.py:241: IndexError
__________________ test_zero_pad_sub_network[mha_enable_qkv] ___________________

qkv_config = 'mha_enable_qkv'

    @pytest.mark.parametrize("qkv_config", lora_qkv_configs.keys())
    def test_zero_pad_sub_network(qkv_config):
        config = lora_qkv_configs[qkv_config]
        seq_length, batch_size = 4, 2
        in_features, n_head, head_size, r = 128, 16, 2, 8
        enable_lora = config["enable_lora"]
        n_query_groups = config["n_query_groups"]
    
        out_features = (n_head + 2 * n_query_groups) * head_size
    
        lora_config = LoRAConfig(
            n_embd=out_features,
            n_head=n_head,
            n_query_groups=n_query_groups,
            head_size=head_size,
        )
        lora_config.fix_head_size = True
    
        qkv = LoRAQKVLinear(
            lora_config,
            in_features,
            out_features,
            head_size,
            n_head,
            n_query_groups,
            True,
            r=r,
            enable_lora=enable_lora,
        )
        qkv.reset_super_network()
    
        # determine sub-network configuration
        if n_query_groups == 1:  # MQA
            # super-network has 16 heads and 1 group
            sub_n_query_groups = 1
            sub_n_head = 8
        elif n_query_groups == n_head:  # MHA
            # super-network has 16 heads in 16 groups
            sub_n_query_groups = 8
            sub_n_head = 8
        else:  # GQA
            # super-network has 16 heads in 4 groups (4 heads per group)
            sub_n_query_groups = 2
            sub_n_head = 8  # 8/16 heads, 2/4 active groups (resulting into 2 heads per group)
    
        attn = CausalSelfAttention(lora_config, 0)
        attn.set_sub_network(lora_config.n_embd, sub_n_head, sub_n_query_groups, head_size)
        qkv_indices = attn.qkv_indices
        qkv.set_sub_network(
            in_features,
            (attn.sub_network_q_per_kv + 2) * head_size * sub_n_query_groups,
            qkv_indices,
            sub_network_query_groups=sub_n_query_groups,
            sub_network_n_head=sub_n_head,
            sub_network_head_size=head_size,
            sub_network_q_per_kv=attn.sub_network_q_per_kv,
        )
    
        q_shape = (
            batch_size,
            seq_length,
            qkv.sub_network_q_per_kv * sub_n_query_groups * head_size,
        )
        k_shape = (batch_size, seq_length, sub_n_query_groups * head_size)
        v_shape = (batch_size, seq_length, sub_n_query_groups * head_size)
    
        # **Generate sequential weights for easy tracking**
        qkv_weights = [
            (i + torch.arange(torch.prod(torch.tensor(shape)).item())).reshape(shape)
            for j, (i, shape) in enumerate(zip([1, 1000, 10000], [q_shape, k_shape, v_shape])) if enable_lora[j]
        ]
    
        result = qkv.zero_pad(qkv_weights)
    
        # now we check back the order
        qkv_weights = []
        if enable_lora[0]:
>           qkv_weights.append(qkv_weights[0].flatten())
E           IndexError: list index out of range

test/test_lora_qkv_linear.py:239: IndexError
__________________ test_zero_pad_sub_network[gqa_enable_qkv] ___________________

qkv_config = 'gqa_enable_qkv'

    @pytest.mark.parametrize("qkv_config", lora_qkv_configs.keys())
    def test_zero_pad_sub_network(qkv_config):
        config = lora_qkv_configs[qkv_config]
        seq_length, batch_size = 4, 2
        in_features, n_head, head_size, r = 128, 16, 2, 8
        enable_lora = config["enable_lora"]
        n_query_groups = config["n_query_groups"]
    
        out_features = (n_head + 2 * n_query_groups) * head_size
    
        lora_config = LoRAConfig(
            n_embd=out_features,
            n_head=n_head,
            n_query_groups=n_query_groups,
            head_size=head_size,
        )
        lora_config.fix_head_size = True
    
        qkv = LoRAQKVLinear(
            lora_config,
            in_features,
            out_features,
            head_size,
            n_head,
            n_query_groups,
            True,
            r=r,
            enable_lora=enable_lora,
        )
        qkv.reset_super_network()
    
        # determine sub-network configuration
        if n_query_groups == 1:  # MQA
            # super-network has 16 heads and 1 group
            sub_n_query_groups = 1
            sub_n_head = 8
        elif n_query_groups == n_head:  # MHA
            # super-network has 16 heads in 16 groups
            sub_n_query_groups = 8
            sub_n_head = 8
        else:  # GQA
            # super-network has 16 heads in 4 groups (4 heads per group)
            sub_n_query_groups = 2
            sub_n_head = 8  # 8/16 heads, 2/4 active groups (resulting into 2 heads per group)
    
        attn = CausalSelfAttention(lora_config, 0)
        attn.set_sub_network(lora_config.n_embd, sub_n_head, sub_n_query_groups, head_size)
        qkv_indices = attn.qkv_indices
        qkv.set_sub_network(
            in_features,
            (attn.sub_network_q_per_kv + 2) * head_size * sub_n_query_groups,
            qkv_indices,
            sub_network_query_groups=sub_n_query_groups,
            sub_network_n_head=sub_n_head,
            sub_network_head_size=head_size,
            sub_network_q_per_kv=attn.sub_network_q_per_kv,
        )
    
        q_shape = (
            batch_size,
            seq_length,
            qkv.sub_network_q_per_kv * sub_n_query_groups * head_size,
        )
        k_shape = (batch_size, seq_length, sub_n_query_groups * head_size)
        v_shape = (batch_size, seq_length, sub_n_query_groups * head_size)
    
        # **Generate sequential weights for easy tracking**
        qkv_weights = [
            (i + torch.arange(torch.prod(torch.tensor(shape)).item())).reshape(shape)
            for j, (i, shape) in enumerate(zip([1, 1000, 10000], [q_shape, k_shape, v_shape])) if enable_lora[j]
        ]
    
        result = qkv.zero_pad(qkv_weights)
    
        # now we check back the order
        qkv_weights = []
        if enable_lora[0]:
>           qkv_weights.append(qkv_weights[0].flatten())
E           IndexError: list index out of range

test/test_lora_qkv_linear.py:239: IndexError
___________________ test_zero_pad_sub_network[gqa_enable_qk] ___________________

qkv_config = 'gqa_enable_qk'

    @pytest.mark.parametrize("qkv_config", lora_qkv_configs.keys())
    def test_zero_pad_sub_network(qkv_config):
        config = lora_qkv_configs[qkv_config]
        seq_length, batch_size = 4, 2
        in_features, n_head, head_size, r = 128, 16, 2, 8
        enable_lora = config["enable_lora"]
        n_query_groups = config["n_query_groups"]
    
        out_features = (n_head + 2 * n_query_groups) * head_size
    
        lora_config = LoRAConfig(
            n_embd=out_features,
            n_head=n_head,
            n_query_groups=n_query_groups,
            head_size=head_size,
        )
        lora_config.fix_head_size = True
    
        qkv = LoRAQKVLinear(
            lora_config,
            in_features,
            out_features,
            head_size,
            n_head,
            n_query_groups,
            True,
            r=r,
            enable_lora=enable_lora,
        )
        qkv.reset_super_network()
    
        # determine sub-network configuration
        if n_query_groups == 1:  # MQA
            # super-network has 16 heads and 1 group
            sub_n_query_groups = 1
            sub_n_head = 8
        elif n_query_groups == n_head:  # MHA
            # super-network has 16 heads in 16 groups
            sub_n_query_groups = 8
            sub_n_head = 8
        else:  # GQA
            # super-network has 16 heads in 4 groups (4 heads per group)
            sub_n_query_groups = 2
            sub_n_head = 8  # 8/16 heads, 2/4 active groups (resulting into 2 heads per group)
    
        attn = CausalSelfAttention(lora_config, 0)
        attn.set_sub_network(lora_config.n_embd, sub_n_head, sub_n_query_groups, head_size)
        qkv_indices = attn.qkv_indices
        qkv.set_sub_network(
            in_features,
            (attn.sub_network_q_per_kv + 2) * head_size * sub_n_query_groups,
            qkv_indices,
            sub_network_query_groups=sub_n_query_groups,
            sub_network_n_head=sub_n_head,
            sub_network_head_size=head_size,
            sub_network_q_per_kv=attn.sub_network_q_per_kv,
        )
    
        q_shape = (
            batch_size,
            seq_length,
            qkv.sub_network_q_per_kv * sub_n_query_groups * head_size,
        )
        k_shape = (batch_size, seq_length, sub_n_query_groups * head_size)
        v_shape = (batch_size, seq_length, sub_n_query_groups * head_size)
    
        # **Generate sequential weights for easy tracking**
        qkv_weights = [
            (i + torch.arange(torch.prod(torch.tensor(shape)).item())).reshape(shape)
            for j, (i, shape) in enumerate(zip([1, 1000, 10000], [q_shape, k_shape, v_shape])) if enable_lora[j]
        ]
    
        result = qkv.zero_pad(qkv_weights)
    
        # now we check back the order
        qkv_weights = []
        if enable_lora[0]:
>           qkv_weights.append(qkv_weights[0].flatten())
E           IndexError: list index out of range

test/test_lora_qkv_linear.py:239: IndexError
___________________ test_zero_pad_sub_network[gqa_enable_qv] ___________________

qkv_config = 'gqa_enable_qv'

    @pytest.mark.parametrize("qkv_config", lora_qkv_configs.keys())
    def test_zero_pad_sub_network(qkv_config):
        config = lora_qkv_configs[qkv_config]
        seq_length, batch_size = 4, 2
        in_features, n_head, head_size, r = 128, 16, 2, 8
        enable_lora = config["enable_lora"]
        n_query_groups = config["n_query_groups"]
    
        out_features = (n_head + 2 * n_query_groups) * head_size
    
        lora_config = LoRAConfig(
            n_embd=out_features,
            n_head=n_head,
            n_query_groups=n_query_groups,
            head_size=head_size,
        )
        lora_config.fix_head_size = True
    
        qkv = LoRAQKVLinear(
            lora_config,
            in_features,
            out_features,
            head_size,
            n_head,
            n_query_groups,
            True,
            r=r,
            enable_lora=enable_lora,
        )
        qkv.reset_super_network()
    
        # determine sub-network configuration
        if n_query_groups == 1:  # MQA
            # super-network has 16 heads and 1 group
            sub_n_query_groups = 1
            sub_n_head = 8
        elif n_query_groups == n_head:  # MHA
            # super-network has 16 heads in 16 groups
            sub_n_query_groups = 8
            sub_n_head = 8
        else:  # GQA
            # super-network has 16 heads in 4 groups (4 heads per group)
            sub_n_query_groups = 2
            sub_n_head = 8  # 8/16 heads, 2/4 active groups (resulting into 2 heads per group)
    
        attn = CausalSelfAttention(lora_config, 0)
        attn.set_sub_network(lora_config.n_embd, sub_n_head, sub_n_query_groups, head_size)
        qkv_indices = attn.qkv_indices
        qkv.set_sub_network(
            in_features,
            (attn.sub_network_q_per_kv + 2) * head_size * sub_n_query_groups,
            qkv_indices,
            sub_network_query_groups=sub_n_query_groups,
            sub_network_n_head=sub_n_head,
            sub_network_head_size=head_size,
            sub_network_q_per_kv=attn.sub_network_q_per_kv,
        )
    
        q_shape = (
            batch_size,
            seq_length,
            qkv.sub_network_q_per_kv * sub_n_query_groups * head_size,
        )
        k_shape = (batch_size, seq_length, sub_n_query_groups * head_size)
        v_shape = (batch_size, seq_length, sub_n_query_groups * head_size)
    
        # **Generate sequential weights for easy tracking**
        qkv_weights = [
            (i + torch.arange(torch.prod(torch.tensor(shape)).item())).reshape(shape)
            for j, (i, shape) in enumerate(zip([1, 1000, 10000], [q_shape, k_shape, v_shape])) if enable_lora[j]
        ]
    
        result = qkv.zero_pad(qkv_weights)
    
        # now we check back the order
        qkv_weights = []
        if enable_lora[0]:
>           qkv_weights.append(qkv_weights[0].flatten())
E           IndexError: list index out of range

test/test_lora_qkv_linear.py:239: IndexError
___________________ test_zero_pad_sub_network[gqa_enable_kv] ___________________

qkv_config = 'gqa_enable_kv'

    @pytest.mark.parametrize("qkv_config", lora_qkv_configs.keys())
    def test_zero_pad_sub_network(qkv_config):
        config = lora_qkv_configs[qkv_config]
        seq_length, batch_size = 4, 2
        in_features, n_head, head_size, r = 128, 16, 2, 8
        enable_lora = config["enable_lora"]
        n_query_groups = config["n_query_groups"]
    
        out_features = (n_head + 2 * n_query_groups) * head_size
    
        lora_config = LoRAConfig(
            n_embd=out_features,
            n_head=n_head,
            n_query_groups=n_query_groups,
            head_size=head_size,
        )
        lora_config.fix_head_size = True
    
        qkv = LoRAQKVLinear(
            lora_config,
            in_features,
            out_features,
            head_size,
            n_head,
            n_query_groups,
            True,
            r=r,
            enable_lora=enable_lora,
        )
        qkv.reset_super_network()
    
        # determine sub-network configuration
        if n_query_groups == 1:  # MQA
            # super-network has 16 heads and 1 group
            sub_n_query_groups = 1
            sub_n_head = 8
        elif n_query_groups == n_head:  # MHA
            # super-network has 16 heads in 16 groups
            sub_n_query_groups = 8
            sub_n_head = 8
        else:  # GQA
            # super-network has 16 heads in 4 groups (4 heads per group)
            sub_n_query_groups = 2
            sub_n_head = 8  # 8/16 heads, 2/4 active groups (resulting into 2 heads per group)
    
        attn = CausalSelfAttention(lora_config, 0)
        attn.set_sub_network(lora_config.n_embd, sub_n_head, sub_n_query_groups, head_size)
        qkv_indices = attn.qkv_indices
        qkv.set_sub_network(
            in_features,
            (attn.sub_network_q_per_kv + 2) * head_size * sub_n_query_groups,
            qkv_indices,
            sub_network_query_groups=sub_n_query_groups,
            sub_network_n_head=sub_n_head,
            sub_network_head_size=head_size,
            sub_network_q_per_kv=attn.sub_network_q_per_kv,
        )
    
        q_shape = (
            batch_size,
            seq_length,
            qkv.sub_network_q_per_kv * sub_n_query_groups * head_size,
        )
        k_shape = (batch_size, seq_length, sub_n_query_groups * head_size)
        v_shape = (batch_size, seq_length, sub_n_query_groups * head_size)
    
        # **Generate sequential weights for easy tracking**
        qkv_weights = [
            (i + torch.arange(torch.prod(torch.tensor(shape)).item())).reshape(shape)
            for j, (i, shape) in enumerate(zip([1, 1000, 10000], [q_shape, k_shape, v_shape])) if enable_lora[j]
        ]
    
        result = qkv.zero_pad(qkv_weights)
    
        # now we check back the order
        qkv_weights = []
        if enable_lora[0]:
            qkv_weights.append(qkv_weights[0].flatten())
        if enable_lora[1]:
>           qkv_weights.append(qkv_weights[1].flatten())
E           IndexError: list index out of range

test/test_lora_qkv_linear.py:241: IndexError
__________________ test_zero_pad_sub_network[mqa_enable_qkv] ___________________

qkv_config = 'mqa_enable_qkv'

    @pytest.mark.parametrize("qkv_config", lora_qkv_configs.keys())
    def test_zero_pad_sub_network(qkv_config):
        config = lora_qkv_configs[qkv_config]
        seq_length, batch_size = 4, 2
        in_features, n_head, head_size, r = 128, 16, 2, 8
        enable_lora = config["enable_lora"]
        n_query_groups = config["n_query_groups"]
    
        out_features = (n_head + 2 * n_query_groups) * head_size
    
        lora_config = LoRAConfig(
            n_embd=out_features,
            n_head=n_head,
            n_query_groups=n_query_groups,
            head_size=head_size,
        )
        lora_config.fix_head_size = True
    
        qkv = LoRAQKVLinear(
            lora_config,
            in_features,
            out_features,
            head_size,
            n_head,
            n_query_groups,
            True,
            r=r,
            enable_lora=enable_lora,
        )
        qkv.reset_super_network()
    
        # determine sub-network configuration
        if n_query_groups == 1:  # MQA
            # super-network has 16 heads and 1 group
            sub_n_query_groups = 1
            sub_n_head = 8
        elif n_query_groups == n_head:  # MHA
            # super-network has 16 heads in 16 groups
            sub_n_query_groups = 8
            sub_n_head = 8
        else:  # GQA
            # super-network has 16 heads in 4 groups (4 heads per group)
            sub_n_query_groups = 2
            sub_n_head = 8  # 8/16 heads, 2/4 active groups (resulting into 2 heads per group)
    
        attn = CausalSelfAttention(lora_config, 0)
        attn.set_sub_network(lora_config.n_embd, sub_n_head, sub_n_query_groups, head_size)
        qkv_indices = attn.qkv_indices
        qkv.set_sub_network(
            in_features,
            (attn.sub_network_q_per_kv + 2) * head_size * sub_n_query_groups,
            qkv_indices,
            sub_network_query_groups=sub_n_query_groups,
            sub_network_n_head=sub_n_head,
            sub_network_head_size=head_size,
            sub_network_q_per_kv=attn.sub_network_q_per_kv,
        )
    
        q_shape = (
            batch_size,
            seq_length,
            qkv.sub_network_q_per_kv * sub_n_query_groups * head_size,
        )
        k_shape = (batch_size, seq_length, sub_n_query_groups * head_size)
        v_shape = (batch_size, seq_length, sub_n_query_groups * head_size)
    
        # **Generate sequential weights for easy tracking**
        qkv_weights = [
            (i + torch.arange(torch.prod(torch.tensor(shape)).item())).reshape(shape)
            for j, (i, shape) in enumerate(zip([1, 1000, 10000], [q_shape, k_shape, v_shape])) if enable_lora[j]
        ]
    
        result = qkv.zero_pad(qkv_weights)
    
        # now we check back the order
        qkv_weights = []
        if enable_lora[0]:
>           qkv_weights.append(qkv_weights[0].flatten())
E           IndexError: list index out of range

test/test_lora_qkv_linear.py:239: IndexError
___________________ test_zero_pad_sub_network[mqa_enable_qk] ___________________

qkv_config = 'mqa_enable_qk'

    @pytest.mark.parametrize("qkv_config", lora_qkv_configs.keys())
    def test_zero_pad_sub_network(qkv_config):
        config = lora_qkv_configs[qkv_config]
        seq_length, batch_size = 4, 2
        in_features, n_head, head_size, r = 128, 16, 2, 8
        enable_lora = config["enable_lora"]
        n_query_groups = config["n_query_groups"]
    
        out_features = (n_head + 2 * n_query_groups) * head_size
    
        lora_config = LoRAConfig(
            n_embd=out_features,
            n_head=n_head,
            n_query_groups=n_query_groups,
            head_size=head_size,
        )
        lora_config.fix_head_size = True
    
        qkv = LoRAQKVLinear(
            lora_config,
            in_features,
            out_features,
            head_size,
            n_head,
            n_query_groups,
            True,
            r=r,
            enable_lora=enable_lora,
        )
        qkv.reset_super_network()
    
        # determine sub-network configuration
        if n_query_groups == 1:  # MQA
            # super-network has 16 heads and 1 group
            sub_n_query_groups = 1
            sub_n_head = 8
        elif n_query_groups == n_head:  # MHA
            # super-network has 16 heads in 16 groups
            sub_n_query_groups = 8
            sub_n_head = 8
        else:  # GQA
            # super-network has 16 heads in 4 groups (4 heads per group)
            sub_n_query_groups = 2
            sub_n_head = 8  # 8/16 heads, 2/4 active groups (resulting into 2 heads per group)
    
        attn = CausalSelfAttention(lora_config, 0)
        attn.set_sub_network(lora_config.n_embd, sub_n_head, sub_n_query_groups, head_size)
        qkv_indices = attn.qkv_indices
        qkv.set_sub_network(
            in_features,
            (attn.sub_network_q_per_kv + 2) * head_size * sub_n_query_groups,
            qkv_indices,
            sub_network_query_groups=sub_n_query_groups,
            sub_network_n_head=sub_n_head,
            sub_network_head_size=head_size,
            sub_network_q_per_kv=attn.sub_network_q_per_kv,
        )
    
        q_shape = (
            batch_size,
            seq_length,
            qkv.sub_network_q_per_kv * sub_n_query_groups * head_size,
        )
        k_shape = (batch_size, seq_length, sub_n_query_groups * head_size)
        v_shape = (batch_size, seq_length, sub_n_query_groups * head_size)
    
        # **Generate sequential weights for easy tracking**
        qkv_weights = [
            (i + torch.arange(torch.prod(torch.tensor(shape)).item())).reshape(shape)
            for j, (i, shape) in enumerate(zip([1, 1000, 10000], [q_shape, k_shape, v_shape])) if enable_lora[j]
        ]
    
        result = qkv.zero_pad(qkv_weights)
    
        # now we check back the order
        qkv_weights = []
        if enable_lora[0]:
>           qkv_weights.append(qkv_weights[0].flatten())
E           IndexError: list index out of range

test/test_lora_qkv_linear.py:239: IndexError
___________________ test_zero_pad_sub_network[mqa_enable_qv] ___________________

qkv_config = 'mqa_enable_qv'

    @pytest.mark.parametrize("qkv_config", lora_qkv_configs.keys())
    def test_zero_pad_sub_network(qkv_config):
        config = lora_qkv_configs[qkv_config]
        seq_length, batch_size = 4, 2
        in_features, n_head, head_size, r = 128, 16, 2, 8
        enable_lora = config["enable_lora"]
        n_query_groups = config["n_query_groups"]
    
        out_features = (n_head + 2 * n_query_groups) * head_size
    
        lora_config = LoRAConfig(
            n_embd=out_features,
            n_head=n_head,
            n_query_groups=n_query_groups,
            head_size=head_size,
        )
        lora_config.fix_head_size = True
    
        qkv = LoRAQKVLinear(
            lora_config,
            in_features,
            out_features,
            head_size,
            n_head,
            n_query_groups,
            True,
            r=r,
            enable_lora=enable_lora,
        )
        qkv.reset_super_network()
    
        # determine sub-network configuration
        if n_query_groups == 1:  # MQA
            # super-network has 16 heads and 1 group
            sub_n_query_groups = 1
            sub_n_head = 8
        elif n_query_groups == n_head:  # MHA
            # super-network has 16 heads in 16 groups
            sub_n_query_groups = 8
            sub_n_head = 8
        else:  # GQA
            # super-network has 16 heads in 4 groups (4 heads per group)
            sub_n_query_groups = 2
            sub_n_head = 8  # 8/16 heads, 2/4 active groups (resulting into 2 heads per group)
    
        attn = CausalSelfAttention(lora_config, 0)
        attn.set_sub_network(lora_config.n_embd, sub_n_head, sub_n_query_groups, head_size)
        qkv_indices = attn.qkv_indices
        qkv.set_sub_network(
            in_features,
            (attn.sub_network_q_per_kv + 2) * head_size * sub_n_query_groups,
            qkv_indices,
            sub_network_query_groups=sub_n_query_groups,
            sub_network_n_head=sub_n_head,
            sub_network_head_size=head_size,
            sub_network_q_per_kv=attn.sub_network_q_per_kv,
        )
    
        q_shape = (
            batch_size,
            seq_length,
            qkv.sub_network_q_per_kv * sub_n_query_groups * head_size,
        )
        k_shape = (batch_size, seq_length, sub_n_query_groups * head_size)
        v_shape = (batch_size, seq_length, sub_n_query_groups * head_size)
    
        # **Generate sequential weights for easy tracking**
        qkv_weights = [
            (i + torch.arange(torch.prod(torch.tensor(shape)).item())).reshape(shape)
            for j, (i, shape) in enumerate(zip([1, 1000, 10000], [q_shape, k_shape, v_shape])) if enable_lora[j]
        ]
    
        result = qkv.zero_pad(qkv_weights)
    
        # now we check back the order
        qkv_weights = []
        if enable_lora[0]:
>           qkv_weights.append(qkv_weights[0].flatten())
E           IndexError: list index out of range

test/test_lora_qkv_linear.py:239: IndexError
___________________ test_zero_pad_sub_network[mqa_enable_kv] ___________________

qkv_config = 'mqa_enable_kv'

    @pytest.mark.parametrize("qkv_config", lora_qkv_configs.keys())
    def test_zero_pad_sub_network(qkv_config):
        config = lora_qkv_configs[qkv_config]
        seq_length, batch_size = 4, 2
        in_features, n_head, head_size, r = 128, 16, 2, 8
        enable_lora = config["enable_lora"]
        n_query_groups = config["n_query_groups"]
    
        out_features = (n_head + 2 * n_query_groups) * head_size
    
        lora_config = LoRAConfig(
            n_embd=out_features,
            n_head=n_head,
            n_query_groups=n_query_groups,
            head_size=head_size,
        )
        lora_config.fix_head_size = True
    
        qkv = LoRAQKVLinear(
            lora_config,
            in_features,
            out_features,
            head_size,
            n_head,
            n_query_groups,
            True,
            r=r,
            enable_lora=enable_lora,
        )
        qkv.reset_super_network()
    
        # determine sub-network configuration
        if n_query_groups == 1:  # MQA
            # super-network has 16 heads and 1 group
            sub_n_query_groups = 1
            sub_n_head = 8
        elif n_query_groups == n_head:  # MHA
            # super-network has 16 heads in 16 groups
            sub_n_query_groups = 8
            sub_n_head = 8
        else:  # GQA
            # super-network has 16 heads in 4 groups (4 heads per group)
            sub_n_query_groups = 2
            sub_n_head = 8  # 8/16 heads, 2/4 active groups (resulting into 2 heads per group)
    
        attn = CausalSelfAttention(lora_config, 0)
        attn.set_sub_network(lora_config.n_embd, sub_n_head, sub_n_query_groups, head_size)
        qkv_indices = attn.qkv_indices
        qkv.set_sub_network(
            in_features,
            (attn.sub_network_q_per_kv + 2) * head_size * sub_n_query_groups,
            qkv_indices,
            sub_network_query_groups=sub_n_query_groups,
            sub_network_n_head=sub_n_head,
            sub_network_head_size=head_size,
            sub_network_q_per_kv=attn.sub_network_q_per_kv,
        )
    
        q_shape = (
            batch_size,
            seq_length,
            qkv.sub_network_q_per_kv * sub_n_query_groups * head_size,
        )
        k_shape = (batch_size, seq_length, sub_n_query_groups * head_size)
        v_shape = (batch_size, seq_length, sub_n_query_groups * head_size)
    
        # **Generate sequential weights for easy tracking**
        qkv_weights = [
            (i + torch.arange(torch.prod(torch.tensor(shape)).item())).reshape(shape)
            for j, (i, shape) in enumerate(zip([1, 1000, 10000], [q_shape, k_shape, v_shape])) if enable_lora[j]
        ]
    
        result = qkv.zero_pad(qkv_weights)
    
        # now we check back the order
        qkv_weights = []
        if enable_lora[0]:
            qkv_weights.append(qkv_weights[0].flatten())
        if enable_lora[1]:
>           qkv_weights.append(qkv_weights[1].flatten())
E           IndexError: list index out of range

test/test_lora_qkv_linear.py:241: IndexError
=============================== warnings summary ===============================
../../../../../home/sukthank/anaconda3/envs/whittle-3.11/lib/python3.11/site-packages/transformers/utils/hub.py:128
  /home/sukthank/anaconda3/envs/whittle-3.11/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED test/test_lora_qkv_linear.py::test_zero_pad[mha_enable_qv] - IndexErro...
FAILED test/test_lora_qkv_linear.py::test_zero_pad[mha_enable_kv] - assert False
FAILED test/test_lora_qkv_linear.py::test_zero_pad[gqa_enable_qv] - IndexErro...
FAILED test/test_lora_qkv_linear.py::test_zero_pad[gqa_enable_kv] - assert False
FAILED test/test_lora_qkv_linear.py::test_zero_pad[mqa_enable_qv] - IndexErro...
FAILED test/test_lora_qkv_linear.py::test_zero_pad[mqa_enable_kv] - assert False
FAILED test/test_lora_qkv_linear.py::test_zero_pad_sub_network[mha_enable_qk]
FAILED test/test_lora_qkv_linear.py::test_zero_pad_sub_network[mha_enable_qv]
FAILED test/test_lora_qkv_linear.py::test_zero_pad_sub_network[mha_enable_kv]
FAILED test/test_lora_qkv_linear.py::test_zero_pad_sub_network[mha_enable_qkv]
FAILED test/test_lora_qkv_linear.py::test_zero_pad_sub_network[gqa_enable_qkv]
FAILED test/test_lora_qkv_linear.py::test_zero_pad_sub_network[gqa_enable_qk]
FAILED test/test_lora_qkv_linear.py::test_zero_pad_sub_network[gqa_enable_qv]
FAILED test/test_lora_qkv_linear.py::test_zero_pad_sub_network[gqa_enable_kv]
FAILED test/test_lora_qkv_linear.py::test_zero_pad_sub_network[mqa_enable_qkv]
FAILED test/test_lora_qkv_linear.py::test_zero_pad_sub_network[mqa_enable_qk]
FAILED test/test_lora_qkv_linear.py::test_zero_pad_sub_network[mqa_enable_qv]
FAILED test/test_lora_qkv_linear.py::test_zero_pad_sub_network[mqa_enable_kv]
=================== 18 failed, 6 passed, 1 warning in 0.39s ====================
