{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#viewing-docs-locally","title":"Viewing docs locally","text":"<pre><code>mkdocs serve  # that's it\n</code></pre> <p>Deployment</p> <p>I'll set up <code>mike</code> and document how to use that to deploy docs to github pages.</p>"},{"location":"#quickstart-mkdocs","title":"Quickstart Mkdocs","text":"<p>This is your bible This is just an overview of some features.</p> <pre><code>print(\"hello world\")\n</code></pre> <pre><code>whittle --help  # (1)!\n</code></pre> <ol> <li>Look, an admonation! You can put <code>code</code> in here too! <pre><code>print(\"cool\")\n</code></pre></li> </ol> <p>You can create tabbed blocks which are super useful.</p> PythoncliDiagram using mermaid <pre><code>print(\"hello from tabbed block 1\")\n\nprint(\"Hello from highlight\")\nprint(\"Hello from highlight as well\")\n</code></pre> <pre><code>whittle --help\n</code></pre> <pre><code>graph LR\n  A[Start] --&gt; B{Error?};\n  B --&gt;|Yes| C[Hmm...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Yay!];</code></pre> <p>Warning</p> <p>This is a warning and set to be expanded with <code>!!!</code></p> My Tip name <p>This is a closed tip with <code>???</code></p> <p>To link to internal api docs, use the following.</p> <pre><code>[LS][whittle.search.local_search]\n</code></pre> <p>See LS for more information.</p> <p>To link to api docs in other codebases, you can directly reference them in much the same way you would import them, for example, this references the <code>ProcessPoolExecutor</code> class in the <code>concurrent.futures</code> module. The backticks are optional, they just make the link look like code.</p> <pre><code>[`ProcessPoolExecutor`][concurrent.futures.ProcessPoolExecutor]\n</code></pre> <p>You can use <code>()</code> like you normally would to link to html links, i.e. Python Docs.</p> <pre><code>[Python Docs](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor).\n</code></pre> <p>You can also link directly to other pages in your repo if you need using a relative syntax,</p> <pre><code>[API](./contributing)\n</code></pre> <p>You can directly include the content of other files if you need, but I've rarely had a use for it. The main one was really for including the <code>CONTIBUTING.md</code> file in the <code>docs/contributing.md</code> file.</p> <p>This is all that has to be done. The path is relative to the root of the repo (where you execute the command from). <pre><code>## Contributing\n\nWork In Progress.\n\n\n## Installation\n\n```bash\n# Create your own fork of the repository if required and replace whittle-org with your username\ngit clone git@github.com/whittle-org/whittle.git\ncd whittle\npip install -e \".[dev]\"  # Install what's here (the `.` part) and install the extra dev dependancies\n</code></pre></p> <p>Setup <code>pre-commit</code> to run on every commit</p> <pre><code>pre-commit install\n</code></pre>"},{"location":"#testing","title":"Testing","text":"<pre><code>pytest\n</code></pre>"},{"location":"#release","title":"Release","text":"<p>Update the version in <code>pyproject.toml</code> first, say to <code>X.Y.Z</code>. If you maintain a changelog, update it.</p> <p>This part just makes a versioned commit and tag for github and to be able to easily find code at a specific version. It will also help with versioned documentation to have a tag.</p> <pre><code>git add pyproject.toml [changelog-file-if-any]\ngit commit -m \"bump: X.Y.Z\"\ngit tag X.Y.Z\ngit push --tags\ngit push\n</code></pre> <p>Then to release on PyPI:</p> <pre><code>pip install twine # If not already\n\nrm -rf ./dist  # Remove anything currently occupying the dist folder\npython -m build --sdist  # Build a source distribution\ntwine upload dist/*  # Publish to PyPI\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>View locally</p> <pre><code>mkdocs --serve\n</code></pre> <p>Build and deploy to GitHub Pages. Make sure to specify the github tag you want to deploy.</p> <p><pre><code>mike deploy --push --update-aliases &lt;TAG&gt; \"latest\"\n</code></pre> ```</p>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#contributing","title":"Contributing","text":"<p>Work In Progress.</p>"},{"location":"contributing/#installation","title":"Installation","text":"<pre><code># Create your own fork of the repository if required and replace whittle-org with your username\ngit clone git@github.com/whittle-org/whittle.git\ncd whittle\npip install -e \".[dev]\"  # Install what's here (the `.` part) and install the extra dev dependancies\n</code></pre> <p>Setup <code>pre-commit</code> to run on every commit</p> <pre><code>pre-commit install\n</code></pre>"},{"location":"contributing/#testing","title":"Testing","text":"<pre><code>pytest\n</code></pre>"},{"location":"contributing/#release","title":"Release","text":"<p>Update the version in <code>pyproject.toml</code> first, say to <code>X.Y.Z</code>. If you maintain a changelog, update it.</p> <p>This part just makes a versioned commit and tag for github and to be able to easily find code at a specific version. It will also help with versioned documentation to have a tag.</p> <pre><code>git add pyproject.toml [changelog-file-if-any]\ngit commit -m \"bump: X.Y.Z\"\ngit tag X.Y.Z\ngit push --tags\ngit push\n</code></pre> <p>Then to release on PyPI:</p> <pre><code>pip install twine # If not already\n\nrm -rf ./dist  # Remove anything currently occupying the dist folder\npython -m build --sdist  # Build a source distribution\ntwine upload dist/*  # Publish to PyPI\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>View locally</p> <pre><code>mkdocs --serve\n</code></pre> <p>Build and deploy to GitHub Pages. Make sure to specify the github tag you want to deploy.</p> <pre><code>mike deploy --push --update-aliases &lt;TAG&gt; \"latest\"\n</code></pre>"},{"location":"api/whittle/extract_subnetworks/","title":"Extract subnetworks","text":""},{"location":"api/whittle/extract_subnetworks/#whittle.extract_subnetworks","title":"whittle.extract_subnetworks","text":""},{"location":"api/whittle/eval/utils/","title":"Utils","text":""},{"location":"api/whittle/eval/utils/#whittle.eval.utils","title":"whittle.eval.utils","text":""},{"location":"api/whittle/eval/utils/#whittle.eval.utils.convert_and_evaluate","title":"convert_and_evaluate","text":"<pre><code>convert_and_evaluate(\n    model: GPT,\n    tasks: str | None = None,\n    out_dir=None,\n    force_conversion: bool = False,\n    num_fewshot: int | None = None,\n    batch_size: int | str = 1,\n    device: str | None = None,\n    dtype: str | dtype | None = None,\n    limit: float | None = None,\n    seed: int = 1234,\n    save_filepath: Path | None = None,\n    access_token: str | None = None,\n) -&gt; None\n</code></pre> <p>Evaluate a model with the LM Evaluation Harness.</p> PARAMETER DESCRIPTION <code>checkpoint_dir</code> <p>Directory where the <code>lit_model.pth</code> and tokenizer files are located.</p> <p> </p> <code>out_dir</code> <p>Directory in which to save the converted checkpoints for evaluation. Saves to <code>checkpoint_dir</code>/evaluate by default.</p> <p> DEFAULT: <code>None</code> </p> <code>force_conversion</code> <p>Set to <code>True</code> to reconvert the model and override an existing model.pth from a previous evaluation call.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>tasks</code> <p>CSV of task names to evaluate. Example: \"hellaswag,truthfulqa_mc2,mmlu\"</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>num_fewshot</code> <p>Number of examples in few-shot context.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>batch_size</code> <p>Batch size configuration as positive integer value (default: 1), \"auto\", in the format 'auto:N', where 'auto:4' recomputes the batch size 4 times.</p> <p> TYPE: <code>int | str</code> DEFAULT: <code>1</code> </p> <code>device</code> <p>Device to use for evaluation, for example, \"cuda\" or \"cuda:0\".</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>limit</code> <p>Limit on number of examples per task.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>Random seed.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1234</code> </p> <code>save_filepath</code> <p>The file where the results will be saved. Saves to <code>out_dir/results.json</code> by default.</p> <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> <code>access_token</code> <p>Optional API token to access models with restrictions.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>whittle/eval/utils.py</code> <pre><code>def convert_and_evaluate(\n    model: GPT,\n    tasks: str | None = None,\n    out_dir=None,\n    force_conversion: bool = False,\n    num_fewshot: int | None = None,\n    batch_size: int | str = 1,\n    device: str | None = None,\n    dtype: str | torch.dtype | None = None,\n    limit: float | None = None,\n    seed: int = 1234,\n    save_filepath: Path | None = None,\n    access_token: str | None = None,\n) -&gt; None:\n    \"\"\"Evaluate a model with the LM Evaluation Harness.\n\n    Arguments:\n        checkpoint_dir: Directory where the `lit_model.pth` and tokenizer files are located.\n        out_dir: Directory in which to save the converted checkpoints for evaluation.\n            Saves to `checkpoint_dir`/evaluate by default.\n        force_conversion: Set to `True` to reconvert the model and override\n            an existing model.pth from a previous evaluation call.\n        tasks: CSV of task names to evaluate. Example: \"hellaswag,truthfulqa_mc2,mmlu\"\n        num_fewshot: Number of examples in few-shot context.\n        batch_size: Batch size configuration as positive integer value (default: 1),\n            \"auto\", in the format 'auto:N', where 'auto:4' recomputes the batch size 4 times.\n        device: Device to use for evaluation, for example, \"cuda\" or \"cuda:0\".\n        limit: Limit on number of examples per task.\n        seed: Random seed.\n        save_filepath: The file where the results will be saved.\n            Saves to `out_dir/results.json` by default.\n        access_token: Optional API token to access models with restrictions.\n    \"\"\"\n    if tasks is None:\n        from lm_eval.tasks import TaskManager\n\n        taskm = TaskManager()\n        print(\"\\n\".join(taskm.task_index.keys()))\n        print(\n            \"\\n\\nTo evaluate multiple tasks, you can chain the task names \"\n            \"listed above via a comma-separated list.\"\n            \"\\nFor example: `--tasks 'hellaswag,truthfulqa_mc2,mmlu'`. \"\n            \"\\nTo search for a specific task, use `litgpt evaluate list | grep task_name`.\"\n        )\n        return\n\n    pprint(locals())\n\n    if not (isinstance(batch_size, int) and batch_size &gt; 0) and not (\n        isinstance(batch_size, str) and batch_size.startswith(\"auto\")\n    ):\n        raise ValueError(\n            \"batch_size must be a positive integer, 'auto', or in the format 'auto:N'.\"\n        )\n\n    from lm_eval import evaluator\n\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    model = WhittleLM(\n        pretrained=model, device=device, batch_size=batch_size, dtype=dtype\n    )\n\n    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n    results = evaluator.simple_evaluate(\n        model=model,\n        tasks=tasks.split(\",\"),\n        num_fewshot=num_fewshot,\n        batch_size=batch_size,\n        device=device,\n        limit=limit,\n        random_seed=seed,\n        numpy_random_seed=seed,\n        torch_random_seed=seed,\n    )\n    out_dir = Path(out_dir)\n    out_dir.mkdir(parents=True, exist_ok=True)\n    save_filepath = (\n        out_dir / Path(\"results.json\") if save_filepath is None else Path(save_filepath)\n    )\n    prepare_results(results, save_filepath)\n</code></pre>"},{"location":"api/whittle/eval/whittle_llms/","title":"Whittle llms","text":""},{"location":"api/whittle/eval/whittle_llms/#whittle.eval.whittle_llms","title":"whittle.eval.whittle_llms","text":""},{"location":"api/whittle/eval/whittle_llms/#whittle.eval.whittle_llms.WhittleLM","title":"WhittleLM","text":"<pre><code>WhittleLM(\n    pretrained: GPT,\n    backend: (\n        Literal[\"default\", \"causal\", \"seq2seq\"] | None\n    ) = \"default\",\n    revision: str | None = \"main\",\n    subfolder=None,\n    tokenizer: (\n        str\n        | PreTrainedTokenizer\n        | PreTrainedTokenizerFast\n        | None\n    ) = None,\n    truncation: bool | None = False,\n    logits_cache: bool = True,\n    max_length=None,\n    device=\"cuda\",\n    dtype=\"auto\",\n    batch_size=1,\n    max_batch_size=64,\n    trust_remote_code: bool | None = False,\n    use_fast_tokenizer: bool | None = True,\n    add_bos_token: bool | None = False,\n    prefix_token_id: int | None = None,\n    parallelize: bool | None = False,\n    device_map_option=\"auto\",\n    max_memory_per_gpu=None,\n    max_cpu_memory=None,\n    offload_folder=\"./offload\",\n    peft: str | None = None,\n    delta: str | None = None,\n    autogptq: bool | str | None = False,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>TemplateLM</code></p> <p>An abstracted whittle model class. Enables usage with both models of <code>whittle.models.gpt.GPT</code></p> Source code in <code>whittle/eval/whittle_llms.py</code> <pre><code>def __init__(\n    self,\n    pretrained: GPT,\n    backend: Literal[\"default\", \"causal\", \"seq2seq\"] | None = \"default\",\n    # override whether the model should be treated as decoder-only (causal) or encoder-decoder (seq2seq)\n    revision: str | None = \"main\",\n    subfolder=None,\n    tokenizer: str\n    | transformers.PreTrainedTokenizer\n    | transformers.PreTrainedTokenizerFast\n    | None = None,\n    truncation: bool | None = False,\n    logits_cache: bool = True,\n    max_length=None,\n    device=\"cuda\",\n    dtype=\"auto\",\n    batch_size=1,\n    max_batch_size=64,\n    trust_remote_code: bool | None = False,\n    use_fast_tokenizer: bool | None = True,\n    add_bos_token: bool | None = False,\n    prefix_token_id: int | None = None,\n    # arguments used for splitting a model across GPUs naively.\n    # only used if `parallelize=True`.\n    parallelize: bool | None = False,\n    device_map_option=\"auto\",\n    max_memory_per_gpu=None,\n    max_cpu_memory=None,\n    offload_folder=\"./offload\",\n    # PEFT, delta weights and quantization options\n    peft: str | None = None,\n    delta: str | None = None,\n    autogptq: bool | str | None = False,\n    **kwargs,\n) -&gt; None:\n    super().__init__()\n\n    # optionally: take in an already-initialized transformers.PreTrainedModel\n    eval_logger.warning(\n        \"`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\"\n    )\n    assert not parallelize, \"`parallelize=True` is not compatible with passing pre-initialized model to `pretrained`\"\n    self._model = pretrained\n    self._device = self._model.device\n    self._config = self._model.config\n\n    if tokenizer:\n        assert isinstance(\n            tokenizer, transformers.PreTrainedTokenizer\n        ) or isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n        self.tokenizer = tokenizer\n    else:\n        # Get tokenizer\n        model_name = self._model.name_or_path\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n            model_name,\n            revision=revision,\n            trust_remote_code=trust_remote_code,\n            use_fast=use_fast_tokenizer,\n        )\n\n    # determine which of 'causal' and 'seq2seq' backends to use\n    self._get_backend(\n        config=self.config, backend=backend, trust_remote_code=trust_remote_code\n    )\n\n    # load tokenizer so we know tokenizer vocabulary size before loading model and PEFT\n    self._create_tokenizer(\n        pretrained,\n        tokenizer,\n        revision=revision,\n        trust_remote_code=trust_remote_code,\n        use_fast_tokenizer=use_fast_tokenizer,\n    )\n\n    # access self._model through self.model property outside this method\n    if isinstance(self.model, torch.nn.Module):\n        self.model.eval()\n        self.model.tie_weights()\n    self.truncation = truncation\n    self.logits_cache = logits_cache\n    self.vocab_size = self.tokenizer.vocab_size\n    # select (or create) a pad token to use\n    self.tokenizer = configure_pad_token(self.tokenizer, model_config=self.config)\n\n    self.add_bos_token = add_bos_token\n    if \"gemma\" in getattr(self.config, \"model_type\", \"\"):\n        self.add_bos_token = True\n        eval_logger.info(\n            f\"Model type is '{self.config.model_type}', part of the Gemma family--a BOS token will be used as Gemma underperforms without it.\"\n        )\n\n    self._max_length = max_length\n    self.pretrained = pretrained\n    self.delta = delta\n    self.peft = peft\n    self.revision = revision\n    self.batch_schedule: float = 1\n    self.batch_sizes: dict = {}\n    self.max_batch_size = max_batch_size\n\n    if str(batch_size).startswith(\"auto\"):\n        batch_size = batch_size.split(\":\")\n        self.batch_size_per_gpu = batch_size[0]\n        self.batch_schedule = float(batch_size[1]) if len(batch_size) &gt; 1 else 1\n    else:\n        self.batch_size_per_gpu = int(batch_size)\n\n    # if a PreTrainedModel was passed into HFLM, we forgo distributed setup.\n    eval_logger.warning(\n        \"Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\"\n    )\n    self._rank = 0\n    self._world_size = 1\n\n    self.custom_prefix_token_id = prefix_token_id\n    if prefix_token_id is not None:\n        eval_logger.info(\n            f\"Loglikelihood prefix token id used in evaluation: {self.prefix_token_id}\"\n        )\n</code></pre>"},{"location":"api/whittle/eval/whittle_llms/#whittle.eval.whittle_llms.WhittleLM.apply_chat_template","title":"apply_chat_template","text":"<pre><code>apply_chat_template(\n    chat_history: list[dict[str, str]]\n) -&gt; str\n</code></pre> <p>Method to apply a chat template to a list of chat history between user and model.</p> Source code in <code>whittle/eval/whittle_llms.py</code> <pre><code>def apply_chat_template(self, chat_history: list[dict[str, str]]) -&gt; str:\n    \"\"\"\n    Method to apply a chat template to a list of chat history between user and model.\n    \"\"\"\n    return self.tokenizer.apply_chat_template(\n        chat_history, tokenize=False, add_generation_prompt=True\n    )\n</code></pre>"},{"location":"api/whittle/eval/whittle_llms/#whittle.eval.whittle_llms.WhittleLM.tok_encode","title":"tok_encode","text":"<pre><code>tok_encode(\n    string: str,\n    left_truncate_len=None,\n    add_special_tokens=None,\n) -&gt; list[int]\n</code></pre> Source code in <code>whittle/eval/whittle_llms.py</code> <pre><code>def tok_encode(\n    self, string: str, left_truncate_len=None, add_special_tokens=None\n) -&gt; list[int]:\n    \"\"\" \"\"\"\n    # default for None - empty dict, use predefined tokenizer param\n    # used for all models except for CausalLM or predefined value\n    special_tokens_kwargs = {}\n\n    # by default for CausalLM - false or self.add_bos_token is set\n    if add_special_tokens is None:\n        if self.AUTO_MODEL_CLASS == GPT:\n            special_tokens_kwargs = {\n                \"add_special_tokens\": False or self.add_bos_token\n            }\n    # otherwise the method explicitly defines the value\n    else:\n        special_tokens_kwargs = {\"add_special_tokens\": add_special_tokens}\n\n    encoding = self.tokenizer.encode(string, **special_tokens_kwargs)\n\n    # left-truncate the encoded context to be at most `left_truncate_len` tokens long\n    if left_truncate_len:\n        encoding = encoding[-left_truncate_len:]\n\n    return encoding\n</code></pre>"},{"location":"api/whittle/eval/whittle_llms/#whittle.eval.whittle_llms.configure_pad_token","title":"configure_pad_token","text":"<pre><code>configure_pad_token(\n    tokenizer: PreTrainedTokenizerBase,\n    model_config: PretrainedConfig | None = None,\n) -&gt; PreTrainedTokenizerBase\n</code></pre> <p>This function checks if the (Hugging Face) tokenizer has a padding token and sets it if not present. Some tokenizers require special handling.</p> PARAMETER DESCRIPTION <code>tokenizer</code> <p>The tokenizer for which the padding token is to be handled.</p> <p> TYPE: <code>PreTrainedTokenizerBase</code> </p> <code>model_config</code> <p>The configuration of the model. Default is None.</p> <p> TYPE: <code>PretrainedConfig | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>PreTrainedTokenizerBase</code> <p>The tokenizer after the padding token has been handled.</p> RAISES DESCRIPTION <code>AssertionError</code> <p>If the tokenizer is of type RWKVWorldTokenizer or Rwkv5Tokenizer and the padding token id is not 0.</p> Source code in <code>whittle/eval/whittle_llms.py</code> <pre><code>def configure_pad_token(\n    tokenizer: PreTrainedTokenizerBase,\n    model_config: PretrainedConfig | None = None,\n) -&gt; PreTrainedTokenizerBase:\n    \"\"\"\n    This function checks if the (Hugging Face) tokenizer has a padding token and sets it if not present.\n    Some tokenizers require special handling.\n\n    Args:\n        tokenizer: The tokenizer for which the padding token is to be handled.\n        model_config: The configuration of the model. Default is None.\n\n    Returns:\n        The tokenizer after the padding token has been handled.\n\n    Raises:\n        AssertionError: If the tokenizer is of type RWKVWorldTokenizer or Rwkv5Tokenizer and the padding token id is not 0.\n    \"\"\"\n    if tokenizer.pad_token:\n        pass\n    elif tokenizer.unk_token:\n        tokenizer.pad_token_id = tokenizer.unk_token_id\n    elif tokenizer.eos_token:\n        tokenizer.pad_token_id = tokenizer.eos_token_id\n    else:\n        # handle special cases\n        if model_config and getattr(model_config, \"model_type\", None) == \"qwen\":\n            # Qwen's trust_remote_code tokenizer does not allow for adding special tokens\n            tokenizer.pad_token = \"&lt;|endoftext|&gt;\"\n        elif (\n            tokenizer.__class__.__name__ == \"RWKVWorldTokenizer\"\n            or tokenizer.__class__.__name__ == \"Rwkv5Tokenizer\"\n        ):\n            # The RWKV world tokenizer, does not allow for adding special tokens / setting the pad token (which is set as 0)\n            # The additional tokenizer name check is needed, as there exists rwkv4 models with neox tokenizer\n            # ---\n            # Note that the world tokenizer class name, might change in the future for the final huggingface merge\n            # https://github.com/huggingface/transformers/pull/26963\n            assert tokenizer.pad_token_id == 0\n        else:\n            tokenizer.add_special_tokens({\"pad_token\": \"&lt;|pad|&gt;\"})\n\n    return tokenizer\n</code></pre>"},{"location":"api/whittle/loss/kd_loss/","title":"Kd loss","text":""},{"location":"api/whittle/loss/kd_loss/#whittle.loss.kd_loss","title":"whittle.loss.kd_loss","text":""},{"location":"api/whittle/loss/kd_loss/#whittle.loss.kd_loss.DistillLoss","title":"DistillLoss","text":"<pre><code>DistillLoss(temperature, distillation_weight)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Custom loss function for knowledge distillation</p> <p>This loss function combines the standard cross-entropy loss with the KL divergence between the soft targets produced by a teacher model and a student model.</p> ATTRIBUTE DESCRIPTION <code>temperature</code> <p>The temperature used for distillation. Higher temperatures                  produce softer probability distributions.</p> <p> TYPE: <code>float</code> </p> <code>distillation_weight</code> <p>The weight factor that balances the importance of                          the distillation loss and the hard target loss.</p> <p> TYPE: <code>float</code> </p> <code>kldiv</code> <p>The KL divergence loss function.</p> <p> TYPE: <code>KLDivLoss</code> </p> PARAMETER DESCRIPTION <code>temperature</code> <p>The temperature for distillation.</p> <p> TYPE: <code>float</code> </p> <code>distillation_weight</code> <p>The weight factor for the distillation loss.</p> <p> TYPE: <code>float</code> </p> Source code in <code>whittle/loss/kd_loss.py</code> <pre><code>def __init__(self, temperature, distillation_weight):\n    \"\"\"\n    Initializes the DistillLoss module.\n\n    Args:\n        temperature (float): The temperature for distillation.\n        distillation_weight (float): The weight factor for the distillation loss.\n    \"\"\"\n    super().__init__()\n\n    self.temperature = temperature\n    self.distillation_weight = distillation_weight\n    self.kldiv = nn.KLDivLoss(reduction=\"batchmean\")\n</code></pre>"},{"location":"api/whittle/loss/kd_loss/#whittle.loss.kd_loss.DistillLoss.forward","title":"forward","text":"<pre><code>forward(outputs, labels, outputs_teacher)\n</code></pre> <p>Compute the distillation loss.</p> <p>This method computes the loss as a weighted sum of the soft target loss (KL divergence between student and teacher outputs) and the hard target loss (cross-entropy loss between student outputs and ground truth labels).</p> PARAMETER DESCRIPTION <code>outputs</code> <p>The logits output by the student model. Shape (batch_size, num_classes).</p> <p> TYPE: <code>Tensor</code> </p> <code>labels</code> <p>The ground truth labels. Shape (batch_size).</p> <p> TYPE: <code>Tensor</code> </p> <code>outputs_teacher</code> <p>The logits output by the teacher model. Shape (batch_size, num_classes).</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <p>torch.Tensor: The combined loss.</p> Source code in <code>whittle/loss/kd_loss.py</code> <pre><code>def forward(self, outputs, labels, outputs_teacher):\n    \"\"\"\n    Compute the distillation loss.\n\n    This method computes the loss as a weighted sum of the soft target loss (KL divergence\n    between student and teacher outputs) and the hard target loss (cross-entropy loss\n    between student outputs and ground truth labels).\n\n    Args:\n        outputs (torch.Tensor): The logits output by the student model. Shape (batch_size, num_classes).\n        labels (torch.Tensor): The ground truth labels. Shape (batch_size).\n        outputs_teacher (torch.Tensor): The logits output by the teacher model. Shape (batch_size, num_classes).\n\n    Returns:\n        torch.Tensor: The combined loss.\n    \"\"\"\n    soft_target_loss = 0\n    if outputs_teacher is not None and self.distillation_weight &gt; 0:\n        soft_target_loss = self.kldiv(\n            F.log_softmax(outputs / self.temperature, dim=1),\n            F.softmax(outputs_teacher / self.temperature, dim=1),\n        ) * (self.temperature**2)\n\n    hard_target_loss = F.cross_entropy(outputs, labels, reduction=\"mean\")\n\n    total_loss = soft_target_loss * self.distillation_weight + hard_target_loss * (\n        1 - self.distillation_weight\n    )\n\n    return total_loss\n</code></pre>"},{"location":"api/whittle/metrics/magnitude/","title":"Magnitude","text":""},{"location":"api/whittle/metrics/magnitude/#whittle.metrics.magnitude","title":"whittle.metrics.magnitude","text":""},{"location":"api/whittle/metrics/parameters/","title":"Parameters","text":""},{"location":"api/whittle/metrics/parameters/#whittle.metrics.parameters","title":"whittle.metrics.parameters","text":""},{"location":"api/whittle/metrics/parameters/#whittle.metrics.parameters.compute_parameters_sub_network_gpt","title":"compute_parameters_sub_network_gpt","text":"<pre><code>compute_parameters_sub_network_gpt(model: GPT)\n</code></pre> <p>Computes parameters of the current sub-network of a GPT mmodel. Make sure to set the sub-network before calling this function.</p> Refs <p>towardsdatascience.com/how-to-estimate-the-number-of-parameters-in-transformer-models-ca0f57d8dff0</p> PARAMETER DESCRIPTION <code>model</code> <p>GPT model</p> <p> TYPE: <code>GPT</code> </p> RETURNS DESCRIPTION <code>float</code> <p>number of parameters of the activated sub-network</p> Source code in <code>whittle/metrics/parameters.py</code> <pre><code>def compute_parameters_sub_network_gpt(model: GPT):\n    \"\"\"\n    Computes parameters of the current sub-network of a GPT mmodel. Make sure to set the sub-network before\n    calling this function.\n\n    Refs:\n        https://towardsdatascience.com/how-to-estimate-the-number-of-parameters-in-transformer-models-ca0f57d8dff0\n\n    Args:\n        model: GPT model\n\n    Returns:\n        float: number of parameters of the activated sub-network\n    \"\"\"\n\n    num_params = 0\n    num_params += params_linear_layer(model.lm_head)\n    num_params += params_embedding_layer(model.transformer.wte)\n    for i in range(model.sub_network_n_layers):\n        block = model.transformer.h[i]\n        num_params += params_mlp(block.mlp)\n        num_params += params_attention_layer(block.attn)\n\n        num_params += params_layer_normalization(block.norm_1)\n        num_params += params_layer_normalization(block.norm_2)\n    num_params += params_layer_normalization(model.transformer.ln_f)\n    return num_params\n</code></pre>"},{"location":"api/whittle/models/gpt/extract/","title":"Extract","text":""},{"location":"api/whittle/models/gpt/extract/#whittle.models.gpt.extract","title":"whittle.models.gpt.extract","text":""},{"location":"api/whittle/models/gpt/model/","title":"Model","text":""},{"location":"api/whittle/models/gpt/model/#whittle.models.gpt.model","title":"whittle.models.gpt.model","text":"<p>Full definition of a decoder-only transformer-based language model, all of it in this single file.</p> <p>Based on the nanoGPT implementation: karpathy/nanoGPT and github.com/EleutherAI/gpt-neox/tree/main/megatron/model.</p>"},{"location":"api/whittle/models/gpt/model/#whittle.models.gpt.model.GPT","title":"GPT","text":"<pre><code>GPT(config: Config)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>whittle/models/gpt/model.py</code> <pre><code>def __init__(self, config: Config) -&gt; None:\n    super().__init__()\n    assert config.padded_vocab_size is not None\n    self.config = config\n    self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    self.lm_head = Linear(\n        config.n_embd, config.padded_vocab_size, bias=config.lm_head_bias\n    )\n    self.transformer = nn.ModuleDict(\n        dict(\n            wte=Embedding(config.padded_vocab_size, config.n_embd),\n            h=nn.ModuleList(Block(config) for i in range(config.n_layer)),\n            ln_f=self.norm_class(config.n_embd, eps=config.norm_eps),\n        )\n    )\n    self.max_layer = config.n_layer\n    self.max_seq_length = self.config.block_size\n    self.mask_cache: torch.Tensor | None = None\n\n    # Set current sub-network to super-network\n    self.sub_network_n_embd = self.config.n_embd\n    self.sub_network_intermediate_size = self.config.intermediate_size\n    self.sub_network_num_heads = self.config.n_head\n    self.sub_network_n_layers = self.config.n_layer\n    self.cos: torch.Tensor\n    self.sin: torch.Tensor\n    self.random_layers = list(range(self.config.n_layer))\n    self.config.is_encoder_decoder = False\n    self.main_input_name = \"input_pos\"\n    self._supports_cache_class = True\n    self.sub_network_head_size = None\n</code></pre>"},{"location":"api/whittle/models/gpt/utils/","title":"Utils","text":""},{"location":"api/whittle/models/gpt/utils/#whittle.models.gpt.utils","title":"whittle.models.gpt.utils","text":"<p>Utility functions for training and inference.</p>"},{"location":"api/whittle/models/gpt/utils/#whittle.models.gpt.utils.CycleIterator","title":"CycleIterator","text":"<pre><code>CycleIterator(iterable: Iterable)\n</code></pre> <p>An iterator that cycles through an iterable indefinitely.</p> Example <p>iterator = CycleIterator([1, 2, 3]) [next(iterator) for _ in range(5)] [1, 2, 3, 1, 2]</p> Note <p>Unlike <code>itertools.cycle</code>, this iterator does not cache the values of the iterable.</p> Source code in <code>whittle/models/gpt/utils.py</code> <pre><code>def __init__(self, iterable: Iterable) -&gt; None:\n    self.iterable = iterable\n    self.epoch = 0\n    self._iterator: Iterator | None = None\n</code></pre>"},{"location":"api/whittle/models/gpt/utils/#whittle.models.gpt.utils.estimate_flops","title":"estimate_flops","text":"<pre><code>estimate_flops(model: GPT, training: bool) -&gt; int\n</code></pre> <p>Measures estimated FLOPs for MFU.</p> Refs <ul> <li>ar5iv.labs.arxiv.org/html/2205.05198#A1</li> <li>ar5iv.labs.arxiv.org/html/2204.02311#A2</li> </ul> Source code in <code>whittle/models/gpt/utils.py</code> <pre><code>def estimate_flops(model: GPT, training: bool) -&gt; int:\n    \"\"\"Measures estimated FLOPs for MFU.\n\n    Refs:\n        * https://ar5iv.labs.arxiv.org/html/2205.05198#A1\n        * https://ar5iv.labs.arxiv.org/html/2204.02311#A2\n    \"\"\"\n    # using all parameters for this is a naive over estimation because not all model parameters actually contribute to\n    # this FLOP computation (e.g. embedding, norm). For this reason, the result will be higher by a fixed percentage\n    # (~10%) compared to the measured FLOPs, making those lower but more realistic.\n    # For a proper estimate, this needs a more fine-grained calculation as in Appendix A of the paper.\n    n_trainable_params = num_parameters(model, requires_grad=True)\n    trainable_flops = flops_per_param(\n        model.max_seq_length,\n        model.config.n_layer,\n        model.config.n_embd,\n        n_trainable_params,\n    )\n    # forward + backward + gradients (assumes no gradient accumulation)\n    ops_per_step = 3 if training else 1\n    n_frozen_params = num_parameters(model, requires_grad=False)\n    frozen_flops = flops_per_param(\n        model.max_seq_length, model.config.n_layer, model.config.n_embd, n_frozen_params\n    )\n    # forward + backward\n    frozen_ops_per_step = 2 if training else 1\n    return ops_per_step * trainable_flops + frozen_ops_per_step * frozen_flops\n</code></pre>"},{"location":"api/whittle/models/gpt/utils/#whittle.models.gpt.utils.get_default_supported_precision","title":"get_default_supported_precision","text":"<pre><code>get_default_supported_precision(training: bool) -&gt; str\n</code></pre> <p>Return default precision that is supported by the hardware: either <code>bf16</code> or <code>16</code>.</p> PARAMETER DESCRIPTION <code>training</code> <p><code>-mixed</code> or <code>-true</code> version of the precision to use</p> <p> TYPE: <code>bool</code> </p> RETURNS DESCRIPTION <code>str</code> <p>default precision that is suitable for the task and is supported by the hardware</p> Source code in <code>whittle/models/gpt/utils.py</code> <pre><code>def get_default_supported_precision(training: bool) -&gt; str:\n    \"\"\"Return default precision that is supported by the hardware: either `bf16` or `16`.\n\n    Args:\n        training: `-mixed` or `-true` version of the precision to use\n\n    Returns:\n        default precision that is suitable for the task and is supported by the hardware\n    \"\"\"\n    from lightning.fabric.accelerators import MPSAccelerator\n\n    if MPSAccelerator.is_available() or (\n        torch.cuda.is_available() and not torch.cuda.is_bf16_supported()\n    ):\n        return \"16-mixed\" if training else \"16-true\"\n    return \"bf16-mixed\" if training else \"bf16-true\"\n</code></pre>"},{"location":"api/whittle/models/gpt/utils/#whittle.models.gpt.utils.sample_config","title":"sample_config","text":"<pre><code>sample_config(\n    choices_dict: dict,\n    layer_sampling_scheme: str = \"normal\",\n    seed: int = 0,\n) -&gt; dict\n</code></pre> <p>Sample a configuration from a dictionary of choices.</p> PARAMETER DESCRIPTION <code>choices_dict</code> <p>a dictionary of choices for each architectural dimension</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>a dictionary with the same keys as <code>choices_dict</code> and the sampled choices</p> Source code in <code>whittle/models/gpt/utils.py</code> <pre><code>def sample_config(\n    choices_dict: dict, layer_sampling_scheme: str = \"normal\", seed: int = 0\n) -&gt; dict:\n    \"\"\"Sample a configuration from a dictionary of choices.\n\n    Args:\n        choices_dict: a dictionary of choices for each architectural dimension\n\n    Returns:\n        a dictionary with the same keys as `choices_dict` and the sampled choices\n    \"\"\"\n    sampled_dict = {}\n    r = random.Random(seed)\n    # sample embed dim -&gt; held constant throughout transformer\n    sampled_dict[\"sample_embed_dim\"] = r.choice(choices_dict[\"embed_dim_choices\"])\n    # sample number of layers\n    sampled_dict[\"sample_n_layer\"] = r.choice(choices_dict[\"n_layer_choices\"])\n    # sample layer indices\n    max_layer = max(choices_dict[\"n_layer_choices\"])\n    sampled_dict[\"sample_layer_indices\"] = []\n    if layer_sampling_scheme == \"normal\":\n        sampled_dict[\"sample_layer_indices\"] = list(\n            range(sampled_dict[\"sample_n_layer\"])\n        )\n    elif layer_sampling_scheme == \"strided\":\n        if sampled_dict[\"sample_n_layer\"] == max_layer:\n            sampled_dict[\"sample_layer_indices\"] = list(\n                range(sampled_dict[\"sample_n_layer\"])\n            )\n        elif sampled_dict[\"sample_n_layer\"] == max_layer - 1:\n            sampled_dict[\"sample_layer_indices\"] = list(\n                range(sampled_dict[\"sample_n_layer\"])\n            )\n            sampled_dict[\"sample_layer_indices\"][-1] = max_layer - 1\n        else:\n            increment_floor = max_layer // sampled_dict[\"sample_n_layer\"]\n            increment_ceil = int(np.ceil(max_layer / sampled_dict[\"sample_n_layer\"]))\n            sampled_dict[\"sample_layer_indices\"] = [\n                0 for _ in range(sampled_dict[\"sample_n_layer\"])\n            ]\n            counter_layer = 0\n            for i in range(sampled_dict[\"sample_n_layer\"]):\n                if counter_layer &lt; (max_layer // 2):\n                    sampled_dict[\"sample_layer_indices\"][i] = counter_layer\n                    counter_layer += increment_ceil\n                else:\n                    sampled_dict[\"sample_layer_indices\"][i] = counter_layer\n                    counter_layer += increment_floor\n\n            sampled_dict[\"sample_layer_indices\"][0] = 0\n            sampled_dict[\"sample_layer_indices\"][-1] = max_layer - 1\n\n    # sample number of heads\n    sampled_dict[\"sample_n_head\"] = [\n        r.choice(choices_dict[\"n_head_choices\"]) for _ in range(max_layer)\n    ]\n    # sample mlp ratio\n    sampled_dict[\"sample_mlp_ratio\"] = [\n        r.choice(choices_dict[\"mlp_ratio_choices\"]) for _ in range(max_layer)\n    ]\n    # sample bias\n    sampled_dict[\"sample_bias\"] = r.choice(choices_dict[\"bias_choices\"])\n\n    return sampled_dict\n</code></pre>"},{"location":"api/whittle/models/gpt/utils/#whittle.models.gpt.utils.sample_config_max","title":"sample_config_max","text":"<pre><code>sample_config_max(\n    choices_dict: dict,\n    layer_sampling_scheme: str = \"normal\",\n) -&gt; dict\n</code></pre> <p>Sample a configuration from a dictionary of choices.</p> PARAMETER DESCRIPTION <code>choices_dict</code> <p>a dictionary of choices for each architectural dimension</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>a dictionary with the same keys as <code>choices_dict</code> and the sampled choices</p> Source code in <code>whittle/models/gpt/utils.py</code> <pre><code>def sample_config_max(\n    choices_dict: dict, layer_sampling_scheme: str = \"normal\"\n) -&gt; dict:\n    \"\"\"Sample a configuration from a dictionary of choices.\n\n    Args:\n        choices_dict: a dictionary of choices for each architectural dimension\n\n    Returns:\n        a dictionary with the same keys as `choices_dict` and the sampled choices\n    \"\"\"\n    sampled_dict = {}\n    # sample embed dim -&gt; held constant throughout transformer\n    sampled_dict[\"sample_embed_dim\"] = max(choices_dict[\"embed_dim_choices\"])\n    # sample number of layers\n    sampled_dict[\"sample_n_layer\"] = max(choices_dict[\"n_layer_choices\"])\n    # sample layer indices\n    max_layer = max(choices_dict[\"n_layer_choices\"])\n    sampled_dict[\"sample_layer_indices\"] = []\n    sampled_dict[\"sample_layer_indices\"] = list(range(sampled_dict[\"sample_n_layer\"]))\n\n    # sample number of heads\n    sampled_dict[\"sample_n_head\"] = [\n        max(choices_dict[\"n_head_choices\"]) for _ in range(max_layer)\n    ]\n    # sample mlp ratio\n    sampled_dict[\"sample_mlp_ratio\"] = [\n        max(choices_dict[\"mlp_ratio_choices\"]) for _ in range(max_layer)\n    ]\n    # sample bias\n    sampled_dict[\"sample_bias\"] = True\n\n    return sampled_dict\n</code></pre>"},{"location":"api/whittle/models/gpt/utils/#whittle.models.gpt.utils.sample_config_mid","title":"sample_config_mid","text":"<pre><code>sample_config_mid(\n    choices_dict: dict,\n    layer_sampling_scheme: str = \"normal\",\n) -&gt; dict\n</code></pre> <p>Sample a configuration from a dictionary of choices.</p> PARAMETER DESCRIPTION <code>choices_dict</code> <p>a dictionary of choices for each architectural dimension</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>a dictionary with the same keys as <code>choices_dict</code> and the sampled choices</p> Source code in <code>whittle/models/gpt/utils.py</code> <pre><code>def sample_config_mid(\n    choices_dict: dict, layer_sampling_scheme: str = \"normal\"\n) -&gt; dict:\n    \"\"\"Sample a configuration from a dictionary of choices.\n\n    Args:\n        choices_dict: a dictionary of choices for each architectural dimension\n\n    Returns:\n        a dictionary with the same keys as `choices_dict` and the sampled choices\n    \"\"\"\n    sampled_dict = {}\n    # sample embed dim -&gt; held constant throughout transformer\n    sampled_dict[\"sample_embed_dim\"] = choices_dict[\"embed_dim_choices\"][1]\n    # sample number of layers\n    sampled_dict[\"sample_n_layer\"] = choices_dict[\"n_layer_choices\"][1]\n    # sample layer indices\n    max_layer = choices_dict[\"n_layer_choices\"][1]\n    sampled_dict[\"sample_layer_indices\"] = []\n    sampled_dict[\"sample_layer_indices\"] = list(range(sampled_dict[\"sample_n_layer\"]))\n\n    # sample number of heads\n    sampled_dict[\"sample_n_head\"] = [\n        choices_dict[\"n_head_choices\"][1] for _ in range(max_layer)\n    ]\n    # sample mlp ratio\n    sampled_dict[\"sample_mlp_ratio\"] = [\n        choices_dict[\"mlp_ratio_choices\"][1] for _ in range(max_layer)\n    ]\n    # sample bias\n    sampled_dict[\"sample_bias\"] = True\n\n    return sampled_dict\n</code></pre>"},{"location":"api/whittle/models/gpt/utils/#whittle.models.gpt.utils.sample_config_min","title":"sample_config_min","text":"<pre><code>sample_config_min(\n    choices_dict: dict,\n    layer_sampling_scheme: str = \"normal\",\n) -&gt; dict\n</code></pre> <p>Sample a configuration from a dictionary of choices.</p> PARAMETER DESCRIPTION <code>choices_dict</code> <p>a dictionary of choices for each architectural dimension</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>a dictionary with the same keys as <code>choices_dict</code> and the sampled choices</p> Source code in <code>whittle/models/gpt/utils.py</code> <pre><code>def sample_config_min(\n    choices_dict: dict, layer_sampling_scheme: str = \"normal\"\n) -&gt; dict:\n    \"\"\"Sample a configuration from a dictionary of choices.\n\n    Args:\n        choices_dict: a dictionary of choices for each architectural dimension\n\n    Returns:\n        a dictionary with the same keys as `choices_dict` and the sampled choices\n    \"\"\"\n    sampled_dict = {}\n    # sample embed dim -&gt; held constant throughout transformer\n    sampled_dict[\"sample_embed_dim\"] = min(choices_dict[\"embed_dim_choices\"])\n    # sample number of layers\n    sampled_dict[\"sample_n_layer\"] = min(choices_dict[\"n_layer_choices\"])\n    # sample layer indices\n    max_layer = min(choices_dict[\"n_layer_choices\"])\n    sampled_dict[\"sample_layer_indices\"] = []\n    sampled_dict[\"sample_layer_indices\"] = list(range(sampled_dict[\"sample_n_layer\"]))\n\n    # sample number of heads\n    sampled_dict[\"sample_n_head\"] = [\n        min(choices_dict[\"n_head_choices\"]) for _ in range(max_layer)\n    ]\n    # sample mlp ratio\n    sampled_dict[\"sample_mlp_ratio\"] = [\n        min(choices_dict[\"mlp_ratio_choices\"]) for _ in range(max_layer)\n    ]\n    # sample bias\n    sampled_dict[\"sample_bias\"] = True\n\n    return sampled_dict\n</code></pre>"},{"location":"api/whittle/models/gpt/blocks/causal_self_attention/","title":"Causal self attention","text":""},{"location":"api/whittle/models/gpt/blocks/causal_self_attention/#whittle.models.gpt.blocks.causal_self_attention","title":"whittle.models.gpt.blocks.causal_self_attention","text":""},{"location":"api/whittle/models/gpt/blocks/mlp/","title":"Mlp","text":""},{"location":"api/whittle/models/gpt/blocks/mlp/#whittle.models.gpt.blocks.mlp","title":"whittle.models.gpt.blocks.mlp","text":""},{"location":"api/whittle/models/gpt/blocks/transformer_block/","title":"Transformer block","text":""},{"location":"api/whittle/models/gpt/blocks/transformer_block/#whittle.models.gpt.blocks.transformer_block","title":"whittle.models.gpt.blocks.transformer_block","text":""},{"location":"api/whittle/modules/embedding/","title":"Embedding","text":""},{"location":"api/whittle/modules/embedding/#whittle.modules.embedding","title":"whittle.modules.embedding","text":""},{"location":"api/whittle/modules/layernorm/","title":"Layernorm","text":""},{"location":"api/whittle/modules/layernorm/#whittle.modules.layernorm","title":"whittle.modules.layernorm","text":""},{"location":"api/whittle/modules/linear/","title":"Linear","text":""},{"location":"api/whittle/modules/linear/#whittle.modules.linear","title":"whittle.modules.linear","text":""},{"location":"api/whittle/modules/linear/#whittle.modules.linear.Linear","title":"Linear","text":"<pre><code>Linear(\n    in_features: int,\n    out_features: int,\n    bias: bool = True,\n    device=None,\n    dtype=None,\n)\n</code></pre> <p>               Bases: <code>Linear</code></p> Source code in <code>whittle/modules/linear.py</code> <pre><code>def __init__(\n    self,\n    in_features: int,\n    out_features: int,\n    bias: bool = True,\n    device=None,\n    dtype=None,\n):\n    \"\"\" \"\"\"\n    super().__init__(in_features, out_features, bias, device, dtype)\n\n    # Set the current sub-network dimensions equal to super-network\n    self.sub_network_in_features = in_features\n    self.sub_network_out_features = out_features\n    self.use_bias = bias\n    self.random_indices_in_features = torch.arange(self.sub_network_in_features)\n    self.random_indices_out_features = torch.arange(self.sub_network_out_features)\n</code></pre>"},{"location":"api/whittle/modules/rmsnorm/","title":"Rmsnorm","text":""},{"location":"api/whittle/modules/rmsnorm/#whittle.modules.rmsnorm","title":"whittle.modules.rmsnorm","text":""},{"location":"api/whittle/modules/rmsnorm/#whittle.modules.rmsnorm.RMSNorm","title":"RMSNorm","text":"<pre><code>RMSNorm(\n    in_features: int,\n    dim: int = -1,\n    eps: float = 1e-06,\n    add_unit_offset: bool = False,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Root Mean Square Layer Normalization.</p> <p>Derived from github.com/bzhangGo/rmsnorm/blob/master/rmsnorm_torch.py. BSD 3-Clause License: github.com/bzhangGo/rmsnorm/blob/master/LICENSE.</p> Source code in <code>whittle/modules/rmsnorm.py</code> <pre><code>def __init__(\n    self,\n    in_features: int,\n    dim: int = -1,\n    eps: float = 1e-6,\n    add_unit_offset: bool = False,\n) -&gt; None:\n    super().__init__()\n    self.in_features = in_features\n    self.weight = torch.nn.Parameter(torch.ones(in_features))\n    self.eps = eps\n    self.dim = dim\n    self.add_unit_offset = add_unit_offset\n    self.sub_network_in_features: int | None = in_features\n    self.random_indices = torch.arange(self.in_features)\n</code></pre>"},{"location":"api/whittle/sampling/random_sampler/","title":"Random sampler","text":""},{"location":"api/whittle/sampling/random_sampler/#whittle.sampling.random_sampler","title":"whittle.sampling.random_sampler","text":""},{"location":"api/whittle/search/ask_tell_scheduler/","title":"Ask tell scheduler","text":""},{"location":"api/whittle/search/ask_tell_scheduler/#whittle.search.ask_tell_scheduler","title":"whittle.search.ask_tell_scheduler","text":""},{"location":"api/whittle/search/ask_tell_scheduler/#whittle.search.ask_tell_scheduler.AskTellScheduler","title":"AskTellScheduler","text":"<pre><code>AskTellScheduler(base_scheduler: TrialScheduler)\n</code></pre> Source code in <code>whittle/search/ask_tell_scheduler.py</code> <pre><code>def __init__(self, base_scheduler: TrialScheduler):\n    self.bscheduler = base_scheduler\n    self.trial_counter = 0\n    self.completed_experiments = {}\n</code></pre>"},{"location":"api/whittle/search/ask_tell_scheduler/#whittle.search.ask_tell_scheduler.AskTellScheduler.ask","title":"ask","text":"<pre><code>ask() -&gt; Trial\n</code></pre> <p>Ask the scheduler for new trial to run :return: Trial to run</p> Source code in <code>whittle/search/ask_tell_scheduler.py</code> <pre><code>def ask(self) -&gt; Trial:\n    \"\"\"\n    Ask the scheduler for new trial to run\n    :return: Trial to run\n    \"\"\"\n    trial_suggestion = self.bscheduler.suggest(self.trial_counter)\n    trial = Trial(\n        trial_id=self.trial_counter,\n        config=trial_suggestion.config,\n        creation_time=datetime.datetime.now(),\n    )\n    self.trial_counter += 1\n    return trial\n</code></pre>"},{"location":"api/whittle/search/ask_tell_scheduler/#whittle.search.ask_tell_scheduler.AskTellScheduler.best_trial","title":"best_trial","text":"<pre><code>best_trial(metris: str) -&gt; TrialResult\n</code></pre> <p>Return the best trial according to the provided metric</p> Source code in <code>whittle/search/ask_tell_scheduler.py</code> <pre><code>def best_trial(self, metris: str) -&gt; TrialResult:\n    \"\"\"\n    Return the best trial according to the provided metric\n    \"\"\"\n    if self.bscheduler.mode == \"max\":\n        sign = 1.0\n    else:\n        sign = -1.0\n\n    return max(\n        [value for key, value in self.completed_experiments.items()],\n        key=lambda trial: sign * trial.metrics[metris],\n    )\n</code></pre>"},{"location":"api/whittle/search/ask_tell_scheduler/#whittle.search.ask_tell_scheduler.AskTellScheduler.tell","title":"tell","text":"<pre><code>tell(trial: Trial, experiment_result: dict[str, float])\n</code></pre> <p>Feed experiment results back to the Scheduler</p> <p>:param trial: Trial that was run :param experiment_result: {metric: value} dictionary with experiment results</p> Source code in <code>whittle/search/ask_tell_scheduler.py</code> <pre><code>def tell(self, trial: Trial, experiment_result: dict[str, float]):\n    \"\"\"\n    Feed experiment results back to the Scheduler\n\n    :param trial: Trial that was run\n    :param experiment_result: {metric: value} dictionary with experiment results\n    \"\"\"\n    trial_result = trial.add_results(\n        metrics=experiment_result,\n        status=Status.completed,\n        training_end_time=datetime.datetime.now(),\n    )\n    self.bscheduler.on_trial_complete(trial=trial, result=experiment_result)\n    self.completed_experiments[trial_result.trial_id] = trial_result\n</code></pre>"},{"location":"api/whittle/search/baselines/","title":"Baselines","text":""},{"location":"api/whittle/search/baselines/#whittle.search.baselines","title":"whittle.search.baselines","text":""},{"location":"api/whittle/search/local_search/","title":"Local search","text":""},{"location":"api/whittle/search/local_search/#whittle.search.local_search","title":"whittle.search.local_search","text":""},{"location":"api/whittle/search/local_search/#whittle.search.local_search.LS","title":"LS","text":"<pre><code>LS(\n    config_space: dict[str, Any],\n    metric: list[str],\n    mode: list[str] | str = \"min\",\n    start_point: dict[str, Any] | None = None,\n    random_seed: int | None = None,\n    points_to_evaluate: list[dict] | None = None,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>FIFOScheduler</code></p> <p>See :class:<code>~syne_tune.optimizer.schedulers.searchers.RandomSearcher</code> for <code>kwargs[\"search_options\"]</code> parameters.</p> <p>:param config_space: Configuration space for evaluation function :param metric: Name of metric to optimize :param population_size: See     :class:<code>~syne_tune.optimizer.schedulers.searchers.RegularizedEvolution</code>.     Defaults to 100 :param sample_size: See     :class:<code>~syne_tune.optimizer.schedulers.searchers.RegularizedEvolution</code>.     Defaults to 10 :param random_seed: Random seed, optional :param kwargs: Additional arguments to     :class:<code>~syne_tune.optimizer.schedulers.FIFOScheduler</code></p> Source code in <code>whittle/search/local_search.py</code> <pre><code>def __init__(\n    self,\n    config_space: dict[str, Any],\n    metric: list[str],\n    mode: list[str] | str = \"min\",\n    start_point: dict[str, Any] | None = None,\n    random_seed: int | None = None,\n    points_to_evaluate: list[dict] | None = None,\n    **kwargs,\n):\n    super().__init__(\n        config_space=config_space,\n        metric=metric,\n        mode=mode,\n        searcher=LocalSearch(\n            config_space=config_space,\n            metric=metric,\n            start_point=start_point,\n            mode=mode,\n            random_seed=random_seed,\n            points_to_evaluate=points_to_evaluate,\n        ),\n        random_seed=random_seed,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/whittle/search/local_search/#whittle.search.local_search.LocalSearch","title":"LocalSearch","text":"<pre><code>LocalSearch(\n    config_space,\n    metric: list[str] | str,\n    points_to_evaluate: list[dict] | None = None,\n    start_point: dict | None = None,\n    mode: list[str] | str = \"min\",\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>StochasticSearcher</code></p> Source code in <code>whittle/search/local_search.py</code> <pre><code>def __init__(\n    self,\n    config_space,\n    metric: list[str] | str,\n    points_to_evaluate: list[dict] | None = None,\n    start_point: dict | None = None,\n    mode: list[str] | str = \"min\",\n    **kwargs,\n):\n    if start_point is None:\n        self.start_point = {\n            k: v.sample() if isinstance(v, Domain) else v\n            for k, v in config_space.items()\n        }\n    else:\n        self.start_point = start_point\n\n    self._pareto_front: list[PopulationElement] = []\n\n    if points_to_evaluate is None:\n        points_to_evaluate = [self.start_point]\n    else:\n        points_to_evaluate.append(self.start_point)\n\n    super().__init__(\n        config_space,\n        metric,\n        mode=mode,\n        points_to_evaluate=points_to_evaluate,\n        **kwargs,\n    )\n    if isinstance(self._mode, list):\n        self._metric_op: dict[str, Any] = {\n            metric: 1 if mode == \"min\" else -1\n            for metric, mode in zip(metric, self._mode)\n        }\n    else:\n        if self._mode == \"min\":\n            self._metric_op = dict(zip(self._metric, [1.0] * len(self._metric)))\n        elif self._mode == \"max\":\n            self._metric_op = dict(zip(self._metric, [-1.0] * len(self._metric)))\n</code></pre>"},{"location":"api/whittle/search/local_search/#whittle.search.local_search.PopulationElement","title":"PopulationElement  <code>dataclass</code>","text":"<pre><code>PopulationElement(\n    trial_id: int, config: dict, result: dict\n)\n</code></pre> <p>Internal PBT state tracked per-trial.</p>"},{"location":"api/whittle/search/multi_objective/","title":"Multi objective","text":""},{"location":"api/whittle/search/multi_objective/#whittle.search.multi_objective","title":"whittle.search.multi_objective","text":""},{"location":"api/whittle/search/multi_objective/#whittle.search.multi_objective.get_pareto_optimal","title":"get_pareto_optimal","text":"<pre><code>get_pareto_optimal(costs: ndarray) -&gt; NDArray[bool_]\n</code></pre> <p>Find the pareto-optimal point.</p> <p>:param costs: (n_points, m_cost_values) array :return: (n_points, 1) indicator if point is on pareto front or not.</p> Source code in <code>whittle/search/multi_objective.py</code> <pre><code>def get_pareto_optimal(costs: np.ndarray) -&gt; npt.NDArray[np.bool_]:\n    \"\"\"Find the pareto-optimal point.\n\n    :param costs: (n_points, m_cost_values) array\n    :return: (n_points, 1) indicator if point is on pareto front or not.\n    \"\"\"\n    assert isinstance(costs, np.ndarray)\n    assert costs.ndim == 2\n\n    # first assume all points are pareto optimal\n    is_pareto = np.ones(costs.shape[0], dtype=bool)\n    for i, c in enumerate(costs):\n        if is_pareto[i]:\n            # determine all points that have a smaller cost\n            all_with_lower_costs = np.any(costs &lt; c, axis=1)\n            keep_on_front = np.logical_and(all_with_lower_costs, is_pareto)\n            is_pareto = keep_on_front\n            is_pareto[i] = True  # keep self\n    return is_pareto\n</code></pre>"},{"location":"api/whittle/search/search/","title":"Search","text":""},{"location":"api/whittle/search/search/#whittle.search.search","title":"whittle.search.search","text":""},{"location":"api/whittle/search/search/#whittle.search.search.multi_objective_search","title":"multi_objective_search","text":"<pre><code>multi_objective_search(\n    objective,\n    search_space: dict,\n    search_strategy: str = \"random_search\",\n    num_samples: int = 100,\n    objective_kwargs: dict | None = None,\n    seed: int | None = None,\n)\n</code></pre> <p>Search for the Pareto optimal sub-networks.</p> <p>:param objective: the objective function to optimize. :param search_space: the search space. :param search_strategy: the search strategy. :param objective_kwargs: the keyword arguments for the objective function. :param num_samples: the number of samples to take. :param seed: the random seed. :return: the results of the search.</p> Source code in <code>whittle/search/search.py</code> <pre><code>def multi_objective_search(\n    objective,\n    search_space: dict,\n    search_strategy: str = \"random_search\",\n    num_samples: int = 100,\n    objective_kwargs: dict | None = None,\n    seed: int | None = None,\n):\n    \"\"\"\n    Search for the Pareto optimal sub-networks.\n\n    :param objective: the objective function to optimize.\n    :param search_space: the search space.\n    :param search_strategy: the search strategy.\n    :param objective_kwargs: the keyword arguments for the objective function.\n    :param num_samples: the number of samples to take.\n    :param seed: the random seed.\n    :return: the results of the search.\n    \"\"\"\n\n    metrics = [\"objective_1\", \"objective_2\"]\n    if seed is None:\n        seed = np.random.randint(0, 1000000)\n\n    assert search_strategy in methods\n\n    base_scheduler = methods[search_strategy](\n        MethodArguments(\n            config_space=search_space,\n            metrics=metrics,\n            mode=[\"min\", \"min\"],\n            random_seed=seed,\n        )\n    )\n\n    scheduler = AskTellScheduler(base_scheduler=base_scheduler)\n\n    costs = np.empty((num_samples, 2))\n    runtime = []\n    configs = []\n    start_time = time.time()\n    for i in range(num_samples):\n        trial_suggestion = scheduler.ask()\n        objective_1, objective_2 = objective(\n            config=trial_suggestion.config, **objective_kwargs\n        )\n\n        scheduler.tell(\n            trial_suggestion, {\"objective_1\": objective_1, \"objective_2\": objective_2}\n        )\n\n        # bookkeeping\n        costs[i][0] = float(objective_1)\n        costs[i][1] = float(objective_2)\n        configs.append(trial_suggestion.config)\n\n        runtime.append(time.time() - start_time)\n        print(\n            f\"iteration {i}: objective_1={objective_1} ; objective_2={objective_2}; runtime = {runtime[-1]}\"\n        )\n    idx = get_pareto_optimal(costs)\n\n    results = {\n        \"costs\": costs,\n        \"configs\": configs,\n        \"runtime\": runtime,\n        \"is_pareto_optimal\": [bool(i) for i in idx],\n    }\n    return results\n</code></pre>"},{"location":"api/whittle/training_strategies/ats/","title":"Ats","text":""},{"location":"api/whittle/training_strategies/ats/#whittle.training_strategies.ats","title":"whittle.training_strategies.ats","text":""},{"location":"api/whittle/training_strategies/ats/#whittle.training_strategies.ats.ATS","title":"ATS","text":"<pre><code>ATS(random_samples: int = 1, **kwargs)\n</code></pre> <p>               Bases: <code>BaseTrainingStrategy</code></p> <p>ATS strategy.</p> <p>Follows the approach by Mohtashami et al. and updates a set of randomly sampled sub-networks if if the current step is even, otherwise it updates the super-network.</p> refs <p>Masked Training of Neural Networks with Partial Gradients Amirkeivan Mohtashami, Martin Jaggi, Sebastian Stich Proceedings of The 25th International Conference on Artificial Intelligence and Statistics arxiv.org/abs/2106.08895</p> PARAMETER DESCRIPTION <code>random_samples</code> <p>the number of randomly sampled sub-networks to sample and update in each step</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>**kwargs</code> <p>kwargs of <code>BaseTrainingStrategy</code></p> <p> DEFAULT: <code>{}</code> </p> Source code in <code>whittle/training_strategies/ats.py</code> <pre><code>def __init__(self, random_samples: int = 1, **kwargs):\n    \"\"\"\n    Initialises an `ATS` strategy.\n\n    Args:\n        random_samples: the number of randomly sampled sub-networks to sample and update in each step\n        **kwargs: kwargs of `BaseTrainingStrategy`\n    \"\"\"\n    super().__init__(**kwargs)\n    self.random_samples = random_samples\n    self.current_step = 0\n</code></pre>"},{"location":"api/whittle/training_strategies/ats/#whittle.training_strategies.ats.ATS.__call__","title":"__call__","text":"<pre><code>__call__(model, inputs, outputs, **kwargs)\n</code></pre> <p>Updates a set of randomly sampled sub-networks if the current step is odd. Else, it updates the super-network.</p> Source code in <code>whittle/training_strategies/ats.py</code> <pre><code>def __call__(self, model, inputs, outputs, **kwargs):\n    \"\"\"\n    Updates a set of randomly sampled sub-networks if the current step is odd. Else, it updates the\n    super-network.\n    \"\"\"\n    total_loss = 0\n    y_supernet = model(inputs).detach()\n    if self.current_step % 2 == 0:\n        # update random sub-networks\n        for i in range(self.random_samples):\n            config = self.sampler.sample()\n            model.select_sub_network(config)\n            y_hat = model(inputs)\n            if self.kd_loss is not None:\n                loss = self.kd_loss(y_hat, outputs, y_supernet)\n            else:\n                loss = self.loss_function(y_hat, outputs)\n            loss.backward()\n            model.reset_super_network()\n\n            total_loss += loss.item()\n    else:\n        y_hat = model(inputs)\n        if self.kd_loss is not None:\n            loss = self.kd_loss(y_hat, outputs, y_supernet)\n        else:\n            loss = self.loss_function(y_hat, outputs)\n        loss.backward()\n        total_loss = loss.item()\n    self.current_step += 1\n    return total_loss\n</code></pre>"},{"location":"api/whittle/training_strategies/base_strategy/","title":"Base strategy","text":""},{"location":"api/whittle/training_strategies/base_strategy/#whittle.training_strategies.base_strategy","title":"whittle.training_strategies.base_strategy","text":""},{"location":"api/whittle/training_strategies/base_strategy/#whittle.training_strategies.base_strategy.BaseTrainingStrategy","title":"BaseTrainingStrategy","text":"<pre><code>BaseTrainingStrategy(\n    sampler: RandomSampler,\n    loss_function: Callable,\n    kd_loss: DistillLoss | None = None,\n    device: str = \"cuda\",\n    **kwargs\n)\n</code></pre> <p>Base Training Strategy.</p> <p>Base class that all training strategies inherit from.</p> <pre><code>sampler: sampler that returns a sub-network when called\nloss_function: loss function to compute the loss of a sub-network\ndevice: device to run the model on\n**kwargs:\n</code></pre> Source code in <code>whittle/training_strategies/base_strategy.py</code> <pre><code>def __init__(\n    self,\n    sampler: RandomSampler,\n    loss_function: Callable,\n    kd_loss: DistillLoss | None = None,\n    device: str = \"cuda\",\n    **kwargs,\n):\n    \"\"\"\n    Initialises a `BaseTrainingStrategy`\n    Args:\n        sampler: sampler that returns a sub-network when called\n        loss_function: loss function to compute the loss of a sub-network\n        device: device to run the model on\n        **kwargs:\n    \"\"\"\n    self.sampler = sampler\n    self.loss_function = loss_function\n    self.device = device\n    self.kd_loss = kd_loss\n    if self.kd_loss is not None:\n        if not isinstance(loss_function, torch.nn.CrossEntropyLoss):\n            raise TypeError(\n                \"KD Loss not yet supported: Expected torch.nn.CrossEntropyLoss\"\n            )\n</code></pre>"},{"location":"api/whittle/training_strategies/random/","title":"Random","text":""},{"location":"api/whittle/training_strategies/random/#whittle.training_strategies.random","title":"whittle.training_strategies.random","text":""},{"location":"api/whittle/training_strategies/random/#whittle.training_strategies.random.RandomStrategy","title":"RandomStrategy","text":"<pre><code>RandomStrategy(random_samples=1, **kwargs)\n</code></pre> <p>               Bases: <code>BaseTrainingStrategy</code></p> <p>Random strategy.</p> <p>Randomly samples and updates <code>random_samples</code> sub-networks in each step.</p> PARAMETER DESCRIPTION <code>random_samples</code> <p>the number of randomly sampled sub-networks to sample and update in each step</p> <p> DEFAULT: <code>1</code> </p> <code>**kwargs</code> <p>kwargs of <code>BaseTrainingStrategy</code></p> <p> DEFAULT: <code>{}</code> </p> Source code in <code>whittle/training_strategies/random.py</code> <pre><code>def __init__(self, random_samples=1, **kwargs):\n    \"\"\"\n    Initialises a `RandomStrategy`\n\n    Args:\n        random_samples: the number of randomly sampled sub-networks to sample and update in each step\n        **kwargs: kwargs of `BaseTrainingStrategy`\n    \"\"\"\n    super().__init__(**kwargs)\n    self.random_samples = random_samples\n</code></pre>"},{"location":"api/whittle/training_strategies/random/#whittle.training_strategies.random.RandomStrategy.__call__","title":"__call__","text":"<pre><code>__call__(model, inputs, outputs, **kwargs)\n</code></pre> <p>Updates randomly sampled sub-networks in each step.</p> Source code in <code>whittle/training_strategies/random.py</code> <pre><code>def __call__(self, model, inputs, outputs, **kwargs):\n    \"\"\"Updates randomly sampled sub-networks in each step.\"\"\"\n    total_loss = 0\n    y_supernet = model(inputs).detach()\n    for i in range(self.random_samples):\n        config = self.sampler.sample()\n        model.select_sub_network(config)\n        y_hat = model(inputs)\n        if self.kd_loss is not None:\n            loss = self.kd_loss(y_hat, outputs, y_supernet)\n        else:\n            loss = self.loss_function(y_hat, outputs)\n        loss.backward()\n        model.reset_super_network()\n\n        total_loss += loss.item()\n    return total_loss\n</code></pre>"},{"location":"api/whittle/training_strategies/random_linear/","title":"Random linear","text":""},{"location":"api/whittle/training_strategies/random_linear/#whittle.training_strategies.random_linear","title":"whittle.training_strategies.random_linear","text":""},{"location":"api/whittle/training_strategies/random_linear/#whittle.training_strategies.random_linear.RandomLinearStrategy","title":"RandomLinearStrategy","text":"<pre><code>RandomLinearStrategy(\n    total_number_of_steps: int,\n    random_samples: int = 1,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>BaseTrainingStrategy</code></p> <p>Random linear strategy.</p> <p>Updates <code>random_samples</code> randomly sampled sub-network with probability <code>p</code> or the super-network with <code>1 - p</code>. <code>p</code> linearly increases with the step count.</p> refs <p>Structural Pruning of Pre-trained Language Models via Neural Architecture Search Aaron Klein, Jacek Golebiowski, Xingchen Ma, Valerio Perrone, Cedric Archambeau arxiv.org/abs/2405.02267</p> <p>Understanding and Simplifying One-Shot Architecture Search Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, Quoc Le International Conference on Machine Learning (ICML) 2018 proceedings.mlr.press/v80/bender18a/bender18a.pdf</p> PARAMETER DESCRIPTION <code>total_number_of_steps</code> <p>the number of steps the optimization runs for</p> <p> TYPE: <code>int</code> </p> <code>random_samples</code> <p>the number of randomly sampled sub-networks to sample and update in each step</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>**kwargs</code> <p>kwargs of <code>BaseTrainingStrategy</code></p> <p> DEFAULT: <code>{}</code> </p> Source code in <code>whittle/training_strategies/random_linear.py</code> <pre><code>def __init__(self, total_number_of_steps: int, random_samples: int = 1, **kwargs):\n    \"\"\"\n    Initialises a `RandomLinearStrategy`\n\n    Args:\n        total_number_of_steps: the number of steps the optimization runs for\n        random_samples: the number of randomly sampled sub-networks to sample and update in each step\n        **kwargs: kwargs of `BaseTrainingStrategy`\n    \"\"\"\n    super().__init__(**kwargs)\n    self.random_samples = random_samples\n    self.total_number_of_steps = total_number_of_steps\n    self.current_step = 0\n    self.rate = np.linspace(0.0, 1, total_number_of_steps)\n</code></pre>"},{"location":"api/whittle/training_strategies/sandwich/","title":"Sandwich","text":""},{"location":"api/whittle/training_strategies/sandwich/#whittle.training_strategies.sandwich","title":"whittle.training_strategies.sandwich","text":""},{"location":"api/whittle/training_strategies/sandwich/#whittle.training_strategies.sandwich.SandwichStrategy","title":"SandwichStrategy","text":"<pre><code>SandwichStrategy(random_samples=2, **kwargs)\n</code></pre> <p>               Bases: <code>BaseTrainingStrategy</code></p> <p>Sandwich strategy.</p> <p>In each step, the sandwich strategy updates the super-network, the smallest, and a set of randomly sampled sub-networks.</p> refs <p>Universally Slimmable Networks and Improved Training Techniques Jiahui Yu, Thomas Huang International Conference on Computer Vision 2019 arxiv.org/abs/1903.05134</p> PARAMETER DESCRIPTION <code>random_samples</code> <p>the number of randomly sampled sub-networks to sample and update in each step</p> <p> DEFAULT: <code>2</code> </p> <code>**kwargs</code> <p>kwargs of <code>BaseTrainingStrategy</code></p> <p> DEFAULT: <code>{}</code> </p> Source code in <code>whittle/training_strategies/sandwich.py</code> <pre><code>def __init__(self, random_samples=2, **kwargs):\n    \"\"\"\n    Initialises a `SandwichStrategy`\n\n    Args:\n        random_samples: the number of randomly sampled sub-networks to sample and update in each step\n        **kwargs: kwargs of `BaseTrainingStrategy`\n    \"\"\"\n    super().__init__(**kwargs)\n    self.random_samples = random_samples\n</code></pre>"},{"location":"api/whittle/training_strategies/standard/","title":"Standard","text":""},{"location":"api/whittle/training_strategies/standard/#whittle.training_strategies.standard","title":"whittle.training_strategies.standard","text":""},{"location":"api/whittle/training_strategies/standard/#whittle.training_strategies.standard.StandardStrategy","title":"StandardStrategy","text":"<pre><code>StandardStrategy(**kwargs)\n</code></pre> <p>               Bases: <code>BaseTrainingStrategy</code></p> <p>Standard strategy.</p> <p>Implements the standard update rule and updates all weights of the super-network.</p> PARAMETER DESCRIPTION <code>**kwargs</code> <p>kwargs of <code>BaseTrainingStrategy</code></p> <p> DEFAULT: <code>{}</code> </p> Source code in <code>whittle/training_strategies/standard.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initialises a `StandardStrategy`\n\n    Args:\n        **kwargs: kwargs of `BaseTrainingStrategy`\n    \"\"\"\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/whittle/utils/auto_search_config/","title":"Auto search config","text":""},{"location":"api/whittle/utils/auto_search_config/#whittle.utils.auto_search_config","title":"whittle.utils.auto_search_config","text":""}]}