{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Whittle","text":"<p>Whittle is a Python library for compressing large language models (LLMs)  by extracting sub-networks to balance performance and efficiency. It is based on LitGPT and allows to compress many state-of-the-art models.</p> <ul> <li>Neural Architecture Search: Workflows for pre-training super-networks and multi-objective search to select sub-networks.</li> <li>Structural Pruning: State-of-the-art approaches to pruning structural components of pre-trained LLMs.</li> <li>Distillation: Workflow to distill a student model given a trained teacher model.</li> <li>Evaluation: Easy extraction of sub-networks checkpoint and evaluation using LM-Eval-Harness</li> <li>Efficiency: Different metrics to estimate efficiency of sub-networks, such as latency, FLOPs, or energy consumption.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Whittle supports and is tested for python 3.9 to 3.11. </p> <p>You can install whittle with:  <pre><code>pip install whittle\n</code></pre></p>"},{"location":"#install-from-source","title":"Install from source","text":"<p>Install whittle from source to get the most recent version: <pre><code>git clone git@github.com:whittle-org/whittle.git\ncd whittle\npip install -e .\n</code></pre></p>"},{"location":"#projects-that-use-whittle","title":"Projects that use whittle","text":"<ul> <li>Structural Pruning of Pre-trained Language Models via Neural Architecture Search</li> <li>HW-GPT Bench</li> </ul>"},{"location":"#how-to-get-involved","title":"How to get involved","text":"<p>We more than happy for any code contribution. If you are interested in contribution to whittle,  please read our contribution guide.</p>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#contributing","title":"Contributing","text":""},{"location":"contributing/#installation","title":"Installation","text":"<pre><code># Create your own fork of the repository if required and replace whittle-org with your username\ngit clone git@github.com:whittle-org/whittle.git\ncd whittle\npip install -e \".[dev]\"  # Install what's here (the `.` part) and install the extra dev dependancies\n</code></pre> <p>Setup <code>pre-commit</code> to run on every commit</p> <pre><code>pre-commit install\n</code></pre>"},{"location":"contributing/#testing","title":"Testing","text":"<pre><code>pytest\n</code></pre>"},{"location":"contributing/#docstring-writing-guidelines","title":"Docstring Writing Guidelines","text":"<p>When adding or updating functions or classes, please ensure that each has a docstring that follows this format:</p> <ul> <li>Summary: A brief description of what the function or class does.</li> <li>args: List each argument with its name, and a short description of its purpose.</li> <li>return: Describe the return value, including what it represents. Note: After adding or updating the docstring, ensure that the code passes the following command with no warnings:</li> </ul> <pre><code>mkdocs build --clean --strict\n</code></pre>"},{"location":"contributing/#conventional-commits-and-commitizen","title":"Conventional commits and Commitizen","text":"<p>We use commitizen to manage commits. This enforces conventional commits.</p> <p>To make a commit, simply run:</p> <pre><code>cz commit\n</code></pre> <p>This will prompt you to enter a commit message and enforce conventional commit formatting.</p> <p>If you do not use <code>cz commit</code> or make a commit with a conventional commit message, your PR will not pass CI.</p>"},{"location":"contributing/#signing-commits","title":"Signing commits","text":"<p>Note: we recommend using SSH keys for signing commits for convenience (e.g., you can use the same key for commit signing and for authentication to GitHub).</p> <ol> <li>Add a SSH (or GPG) key as a signing key to you GitHub account.</li> <li>Configure <code>git</code> to use the key.</li> </ol> <p>Note: if you don't configure commit signing globally, you will need to use <code>git commit -s</code>/<code>cz commit -s</code> to sign your commits.</p> <p>&lt;!-- ## Release</p> <p>Update the version in <code>pyproject.toml</code> first, say to <code>X.Y.Z</code>. If you maintain a changelog, update it.</p> <p>This part just makes a versioned commit and tag for github and to be able to easily find code at a specific version. It will also help with versioned documentation to have a tag.</p> <pre><code>git add pyproject.toml [changelog-file-if-any]\ngit commit -m \"bump: X.Y.Z\"\ngit tag X.Y.Z\ngit push --tags\ngit push\n</code></pre> <p>Then to release on PyPI:</p> <pre><code>pip install twine # If not already\n\nrm -rf ./dist  # Remove anything currently occupying the dist folder\npython -m build --sdist  # Build a source distribution\ntwine upload dist/*  # Publish to PyPI\n``` --&gt;\n\n## Documentation\n\nView locally\n\n```bash\nmkdocs --serve\n</code></pre> <p>Build and deploy to GitHub Pages. Make sure to specify the github tag you want to deploy.</p> <pre><code>mike deploy --push --update-aliases &lt;TAG&gt; \"latest\"\n</code></pre>"},{"location":"api/whittle/args/","title":"Args","text":""},{"location":"api/whittle/args/#whittle.args","title":"whittle.args","text":""},{"location":"api/whittle/args/#whittle.args.DistillArgs","title":"DistillArgs  <code>dataclass</code>","text":"<pre><code>DistillArgs(\n    method: str = \"logits\",\n    alpha: float = 1.0,\n    beta: float = 0.0,\n    temperature: float = 5,\n    loss: str = \"forward_kld\",\n    weight_scheme: str = \"other\",\n)\n</code></pre> <p>Distillation-related arguments</p> PARAMETER DESCRIPTION <code>method</code> <p>Distillation method to use ('logits' or 'hidden_states') - Only supports 'logits' for now</p> <p> TYPE: <code>str</code> DEFAULT: <code>'logits'</code> </p> <code>temperature</code> <p>Controls softening of output probabilities. Higher values (&gt;1) produce softer distributions,     emphasizing less confident predictions. Lower values (&lt;1) make distributions sharper, focusing on     confident predictions.</p> <p> TYPE: <code>float</code> DEFAULT: <code>5</code> </p> <code>alpha</code> <p>Weight for the cross-entropy loss. Higher values give more importance to the loss between student logits and ground truth labels.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>beta</code> <p>Weight for the distillation loss. Higher values give more importance to the loss between student and teacher logits.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>loss</code> <p>Loss function to use for distillation. Options are 'forward_kld', 'reverse_kld', 'symmetric_kld', 'js_distance', 'simple_cross_entropy', 'cosine_similarity', 'l1_loss', 'l2_loss', 'mmd_loss'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'forward_kld'</code> </p> <code>weight_scheme</code> <p>Weight scheme to use for the distillation loss. Options are 'default' (use alpha=1 and beta=hard_target_loss/soft_target_loss). Default is 'other' (use alpha and beta as provided).</p> <p> TYPE: <code>str</code> DEFAULT: <code>'other'</code> </p>"},{"location":"api/whittle/args/#whittle.args.ParamBinArgs","title":"ParamBinArgs","text":"<p>parameter bin-related arguments - to limit what networks are sampled</p>"},{"location":"api/whittle/args/#whittle.args.ParamBinArgs.log_bins","title":"log_bins  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>log_bins: bool = False\n</code></pre> <p>Starting size of the bins (how many configs must be in each bin until the total limit is increased)</p>"},{"location":"api/whittle/args/#whittle.args.ParamBinArgs.num_bins","title":"num_bins  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_bins: int = 20\n</code></pre> <p>Whether to use log spaced bins</p>"},{"location":"api/whittle/args/#whittle.args.ParamBinArgs.start_bin_size","title":"start_bin_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>start_bin_size: int = 1\n</code></pre> <p>The total limit will be increased even if K bins are not full yet (some param counts may have only few nets)</p>"},{"location":"api/whittle/args/#whittle.args.PruningArgs","title":"PruningArgs  <code>dataclass</code>","text":"<pre><code>PruningArgs(\n    pruning_strategy: str = \"mag\",\n    prune_n_weights_per_group: int = 2,\n    weights_per_group: int = 4,\n    n_samples: int = 32,\n)\n</code></pre> <p>pruning-related arguments</p>"},{"location":"api/whittle/args/#whittle.args.PruningArgs.n_samples","title":"n_samples  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>n_samples: int = 32\n</code></pre> <p>Number of samples for calibration</p>"},{"location":"api/whittle/args/#whittle.args.PruningArgs.prune_n_weights_per_group","title":"prune_n_weights_per_group  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>prune_n_weights_per_group: int = 2\n</code></pre> <p>Number of weights to prune per group</p>"},{"location":"api/whittle/args/#whittle.args.PruningArgs.pruning_strategy","title":"pruning_strategy  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pruning_strategy: str = 'mag'\n</code></pre> <p>Structural pruning strategy</p>"},{"location":"api/whittle/args/#whittle.args.PruningArgs.weights_per_group","title":"weights_per_group  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>weights_per_group: int = 4\n</code></pre> <p>Total number of weights per group</p>"},{"location":"api/whittle/args/#whittle.args.SamplerArgs","title":"SamplerArgs  <code>dataclass</code>","text":"<pre><code>SamplerArgs(\n    num_configs: int = 21,\n    seed_sampler: int = 42,\n    n_trials: int = 10000,\n    sampling_strategy: str = \"random\",\n)\n</code></pre> <p>sampler-related arguments</p>"},{"location":"api/whittle/args/#whittle.args.SamplerArgs.n_trials","title":"n_trials  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>n_trials: int = 10000\n</code></pre> <p>Sampling strategy</p>"},{"location":"api/whittle/args/#whittle.args.SamplerArgs.num_configs","title":"num_configs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_configs: int = 21\n</code></pre> <p>Seed for the random sampler</p>"},{"location":"api/whittle/args/#whittle.args.SamplerArgs.seed_sampler","title":"seed_sampler  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>seed_sampler: int = 42\n</code></pre> <p>Number of trials to run for each bin</p>"},{"location":"api/whittle/args/#whittle.args.SearchArgs","title":"SearchArgs  <code>dataclass</code>","text":"<pre><code>SearchArgs(\n    save_interval: int | None = 1000,\n    log_interval: int = 1,\n    search_strategy: str = \"random_search\",\n    iterations: int = 100,\n)\n</code></pre> <p>search-related arguments</p>"},{"location":"api/whittle/args/#whittle.args.SearchArgs.iterations","title":"iterations  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>iterations: int = 100\n</code></pre> <p>Number of iterations for the multi-objective search</p>"},{"location":"api/whittle/args/#whittle.args.SearchArgs.log_interval","title":"log_interval  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>log_interval: int = 1\n</code></pre> <p>Number of iterations between logging calls</p>"},{"location":"api/whittle/args/#whittle.args.SearchArgs.save_interval","title":"save_interval  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>save_interval: int | None = 1000\n</code></pre> <p>Number of optimizer steps between saving checkpoints</p>"},{"location":"api/whittle/args/#whittle.args.SearchArgs.search_strategy","title":"search_strategy  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>search_strategy: str = 'random_search'\n</code></pre> <p>Multi-objective search strategy</p>"},{"location":"api/whittle/convert_to_litgpt/","title":"Convert to litgpt","text":""},{"location":"api/whittle/convert_to_litgpt/#whittle.convert_to_litgpt","title":"whittle.convert_to_litgpt","text":""},{"location":"api/whittle/convert_to_litgpt/#whittle.convert_to_litgpt.setup","title":"setup","text":"<pre><code>setup(\n    sub_network_dir: Path,\n    out_dir: Path | None = None,\n    parent_dir: Path | None = None,\n    super_network_cls: type[GPT] = GPT,\n    no_model_key: bool = True,\n) -&gt; None\n</code></pre> <p>Convert a sub-network to a LitGPT model checkpoint. The sub-network checkpoints can have the following design:</p> <p>a) same as litgpt models:     sub_network_dir/     - lit_model.pth ... {model: sub_network.state_dict()} or sub_network.state_dict() (former - whittle model, latter - litgpt model)     - configs (model_config.yaml, tokenizer.json, etc.) b) litgpt model with saving space (not copying the tokenizer and other configs):     sub_network_dir/     - lit_model.pth ... {model: sub_network.state_dict(), parent_dir: super-network checkpoint dir}     - model_config.yaml c) compressed checkpoint:     sub_network_dir/     - lit_model.pth ... {sub_network_config: sub_network_config, parent_dir: super-network checkpoint dir}     - model_config.yaml</p> <p>Conversion procedure: a) No modification (copying to a new directory if <code>sub_network_dir != out_dir</code>). b) Copy the other configs from the parent checkpoint directory, and model checkpoint and config from <code>sub_network_dir</code>. c) Extract the sub-network from the super-network, save the sub-network weights and config, and copy the other configs from the parent checkpoint directory.</p> PARAMETER DESCRIPTION <code>sub_network_dir</code> <p>The path to the sub-network directory to convert.</p> <p> TYPE: <code>Path</code> </p> <code>out_dir</code> <p>Directory in which to save the converted sub-network. If not provided, saving to <code>sub_network_dir</code> by default.</p> <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> <code>parent_dir</code> <p>The parent directory of the sub-network. Required if the checkpoint is in the format b) or c). If not provided, the parent directory is extracted from the checkpoint.</p> <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> <code>super_network_cls</code> <p>The super-network class to instantiate the super-network model. Defaults to <code>GPT</code>.</p> <p> TYPE: <code>type[GPT]</code> DEFAULT: <code>GPT</code> </p> <code>no_model_key</code> <p>Convert strictly to litgpt - lit_model.pth will contain only the sub-network weights, and not {'model': sub_network_weights}.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>whittle/convert_to_litgpt.py</code> <pre><code>def setup(\n    sub_network_dir: Path,\n    out_dir: Path | None = None,\n    parent_dir: Path | None = None,\n    super_network_cls: type[GPT] = GPT,\n    no_model_key: bool = True,\n) -&gt; None:\n    \"\"\"\n    Convert a sub-network to a LitGPT model checkpoint.\n    The sub-network checkpoints can have the following design:\n\n    a) same as litgpt models:\n        sub_network_dir/\n        - lit_model.pth ... {model: sub_network.state_dict()} or sub_network.state_dict() (former - whittle model, latter - litgpt model)\n        - configs (model_config.yaml, tokenizer.json, etc.)\n    b) litgpt model with saving space (not copying the tokenizer and other configs):\n        sub_network_dir/\n        - lit_model.pth ... {model: sub_network.state_dict(), parent_dir: super-network checkpoint dir}\n        - model_config.yaml\n    c) compressed checkpoint:\n        sub_network_dir/\n        - lit_model.pth ... {sub_network_config: sub_network_config, parent_dir: super-network checkpoint dir}\n        - model_config.yaml\n\n    Conversion procedure:\n    a) No modification (copying to a new directory if `sub_network_dir != out_dir`).\n    b) Copy the other configs from the parent checkpoint directory, and model checkpoint and config from `sub_network_dir`.\n    c) Extract the sub-network from the super-network, save the sub-network weights and config, and copy the other configs from the parent checkpoint directory.\n\n    Arguments:\n        sub_network_dir: The path to the sub-network directory to convert.\n        out_dir: Directory in which to save the converted sub-network. If not provided, saving to `sub_network_dir` by default.\n        parent_dir: The parent directory of the sub-network. Required if the checkpoint is in the format b) or c). If not provided, the parent directory is extracted from the checkpoint.\n        super_network_cls: The super-network class to instantiate the super-network model. Defaults to `GPT`.\n        no_model_key: Convert strictly to litgpt - lit_model.pth will contain only the sub-network weights, and not {'model': sub_network_weights}.\n    \"\"\"\n\n    if out_dir is None:\n        out_dir = sub_network_dir\n\n    ckp = lazy_load(sub_network_dir / \"lit_model.pth\")\n\n    # sub-network config loading (contains the config and checkpoint path of the parent)\n\n    parent_dir = ckp.get(\"parent_dir\", None) if parent_dir is None else parent_dir\n\n    if \"model\" in ckp:\n        model_path = sub_network_dir / \"lit_model.pth\"\n        configs_path = (\n            sub_network_dir if parent_dir is None else Path(parent_dir)\n        )  # config files were copied\n\n        # copy model only if the destination is different\n        if out_dir != sub_network_dir and not no_model_key:\n            shutil.copy(model_path, out_dir / \"lit_model.pth\")\n        # re-save the model if the required format is state_dict instead of {'model': state_dict}\n        elif no_model_key:\n            ckp = torch.load(\n                model_path\n            )  # this time not lazy loading because we're re-saving the weights\n            torch.save(ckp[\"model\"], out_dir / \"lit_model.pth\")\n\n        if configs_path == parent_dir:\n            # we don't want to overwrite the sub-network config in case `out_dir` == `sub_network_dir`\n            shutil.copy(\n                sub_network_dir / \"model_config.yaml\", out_dir / \"model_config.yaml.tmp\"\n            )\n            copy_config_files(configs_path, out_dir)\n            shutil.copy(out_dir / \"model_config.yaml.tmp\", out_dir / \"model_config.yaml\")\n        elif out_dir != sub_network_dir:\n            copy_config_files(configs_path, out_dir)\n    else:\n        assert parent_dir is not None, (\n            'Weights are not saved in the checkpoint under \"model\", but `parent_dir` is not saved in the checkpoints provided.'\n        )\n        configs_path = Path(parent_dir)\n        model_path = configs_path / \"lit_model.pth\"\n\n        # we will need to extract the sub-network config and weights\n        assert \"sub_network_config\" in ckp, (\n            '\"model\" or \"sub_network_config\" not found in checkpoint'\n        )\n        sub_network_config = ckp[\"sub_network_config\"]\n\n        # instantiate the super-network\n        config = Config.from_file(model_path.parent / \"model_config.yaml\")\n        config.fix_head_size = True\n        model = super_network_cls(config)\n\n        # set the sub-network via the saved sub-network config\n        model.select_sub_network(sub_network_config)\n        sub_network = extract_current_sub_network(model)\n\n        # copy the config files, save the sub-network, and overwrite model_config.yaml with the sub-network config\n        copy_config_files(configs_path, out_dir)\n        # save the sub-network in the litgpt vs whittle format\n        save_data = (\n            sub_network.state_dict()\n            if no_model_key\n            else {\"model\": sub_network.state_dict()}\n        )\n        torch.save(save_data, out_dir / \"lit_model.pth\")\n        save_config(sub_network.config, out_dir)\n</code></pre>"},{"location":"api/whittle/distill/","title":"Distill","text":""},{"location":"api/whittle/distill/#whittle.distill","title":"whittle.distill","text":""},{"location":"api/whittle/distill/#whittle.distill.setup","title":"setup","text":"<pre><code>setup(\n    out_dir: Path = Path(\"out/distill\"),\n    precision: Literal[\n        \"bf16-true\", \"bf16-mixed\", \"32-true\", None\n    ] = None,\n    initial_checkpoint_dir: Path | None = None,\n    student_dir: Path | None = None,\n    data: DataModule | None = None,\n    train: TrainArgs = TrainArgs(\n        save_interval=1000,\n        log_interval=1,\n        global_batch_size=512,\n        micro_batch_size=4,\n        max_tokens=int(500000000.0),\n        max_norm=1.0,\n        min_lr=4e-05,\n        lr_warmup_steps=2000,\n        tie_embeddings=False,\n    ),\n    distill: DistillArgs = DistillArgs(\n        method=\"logits\",\n        temperature=5,\n        alpha=0.6,\n        beta=0.4,\n        loss=\"forward_kld\",\n    ),\n    eval: EvalArgs = EvalArgs(\n        interval=1000,\n        max_iters=100,\n        initial_validation=True,\n    ),\n    optimizer: str | dict = \"AdamW\",\n    devices: int | str = \"auto\",\n    num_nodes: int = 1,\n    tokenizer_dir: Path | None = None,\n    logger_name: Literal[\n        \"wandb\", \"tensorboard\", \"csv\"\n    ] = \"csv\",\n    seed: int = 42,\n    min_ratio: float = 0.3,\n    max_ratio: float = 0.7,\n)\n</code></pre> <p>Train a (random) subnet of the teacher model using knowledge distillation.</p> PARAMETER DESCRIPTION <code>out_dir</code> <p>Directory in which to save checkpoints and logs. If running in a Lightning Studio Job, look for it in /teamspace/jobs//share. <p> TYPE: <code>Path</code> DEFAULT: <code>Path('out/distill')</code> </p> <code>precision</code> <p>The precision to use for finetuning. Determines a compatible precision setting by default.</p> <p> TYPE: <code>Literal['bf16-true', 'bf16-mixed', '32-true', None]</code> DEFAULT: <code>None</code> </p> <code>initial_checkpoint_dir</code> <p>Path to a checkpoint directory to initialize the teacher model from.</p> <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> <code>student_dir</code> <p>Optional path to a directory to initialize the student model from. Checks for student model config and checkpoint. If not provided, the student model will be initialized as a random subnetwork of the teacher model.</p> <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> <code>data</code> <p>Data-related arguments. If not provided, the default is <code>litgpt.data.TinyStories</code>.</p> <p> TYPE: <code>DataModule | None</code> DEFAULT: <code>None</code> </p> <code>train</code> <p>Training-related arguments. See <code>litgpt.args.TrainArgs</code> for details.</p> <p> TYPE: <code>TrainArgs</code> DEFAULT: <code>TrainArgs(save_interval=1000, log_interval=1, global_batch_size=512, micro_batch_size=4, max_tokens=int(500000000.0), max_norm=1.0, min_lr=4e-05, lr_warmup_steps=2000, tie_embeddings=False)</code> </p> <code>distill</code> <p>Distillation-related arguments. See <code>whittle.args.DistillArgs</code> for details.</p> <p> TYPE: <code>DistillArgs</code> DEFAULT: <code>DistillArgs(method='logits', temperature=5, alpha=0.6, beta=0.4, loss='forward_kld')</code> </p> <code>eval</code> <p>Evaluation-related arguments. See <code>litgpt.args.EvalArgs</code> for details.</p> <p> TYPE: <code>EvalArgs</code> DEFAULT: <code>EvalArgs(interval=1000, max_iters=100, initial_validation=True)</code> </p> <code>optimizer</code> <p>An optimizer name (such as \"AdamW\") or config.</p> <p> TYPE: <code>str | dict</code> DEFAULT: <code>'AdamW'</code> </p> <code>devices</code> <p>How many devices/GPUs to use. Uses all GPUs by default.</p> <p> TYPE: <code>int | str</code> DEFAULT: <code>'auto'</code> </p> <code>num_nodes</code> <p>How many nodes the code is being run on.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>tokenizer_dir</code> <p>Optional path to the tokenizer dir that was used for preprocessing the dataset. Only some data modules require this.</p> <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> <code>logger_name</code> <p>The name of the logger to send metrics to.</p> <p> TYPE: <code>Literal['wandb', 'tensorboard', 'csv']</code> DEFAULT: <code>'csv'</code> </p> <code>seed</code> <p>The random seed to use for reproducibility.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> <code>min_ratio</code> <p>Minimum allowed ratio (student_params / teacher_params).</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.3</code> </p> <code>max_ratio</code> <p>Maximum allowed ratio (student_params / teacher_params).</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.7</code> </p> Source code in <code>whittle/distill.py</code> <pre><code>def setup(\n    out_dir: Path = Path(\"out/distill\"),\n    precision: Literal[\"bf16-true\", \"bf16-mixed\", \"32-true\", None] = None,\n    initial_checkpoint_dir: Path | None = None,\n    student_dir: Path | None = None,\n    data: DataModule | None = None,\n    train: TrainArgs = TrainArgs(\n        save_interval=1000,\n        log_interval=1,\n        global_batch_size=512,\n        micro_batch_size=4,\n        max_tokens=int(5e8),\n        max_norm=1.0,\n        min_lr=4e-5,\n        lr_warmup_steps=2000,\n        tie_embeddings=False,\n    ),\n    distill: DistillArgs = DistillArgs(\n        method=\"logits\",\n        temperature=5,\n        alpha=0.6,\n        beta=0.4,\n        loss=\"forward_kld\",\n    ),\n    eval: EvalArgs = EvalArgs(interval=1000, max_iters=100, initial_validation=True),\n    optimizer: str | dict = \"AdamW\",\n    devices: int | str = \"auto\",\n    num_nodes: int = 1,\n    tokenizer_dir: Path | None = None,\n    logger_name: Literal[\"wandb\", \"tensorboard\", \"csv\"] = \"csv\",\n    seed: int = 42,\n    min_ratio: float = 0.3,\n    max_ratio: float = 0.7,\n):\n    \"\"\"Train a (random) subnet of the teacher model using knowledge distillation.\n\n    Arguments:\n        out_dir: Directory in which to save checkpoints and logs. If running in a Lightning Studio Job, look for it in\n            /teamspace/jobs/&lt;job-name&gt;/share.\n        precision: The precision to use for finetuning. Determines a compatible precision setting by default.\n        initial_checkpoint_dir: Path to a checkpoint directory to initialize the teacher model from.\n        student_dir: Optional path to a directory to initialize the student model from.\n            Checks for student model config and checkpoint.\n            If not provided, the student model will be initialized as a random subnetwork of the teacher model.\n        data: Data-related arguments. If not provided, the default is ``litgpt.data.TinyStories``.\n        train: Training-related arguments. See ``litgpt.args.TrainArgs`` for details.\n        distill: Distillation-related arguments. See ``whittle.args.DistillArgs`` for details.\n        eval: Evaluation-related arguments. See ``litgpt.args.EvalArgs`` for details.\n        optimizer: An optimizer name (such as \"AdamW\") or config.\n        devices: How many devices/GPUs to use. Uses all GPUs by default.\n        num_nodes: How many nodes the code is being run on.\n        tokenizer_dir: Optional path to the tokenizer dir that was used for preprocessing the dataset. Only some data\n            modules require this.\n        logger_name: The name of the logger to send metrics to.\n        seed: The random seed to use for reproducibility.\n        min_ratio: Minimum allowed ratio (student_params / teacher_params).\n        max_ratio: Maximum allowed ratio (student_params / teacher_params).\n    \"\"\"\n    if initial_checkpoint_dir is not None:\n        print(f\"Loading teacher model config from {initial_checkpoint_dir}\")\n        config = Config.from_file(initial_checkpoint_dir / \"model_config.yaml\")\n        config.fix_head_size = True\n    else:\n        raise ValueError(\"initial_checkpoint_dir is required\")\n\n    if student_dir:\n        print(f\"Loading student model config from {student_dir}\")\n        student_config = Config.from_file(student_dir / \"model_config.yaml\")\n    else:\n        student_config = None\n        assert min_ratio &lt; max_ratio, \"min_ratio must be less than max_ratio\"\n        assert 0 &lt; min_ratio &lt; 1, \"min_ratio must be between 0 and 1\"\n        assert 0 &lt; max_ratio &lt; 1, \"max_ratio must be between 0 and 1\"\n\n    hparams = capture_hparams()\n    data = TinyStories() if data is None else data\n\n    precision = precision or get_default_supported_precision(training=True)\n    num_devices = int(parse_devices(devices))\n    out_dir = init_out_dir(out_dir)\n    # in case the dataset requires the Tokenizer\n    tokenizer = Tokenizer(tokenizer_dir) if tokenizer_dir is not None else None\n\n    logger = choose_logger(\n        logger_name,\n        out_dir,\n        name=f\"distill-{config.name}\",\n        log_interval=train.log_interval,\n    )\n\n    if num_devices * num_nodes &gt; 1:\n        strategy = FSDPStrategy(\n            auto_wrap_policy={Block},\n            activation_checkpointing_policy={Block},\n            state_dict_type=\"full\",\n            limit_all_gathers=True,\n            cpu_offload=False,\n        )\n    else:\n        strategy = \"auto\"\n\n    fabric = L.Fabric(\n        devices=num_devices,\n        num_nodes=num_nodes,\n        strategy=strategy,\n        precision=precision,\n        loggers=[logger],\n    )\n\n    if torch.cuda.is_available() and num_devices &gt; 1:\n        check_nvlink_connectivity(fabric)\n\n    fabric.launch()\n\n    fabric.print(pprint.pformat(hparams))\n    if logger_name in (\"tensorboard\", \"wandb\"):\n        fabric.logger.log_hyperparams(hparams)\n\n    main(\n        fabric,\n        initial_checkpoint_dir,\n        num_devices,\n        config,\n        student_config,\n        data,\n        out_dir,\n        tokenizer_dir,\n        tokenizer,\n        seed,\n        train,\n        distill,\n        eval,\n        optimizer,\n        student_dir,\n        min_ratio,\n        max_ratio,\n        logger_name,\n    )\n</code></pre>"},{"location":"api/whittle/evaluate_network/","title":"Evaluate network","text":""},{"location":"api/whittle/evaluate_network/#whittle.evaluate_network","title":"whittle.evaluate_network","text":""},{"location":"api/whittle/evaluate_network/#whittle.evaluate_network.setup","title":"setup","text":"<pre><code>setup(\n    checkpoint_dir: Path,\n    out_dir: Path | None = None,\n    tasks: str | None = None,\n    seed: int = 1337,\n    num_fewshot: int | None = None,\n    batch_size: int | str = 1,\n    latency_batch_size: int = 8,\n    device: str | None = None,\n    limit: float | None = None,\n    measure_flops: bool = False,\n    measure_latency: bool = False,\n) -&gt; None\n</code></pre> <p>Evaluate a model with the LM Evaluation Harness. Compute the latency of a PyTorch model for inference, and FLOPs.</p> PARAMETER DESCRIPTION <code>checkpoint_dir</code> <p>The path to the model's checkpoint directory to load for evaluation.</p> <p> TYPE: <code>Path</code> </p> <code>out_dir</code> <p>Directory in which to save evaluation results. If not provided, saving to <code>checkpoint_dir/eval</code> by default.</p> <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> <code>tasks</code> <p>Task names to evaluate. Example: \"hellaswag,mmlu\"</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The random seed to use for reproducibility.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1337</code> </p> <code>num_fewshot</code> <p>Number of examples in few-shot context.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>batch_size</code> <p>Batch size configuration as positive integer value (default: 1), \"auto\", in the format 'auto:N', where 'auto:4' recomputes the batch size 4 times.</p> <p> TYPE: <code>int | str</code> DEFAULT: <code>1</code> </p> <code>latency_batch_size</code> <p>Batch size for latency computation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>8</code> </p> <code>device</code> <p>Device to use for evaluation, for example, \"cuda\" or \"cuda:0\".</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>limit</code> <p>Limit on number of examples per task.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>measure_flops</code> <p>Whether to compute FLOPs. Default is False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>measure_latency</code> <p>Whether to compute latency. Default is False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>whittle/evaluate_network.py</code> <pre><code>def setup(\n    checkpoint_dir: Path,\n    out_dir: Path | None = None,\n    tasks: str | None = None,\n    seed: int = 1337,\n    num_fewshot: int | None = None,\n    batch_size: int | str = 1,\n    latency_batch_size: int = 8,\n    device: str | None = None,\n    limit: float | None = None,\n    measure_flops: bool = False,\n    measure_latency: bool = False,\n) -&gt; None:\n    \"\"\"\n    Evaluate a model with the LM Evaluation Harness. Compute the latency of a PyTorch model for inference, and FLOPs.\n\n    Arguments:\n        checkpoint_dir: The path to the model's checkpoint directory to load for evaluation.\n        out_dir: Directory in which to save evaluation results. If not provided, saving to `checkpoint_dir/eval` by default.\n        tasks: Task names to evaluate. Example: \"hellaswag,mmlu\"\n        seed: The random seed to use for reproducibility.\n        num_fewshot: Number of examples in few-shot context.\n        batch_size: Batch size configuration as positive integer value (default: 1),\n            \"auto\", in the format 'auto:N', where 'auto:4' recomputes the batch size 4 times.\n        latency_batch_size: Batch size for latency computation.\n        device: Device to use for evaluation, for example, \"cuda\" or \"cuda:0\".\n        limit: Limit on number of examples per task.\n        measure_flops: Whether to compute FLOPs. Default is False.\n        measure_latency: Whether to compute latency. Default is False.\n    \"\"\"\n    if out_dir is None:\n        out_dir = checkpoint_dir / \"eval\"\n\n    metrics_path = out_dir / \"metrics.json\"\n    metrics_path.parent.mkdir(parents=True, exist_ok=True)\n\n    config_attr = {\n        \"fix_head_size\": True,\n        \"model_type\": \"gpt\",\n        \"tie_embeddings\": False,\n    }\n    model = load_checkpoint(\n        checkpoint_dir, model_cls=GPT, config_cls=Config, config_attr=config_attr\n    )\n\n    # compute metrics\n    metrics = {}\n    metrics[\"parameters\"] = compute_parameters(model)\n    if measure_latency:\n        metrics[\"latency\"] = compute_latency(model)\n    if measure_flops:\n        metrics[\"flops\"] = compute_flops(\n            model, batch_size=latency_batch_size, previous_device=device\n        )\n    metrics_path.write_text(json.dumps(metrics, indent=2))\n\n    # downstream task evaluation\n    model.to(device)\n    convert_and_evaluate(\n        model,\n        out_dir=out_dir,\n        tasks=tasks,\n        seed=seed,\n        num_fewshot=num_fewshot,\n        batch_size=batch_size,\n        device=device,\n        limit=limit,\n    )\n</code></pre>"},{"location":"api/whittle/full_finetune/","title":"Full finetune","text":""},{"location":"api/whittle/full_finetune/#whittle.full_finetune","title":"whittle.full_finetune","text":""},{"location":"api/whittle/full_finetune/#whittle.full_finetune.setup","title":"setup","text":"<pre><code>setup(\n    checkpoint_dir: Path,\n    out_dir: Path = Path(\"out/finetune/full\"),\n    precision: str | None = None,\n    devices: int | str = 1,\n    num_nodes: int = 1,\n    resume: bool | Literal[\"auto\"] | Path = False,\n    data: DataModule | None = None,\n    train: TrainArgs = TrainArgs(\n        save_interval=1000,\n        log_interval=1,\n        global_batch_size=16,\n        micro_batch_size=1,\n        lr_warmup_steps=100,\n        epochs=5,\n        max_seq_length=None,\n        max_steps=10000,\n        min_lr=4e-05,\n    ),\n    eval: EvalArgs = EvalArgs(\n        interval=600, max_new_tokens=100\n    ),\n    optimizer: str | dict = \"AdamW\",\n    training_strategy: str = \"sandwich\",\n    logger_name: Literal[\n        \"wandb\", \"tensorboard\", \"csv\"\n    ] = \"csv\",\n    seed: int = 1337,\n    access_token: str | None = None,\n) -&gt; None\n</code></pre> <p>Finetune a model using super-network training strategies.</p> PARAMETER DESCRIPTION <code>checkpoint_dir</code> <p>The path to the base model's checkpoint directory to load for finetuning.</p> <p> TYPE: <code>Path</code> </p> <code>out_dir</code> <p>Directory in which to save checkpoints and logs. If running in a Lightning Studio Job, look for it in /teamspace/jobs//share. <p> TYPE: <code>Path</code> DEFAULT: <code>Path('out/finetune/full')</code> </p> <code>precision</code> <p>The precision to use for finetuning. Possible choices: \"bf16-true\", \"bf16-mixed\", \"32-true\".</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>devices</code> <p>How many devices/GPUs to use</p> <p> TYPE: <code>int | str</code> DEFAULT: <code>1</code> </p> <code>num_nodes</code> <p>How many nodes the code is being run on.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>resume</code> <p>Path to a checkpoint directory to resume from in case training was interrupted, or <code>True</code> to resume from the latest checkpoint in <code>out_dir</code>. An error will be raised if no checkpoint is found. Passing <code>'auto'</code> will resume from the latest checkpoint but not error if no checkpoint exists.</p> <p> TYPE: <code>bool | Literal['auto'] | Path</code> DEFAULT: <code>False</code> </p> <code>data</code> <p>Data-related arguments. If not provided, the default is <code>litgpt.data.Alpaca</code>.</p> <p> TYPE: <code>DataModule | None</code> DEFAULT: <code>None</code> </p> <code>train</code> <p>Training-related arguments. See <code>litgpt.args.TrainArgs</code> for details.</p> <p> TYPE: <code>TrainArgs</code> DEFAULT: <code>TrainArgs(save_interval=1000, log_interval=1, global_batch_size=16, micro_batch_size=1, lr_warmup_steps=100, epochs=5, max_seq_length=None, max_steps=10000, min_lr=4e-05)</code> </p> <code>eval</code> <p>Evaluation-related arguments. See <code>litgpt.args.EvalArgs</code> for details.</p> <p> TYPE: <code>EvalArgs</code> DEFAULT: <code>EvalArgs(interval=600, max_new_tokens=100)</code> </p> <code>optimizer</code> <p>An optimizer name (such as \"AdamW\") or config.</p> <p> TYPE: <code>str | dict</code> DEFAULT: <code>'AdamW'</code> </p> <code>training_strategy</code> <p>Training strategy for super-network training. Possible choices: sandwich, standard</p> <p> TYPE: <code>str</code> DEFAULT: <code>'sandwich'</code> </p> <code>logger_name</code> <p>The name of the logger to send metrics to.</p> <p> TYPE: <code>Literal['wandb', 'tensorboard', 'csv']</code> DEFAULT: <code>'csv'</code> </p> <code>seed</code> <p>The random seed to use for reproducibility.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1337</code> </p> <code>access_token</code> <p>Optional API token to access models with restrictions.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>whittle/full_finetune.py</code> <pre><code>def setup(\n    checkpoint_dir: Path,\n    out_dir: Path = Path(\"out/finetune/full\"),\n    precision: str | None = None,\n    devices: int | str = 1,\n    num_nodes: int = 1,\n    resume: bool | Literal[\"auto\"] | Path = False,\n    data: DataModule | None = None,\n    train: TrainArgs = TrainArgs(\n        save_interval=1000,\n        log_interval=1,\n        global_batch_size=16,\n        micro_batch_size=1,\n        lr_warmup_steps=100,\n        epochs=5,\n        max_seq_length=None,\n        max_steps=10000,\n        #        max_tokens=int(3e12),  # 3 trillion\n        #        max_norm=1.0,\n        min_lr=4e-5,\n    ),\n    eval: EvalArgs = EvalArgs(interval=600, max_new_tokens=100),\n    optimizer: str | dict = \"AdamW\",\n    training_strategy: str = \"sandwich\",\n    logger_name: Literal[\"wandb\", \"tensorboard\", \"csv\"] = \"csv\",\n    seed: int = 1337,\n    access_token: str | None = None,\n) -&gt; None:\n    \"\"\"Finetune a model using super-network training strategies.\n\n    Arguments:\n        checkpoint_dir: The path to the base model's checkpoint directory to load for finetuning.\n        out_dir: Directory in which to save checkpoints and logs. If running in a Lightning Studio Job, look for it in\n            /teamspace/jobs/&lt;job-name&gt;/share.\n        precision: The precision to use for finetuning. Possible choices: \"bf16-true\", \"bf16-mixed\", \"32-true\".\n        devices: How many devices/GPUs to use\n        num_nodes: How many nodes the code is being run on.\n        resume: Path to a checkpoint directory to resume from in case training was interrupted, or ``True`` to resume\n            from the latest checkpoint in ``out_dir``. An error will be raised if no checkpoint is found. Passing\n            ``'auto'`` will resume from the latest checkpoint but not error if no checkpoint exists.\n        data: Data-related arguments. If not provided, the default is ``litgpt.data.Alpaca``.\n        train: Training-related arguments. See ``litgpt.args.TrainArgs`` for details.\n        eval: Evaluation-related arguments. See ``litgpt.args.EvalArgs`` for details.\n        optimizer: An optimizer name (such as \"AdamW\") or config.\n        training_strategy: Training strategy for super-network training. Possible choices: sandwich, standard\n        logger_name: The name of the logger to send metrics to.\n        seed: The random seed to use for reproducibility.\n        access_token: Optional API token to access models with restrictions.\n    \"\"\"\n    checkpoint_dir = auto_download_checkpoint(\n        model_name=checkpoint_dir, access_token=access_token\n    )\n    pprint(locals())\n    data = LLaMaMini() if data is None else data\n    num_devices = parse_devices(devices)\n    out_dir = init_out_dir(out_dir)\n\n    check_valid_checkpoint_dir(checkpoint_dir)\n    config = Config.from_file(checkpoint_dir / \"model_config.yaml\")\n    config.fix_head_size = True\n\n    precision = precision or get_default_supported_precision(training=True)\n    logger = choose_logger(\n        logger_name,\n        out_dir,\n        name=f\"finetune-{config.name}\",\n        resume=bool(resume),\n        log_interval=train.log_interval,\n    )\n\n    assert training_strategy in training_strategies_cls, print(\n        f\"Training strategy is {training_strategy}. Should be in {list(training_strategies_cls)}\"\n    )\n\n    if num_devices * num_nodes &gt; 1:\n        strategy = FSDPStrategy(\n            auto_wrap_policy={Block},\n            activation_checkpointing_policy={Block},\n            state_dict_type=\"full\",\n            limit_all_gathers=True,\n            cpu_offload=False,\n        )\n    else:\n        strategy = \"auto\"\n\n    fabric = L.Fabric(\n        devices=num_devices,\n        num_nodes=num_nodes,\n        strategy=strategy,\n        precision=precision,\n        loggers=logger,\n    )\n\n    if torch.cuda.is_available() and num_devices &gt; 1:\n        check_nvlink_connectivity(fabric)\n\n    fabric.launch(\n        main,\n        num_devices,\n        resume,\n        seed,\n        config,\n        data,\n        checkpoint_dir,\n        out_dir,\n        train,\n        eval,\n        optimizer,\n        training_strategy,\n    )\n</code></pre>"},{"location":"api/whittle/lora/","title":"Lora","text":""},{"location":"api/whittle/lora/#whittle.lora","title":"whittle.lora","text":""},{"location":"api/whittle/lora/#whittle.lora.setup","title":"setup","text":"<pre><code>setup(\n    checkpoint_dir: Path,\n    out_dir: Path = Path(\"out/finetune/lora\"),\n    precision: str | None = None,\n    devices: int = 1,\n    num_nodes: int = 1,\n    lora_r: int = 8,\n    lora_alpha: int = 16,\n    lora_dropout: float = 0.05,\n    lora_query: bool = True,\n    lora_key: bool = True,\n    lora_value: bool = True,\n    lora_projection: bool = True,\n    lora_mlp: bool = True,\n    lora_head: bool = True,\n    lora_emb: bool = True,\n    data: DataModule | None = None,\n    train: TrainArgs = TrainArgs(\n        save_interval=100,\n        log_interval=1,\n        global_batch_size=64,\n        micro_batch_size=1,\n        lr_warmup_steps=100,\n        epochs=1,\n        max_seq_length=None,\n    ),\n    train_strategy: str = \"sandwich\",\n    search_space_type: str = \"hw_gpt_bench\",\n    eval: EvalArgs = EvalArgs(\n        interval=10, max_new_tokens=100, max_iters=100\n    ),\n    optimizer: str | dict = \"AdamW\",\n    logger_name: Literal[\n        \"wandb\", \"tensorboard\", \"csv\"\n    ] = \"wandb\",\n    seed: int = 1337,\n    access_token: str | None = None,\n    downstream_test_iters: int = 500,\n    downstream_dataset: str = \"arc_easy\",\n    resume: bool | Path = True,\n    dataset: str = \"alpaca\",\n    sampler: SamplerArgs = SamplerArgs(),\n) -&gt; None\n</code></pre> <p>Finetune a model using the LoRA method using super-network training strategies.</p> PARAMETER DESCRIPTION <code>checkpoint_dir</code> <p>The path to the base model's checkpoint directory to load for finetuning.</p> <p> TYPE: <code>Path</code> </p> <code>out_dir</code> <p>Directory in which to save checkpoints and logs. If running in a Lightning Studio Job, look for it in /teamspace/jobs//share. <p> TYPE: <code>Path</code> DEFAULT: <code>Path('out/finetune/lora')</code> </p> <code>precision</code> <p>The precision to use for finetuning. Possible choices: \"bf16-true\", \"bf16-mixed\", \"32-true\".</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>devices</code> <p>How many devices/GPUs to use.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>num_nodes</code> <p>How many nodes the code is being run on.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>lora_r</code> <p>The LoRA rank.</p> <p> TYPE: <code>int</code> DEFAULT: <code>8</code> </p> <code>lora_alpha</code> <p>The LoRA alpha.</p> <p> TYPE: <code>int</code> DEFAULT: <code>16</code> </p> <code>lora_dropout</code> <p>The LoRA dropout value.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.05</code> </p> <code>lora_emb</code> <p>Whether to apply LoRA to the embedding weights in the model.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>lora_query</code> <p>Whether to apply LoRA to the query weights in attention.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>lora_key</code> <p>Whether to apply LoRA to the key weights in attention.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>lora_value</code> <p>Whether to apply LoRA to the value weights in attention.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>lora_projection</code> <p>Whether to apply LoRA to the output projection in the attention block.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>lora_mlp</code> <p>Whether to apply LoRA to the weights of the MLP in the attention block.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>lora_head</code> <p>Whether to apply LoRA to output head in GPT.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>data</code> <p>Data-related arguments. If not provided, the default is <code>litgpt.data.Alpaca</code>.</p> <p> TYPE: <code>DataModule | None</code> DEFAULT: <code>None</code> </p> <code>train</code> <p>Training-related arguments. See <code>litgpt.args.TrainArgs</code> for details.</p> <p> TYPE: <code>TrainArgs</code> DEFAULT: <code>TrainArgs(save_interval=100, log_interval=1, global_batch_size=64, micro_batch_size=1, lr_warmup_steps=100, epochs=1, max_seq_length=None)</code> </p> <code>train_strategy</code> <p>The training strategy to use. Possible choices: \"sandwich\", \"standard\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'sandwich'</code> </p> <code>search_space_type</code> <p>The search space to use. Possible choices: \"small\", \"medium\", \"hw_gpt_bench\", \"llama_joint\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'hw_gpt_bench'</code> </p> <code>eval</code> <p>Evaluation-related arguments. See <code>litgpt.args.EvalArgs</code> for details.</p> <p> TYPE: <code>EvalArgs</code> DEFAULT: <code>EvalArgs(interval=10, max_new_tokens=100, max_iters=100)</code> </p> <code>optimizer</code> <p>An optimizer name (such as \"AdamW\") or config.</p> <p> TYPE: <code>str | dict</code> DEFAULT: <code>'AdamW'</code> </p> <code>logger_name</code> <p>The name of the logger to send metrics to.</p> <p> TYPE: <code>Literal['wandb', 'tensorboard', 'csv']</code> DEFAULT: <code>'wandb'</code> </p> <code>seed</code> <p>The random seed to use for reproducibility.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1337</code> </p> <code>access_token</code> <p>Optional API token to access models with restrictions.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>downstream_test_iters</code> <p>The number of iterations after which to test the model on a downstream dataset.</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p> <code>downstream_dataset</code> <p>The downstream dataset to test on.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'arc_easy'</code> </p> <code>resume</code> <p>Whether to resume training from the last checkpoint.</p> <p> TYPE: <code>bool | Path</code> DEFAULT: <code>True</code> </p> <code>dataset</code> <p>The dataset to use for finetuning. Possible choices: \"alpaca\", \"llamamini\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'alpaca'</code> </p> <code>sampler</code> <p>Sampler-related arguments. See <code>whittle.args.SamplerArgs</code> for details.</p> <p> TYPE: <code>SamplerArgs</code> DEFAULT: <code>SamplerArgs()</code> </p> Source code in <code>whittle/lora.py</code> <pre><code>def setup(\n    checkpoint_dir: Path,\n    out_dir: Path = Path(\"out/finetune/lora\"),\n    precision: str | None = None,\n    devices: int = 1,\n    num_nodes: int = 1,\n    lora_r: int = 8,\n    lora_alpha: int = 16,\n    lora_dropout: float = 0.05,\n    lora_query: bool = True,\n    lora_key: bool = True,\n    lora_value: bool = True,\n    lora_projection: bool = True,\n    lora_mlp: bool = True,\n    lora_head: bool = True,\n    lora_emb: bool = True,\n    data: DataModule | None = None,\n    train: TrainArgs = TrainArgs(\n        save_interval=100,\n        log_interval=1,\n        global_batch_size=64,\n        micro_batch_size=1,\n        lr_warmup_steps=100,\n        epochs=1,\n        max_seq_length=None,\n    ),\n    train_strategy: str = \"sandwich\",\n    search_space_type: str = \"hw_gpt_bench\",\n    eval: EvalArgs = EvalArgs(interval=10, max_new_tokens=100, max_iters=100),\n    optimizer: str | dict = \"AdamW\",\n    logger_name: Literal[\"wandb\", \"tensorboard\", \"csv\"] = \"wandb\",\n    seed: int = 1337,\n    access_token: str | None = None,\n    downstream_test_iters: int = 500,\n    downstream_dataset: str = \"arc_easy\",\n    resume: bool | Path = True,\n    dataset: str = \"alpaca\",\n    sampler: SamplerArgs = SamplerArgs(),\n) -&gt; None:\n    \"\"\"Finetune a model using the LoRA method using super-network training strategies.\n\n    Arguments:\n        checkpoint_dir: The path to the base model's checkpoint directory to load for finetuning.\n        out_dir: Directory in which to save checkpoints and logs. If running in a Lightning Studio Job, look for it in\n            /teamspace/jobs/&lt;job-name&gt;/share.\n        precision: The precision to use for finetuning. Possible choices: \"bf16-true\", \"bf16-mixed\", \"32-true\".\n        devices: How many devices/GPUs to use.\n        num_nodes: How many nodes the code is being run on.\n        lora_r: The LoRA rank.\n        lora_alpha: The LoRA alpha.\n        lora_dropout: The LoRA dropout value.\n        lora_emb: Whether to apply LoRA to the embedding weights in the model.\n        lora_query: Whether to apply LoRA to the query weights in attention.\n        lora_key: Whether to apply LoRA to the key weights in attention.\n        lora_value: Whether to apply LoRA to the value weights in attention.\n        lora_projection: Whether to apply LoRA to the output projection in the attention block.\n        lora_mlp: Whether to apply LoRA to the weights of the MLP in the attention block.\n        lora_head: Whether to apply LoRA to output head in GPT.\n        data: Data-related arguments. If not provided, the default is ``litgpt.data.Alpaca``.\n        train: Training-related arguments. See ``litgpt.args.TrainArgs`` for details.\n        train_strategy: The training strategy to use. Possible choices: \"sandwich\", \"standard\".\n        search_space_type: The search space to use. Possible choices: \"small\", \"medium\", \"hw_gpt_bench\", \"llama_joint\".\n        eval: Evaluation-related arguments. See ``litgpt.args.EvalArgs`` for details.\n        optimizer: An optimizer name (such as \"AdamW\") or config.\n        logger_name: The name of the logger to send metrics to.\n        seed: The random seed to use for reproducibility.\n        access_token: Optional API token to access models with restrictions.\n        downstream_test_iters: The number of iterations after which to test the model on a downstream dataset.\n        downstream_dataset: The downstream dataset to test on.\n        resume: Whether to resume training from the last checkpoint.\n        dataset: The dataset to use for finetuning. Possible choices: \"alpaca\", \"llamamini\".\n        sampler: Sampler-related arguments. See ``whittle.args.SamplerArgs`` for details.\n    \"\"\"\n\n    checkpoint_dir = auto_download_checkpoint(\n        model_name=checkpoint_dir, access_token=access_token\n    )\n    pprint(locals())\n    data_str = dataset\n    if data_str == \"alpaca\":\n        data = Alpaca()\n    elif data_str == \"llamamini\":\n        data = LLaMaMini()\n    else:\n        data = None\n    devices = parse_devices(devices)\n    out_dir = init_out_dir(out_dir)\n\n    check_valid_checkpoint_dir(checkpoint_dir)\n    config = Config.from_file(\n        checkpoint_dir / \"model_config.yaml\",\n        lora_r=lora_r,\n        lora_alpha=lora_alpha,\n        lora_dropout=lora_dropout,\n        lora_query=lora_query,\n        lora_key=lora_key,\n        lora_value=lora_value,\n        lora_projection=lora_projection,\n        lora_mlp=lora_mlp,\n        lora_head=lora_head,\n        lora_emb=lora_emb,\n    )\n    config.fix_head_size = True\n    config.tie_embeddings = False\n    config.model_type = \"gpt\"\n    now = datetime.now()\n    sampling_strategy = sampler.sampling_strategy\n    seed_sampler = sampler.seed_sampler\n    num_configs = sampler.num_configs\n    n_trials = sampler.n_trials\n    # Create a timestamp with nanosecond precision\n    time_string = now.strftime(\"%Y%m%d_%H%M%S\")\n    search_space = search_spaces[search_space_type](config)\n    out_dir = Path(\n        f\"{config.name}-{train_strategy}-{search_space_type}-{sampling_strategy}-{data_str}/finetune/lora/\"\n    )\n    id = f\"{train_strategy}-{search_space_type}-{sampling_strategy}-{time_string}-{data_str}-lora\"\n    precision = precision or get_default_supported_precision(training=True)\n    logger = choose_logger(\n        logger_name,\n        out_dir,\n        name=f\"finetune-{config.name}\",\n        log_interval=train.log_interval,\n        id=id,\n        resume=bool(resume),\n        config=dict(\n            train_strategy=train_strategy,\n            search_space_type=search_space_type,\n            sampling_strategy=sampling_strategy,\n            data=data_str,\n            lora_r=lora_r,\n            lora_alpha=lora_alpha,\n            lora_emb=lora_emb,\n            lora_mlp=lora_mlp,\n            lora_head=lora_head,\n            lora_projection=lora_projection,\n            lr_warmup_steps=train.lr_warmup_steps,\n        ),\n    )\n\n    plugins = None\n\n    if devices * num_nodes &gt; 1:\n        strategy = DDPStrategy(find_unused_parameters=True)\n    else:\n        strategy = \"auto\"\n\n    fabric = L.Fabric(\n        devices=devices,\n        num_nodes=num_nodes,\n        strategy=strategy,\n        precision=precision,\n        loggers=logger,\n        plugins=plugins,\n    )\n\n    sampler = get_sampler(\n        sampling_strategy,\n        search_space=search_space,\n        seed=seed_sampler,\n        num_configs=num_configs,\n        n_trials=n_trials,\n    )\n    strategy = get_training_strategy(\n        train_strategy, loss_function=chunked_cross_entropy, lora=True, sampler=sampler\n    )\n    strategy.fabric = fabric\n    strategy.gradient_accumulation_step = train.gradient_accumulation_iters(devices)\n    if torch.cuda.is_available() and devices &gt; 1:\n        check_nvlink_connectivity(fabric)\n\n    fabric.launch(\n        main,\n        devices,\n        seed,\n        config,\n        data,\n        checkpoint_dir,\n        out_dir,\n        train,\n        eval,\n        optimizer,\n        sampling_strategy,\n        strategy,\n        downstream_dataset,\n        downstream_test_iters,\n        resume,\n    )\n</code></pre>"},{"location":"api/whittle/pretrain_super_network/","title":"Pretrain super network","text":""},{"location":"api/whittle/pretrain_super_network/#whittle.pretrain_super_network","title":"whittle.pretrain_super_network","text":""},{"location":"api/whittle/pretrain_super_network/#whittle.pretrain_super_network.setup","title":"setup","text":"<pre><code>setup(\n    model_name: str,\n    model_config: Config | None = None,\n    out_dir: Path = Path(\"../examples/gpt/out/pretrain\"),\n    precision: Literal[\n        \"bf16-true\", \"bf16-mixed\", \"32-true\", None\n    ] = None,\n    initial_checkpoint_dir: Path | None = None,\n    resume: bool | Literal[\"auto\"] | Path = False,\n    data: DataModule | None = None,\n    train: TrainArgs = TrainArgs(\n        save_interval=1000,\n        log_interval=1,\n        global_batch_size=512,\n        micro_batch_size=4,\n        max_tokens=int(3000000000000.0),\n        max_norm=1.0,\n        min_lr=4e-05,\n        lr_warmup_steps=2000,\n        tie_embeddings=False,\n    ),\n    eval: EvalArgs = EvalArgs(interval=1000, max_iters=100),\n    optimizer: str | dict = \"AdamW\",\n    devices: int | str = \"auto\",\n    num_nodes: int = 1,\n    training_strategy: str = \"sandwich\",\n    tokenizer_dir: Path | None = None,\n    logger_name: Literal[\n        \"wandb\", \"tensorboard\", \"csv\"\n    ] = \"tensorboard\",\n    seed: int = 42,\n)\n</code></pre> <p>Pretrain a model.</p> PARAMETER DESCRIPTION <code>model_name</code> <p>The name of the model to pretrain. Choose from names in <code>litgpt.config</code>. Use \"list\" to list the supported models.</p> <p> TYPE: <code>str</code> </p> <code>model_config</code> <p>A <code>litgpt.Config</code> object to define the model architecture. Mutually exclusive with <code>model_config</code>. Overrides the <code>model_name</code> if specified.</p> <p> TYPE: <code>Config | None</code> DEFAULT: <code>None</code> </p> <code>out_dir</code> <p>Directory in which to save checkpoints and logs. If running in a Lightning Studio Job, look for it in /teamspace/jobs//share. <p> TYPE: <code>Path</code> DEFAULT: <code>Path('../examples/gpt/out/pretrain')</code> </p> <code>precision</code> <p>The precision to use for finetuning. Determines a compatible precision setting by default.</p> <p> TYPE: <code>Literal['bf16-true', 'bf16-mixed', '32-true', None]</code> DEFAULT: <code>None</code> </p> <code>initial_checkpoint_dir</code> <p>Optional path to a checkpoint directory to initialize the model from. Useful for continued pretraining. Mutually exclusive with <code>resume</code>.</p> <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> <code>resume</code> <p>Path to a checkpoint directory to resume from in case training was interrupted, or <code>True</code> to resume from the latest checkpoint in <code>out_dir</code>. An error will be raised if no checkpoint is found. Passing <code>'auto'</code> will resume from the latest checkpoint but not error if no checkpoint exists.</p> <p> TYPE: <code>bool | Literal['auto'] | Path</code> DEFAULT: <code>False</code> </p> <code>data</code> <p>Data-related arguments. If not provided, the default is <code>litgpt.data.TinyLlama</code>.</p> <p> TYPE: <code>DataModule | None</code> DEFAULT: <code>None</code> </p> <code>train</code> <p>Training-related arguments. See <code>litgpt.args.TrainArgs</code> for details.</p> <p> TYPE: <code>TrainArgs</code> DEFAULT: <code>TrainArgs(save_interval=1000, log_interval=1, global_batch_size=512, micro_batch_size=4, max_tokens=int(3000000000000.0), max_norm=1.0, min_lr=4e-05, lr_warmup_steps=2000, tie_embeddings=False)</code> </p> <code>eval</code> <p>Evaluation-related arguments. See <code>litgpt.args.EvalArgs</code> for details.</p> <p> TYPE: <code>EvalArgs</code> DEFAULT: <code>EvalArgs(interval=1000, max_iters=100)</code> </p> <code>optimizer</code> <p>An optimizer name (such as \"AdamW\") or config.</p> <p> TYPE: <code>str | dict</code> DEFAULT: <code>'AdamW'</code> </p> <code>training_strategy</code> <p> TYPE: <code>str</code> DEFAULT: <code>'sandwich'</code> </p> <code>devices</code> <p>How many devices/GPUs to use. Uses all GPUs by default.</p> <p> TYPE: <code>int | str</code> DEFAULT: <code>'auto'</code> </p> <code>num_nodes</code> <p>How many nodes the code is being run on.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>tokenizer_dir</code> <p>Optional path to the tokenizer dir that was used for preprocessing the dataset. Only some data module require this.</p> <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> <code>logger_name</code> <p>The name of the logger to send metrics to.</p> <p> TYPE: <code>Literal['wandb', 'tensorboard', 'csv']</code> DEFAULT: <code>'tensorboard'</code> </p> <code>seed</code> <p>The random seed to use for reproducibility.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> Source code in <code>whittle/pretrain_super_network.py</code> <pre><code>def setup(\n    model_name: str,\n    model_config: Config | None = None,\n    out_dir: Path = Path(\"../examples/gpt/out/pretrain\"),\n    precision: Literal[\"bf16-true\", \"bf16-mixed\", \"32-true\", None] = None,\n    initial_checkpoint_dir: Path | None = None,\n    resume: bool | Literal[\"auto\"] | Path = False,\n    data: DataModule | None = None,\n    train: TrainArgs = TrainArgs(\n        save_interval=1000,\n        log_interval=1,\n        global_batch_size=512,\n        micro_batch_size=4,\n        max_tokens=int(3e12),  # 3 trillion\n        max_norm=1.0,\n        min_lr=4e-5,\n        lr_warmup_steps=2000,\n        tie_embeddings=False,\n    ),\n    eval: EvalArgs = EvalArgs(interval=1000, max_iters=100),\n    optimizer: str | dict = \"AdamW\",\n    devices: int | str = \"auto\",\n    num_nodes: int = 1,\n    training_strategy: str = \"sandwich\",\n    tokenizer_dir: Path | None = None,\n    logger_name: Literal[\"wandb\", \"tensorboard\", \"csv\"] = \"tensorboard\",\n    seed: int = 42,\n):\n    \"\"\"Pretrain a model.\n\n    Arguments:\n        model_name: The name of the model to pretrain. Choose from names in ``litgpt.config``. Use \"list\" to list the supported models.\n        model_config: A ``litgpt.Config`` object to define the model architecture. Mutually exclusive with\n            ``model_config``. Overrides the `model_name` if specified.\n        out_dir: Directory in which to save checkpoints and logs. If running in a Lightning Studio Job, look for it in\n            /teamspace/jobs/&lt;job-name&gt;/share.\n        precision: The precision to use for finetuning. Determines a compatible precision setting by default.\n        initial_checkpoint_dir: Optional path to a checkpoint directory to initialize the model from.\n            Useful for continued pretraining. Mutually exclusive with ``resume``.\n        resume: Path to a checkpoint directory to resume from in case training was interrupted, or ``True`` to resume\n            from the latest checkpoint in ``out_dir``. An error will be raised if no checkpoint is found. Passing\n            ``'auto'`` will resume from the latest checkpoint but not error if no checkpoint exists.\n        data: Data-related arguments. If not provided, the default is ``litgpt.data.TinyLlama``.\n        train: Training-related arguments. See ``litgpt.args.TrainArgs`` for details.\n        eval: Evaluation-related arguments. See ``litgpt.args.EvalArgs`` for details.\n        optimizer: An optimizer name (such as \"AdamW\") or config.\n        training_strategy:\n        devices: How many devices/GPUs to use. Uses all GPUs by default.\n        num_nodes: How many nodes the code is being run on.\n        tokenizer_dir: Optional path to the tokenizer dir that was used for preprocessing the dataset. Only some data\n            module require this.\n        logger_name: The name of the logger to send metrics to.\n        seed: The random seed to use for reproducibility.\n    \"\"\"\n    if model_name == \"list\":\n        available_models = \"\\n\".join(sorted(name_to_config))\n        print(f\"Available values:\\n{available_models}\")\n        quit()\n\n    if initial_checkpoint_dir is not None:\n        initial_checkpoint_dir = extend_checkpoint_dir(initial_checkpoint_dir)\n\n    if tokenizer_dir is not None:\n        tokenizer_dir = extend_checkpoint_dir(tokenizer_dir)\n\n    if model_config is None:\n        # Support both model_name options: meta-llama/Meta-Llama-3-8B &amp; Meta-Llama-3-8B\n        try:\n            model_config = Config.from_name(model_name)\n        except ValueError:\n            print(f\"Model name {model_name} is not supported.\\n\")\n            available_models = \"\\n\".join(sorted(name_to_config))\n            print(f\"Available values:\\n{available_models}\")\n            quit()\n\n    assert training_strategy in training_strategies_cls, print(\n        f\"Training strategy is {training_strategy}. Should be in {list(training_strategies_cls)}\"\n    )\n\n    hparams = capture_hparams()\n    data = TinyLlama() if data is None else data\n\n    config = Config.from_name(model_name) if model_config is None else model_config\n    config.fix_head_size = True\n\n    precision = precision or get_default_supported_precision(training=True)\n    num_devices = parse_devices(devices)\n    out_dir = init_out_dir(out_dir)\n    # in case the dataset requires the Tokenizer\n    tokenizer = Tokenizer(tokenizer_dir) if tokenizer_dir is not None else None\n\n    logger = choose_logger(\n        logger_name,\n        out_dir,\n        name=f\"pretrain-{config.name}\",\n        resume=bool(resume),\n        log_interval=train.log_interval,\n    )\n\n    if num_devices * num_nodes &gt; 1:\n        strategy = FSDPStrategy(\n            auto_wrap_policy={Block},\n            state_dict_type=\"full\",\n            sharding_strategy=\"HYBRID_SHARD\",\n        )\n    else:\n        strategy = \"auto\"\n\n    fabric = L.Fabric(\n        devices=num_devices,\n        num_nodes=num_nodes,\n        strategy=strategy,\n        precision=precision,\n        loggers=[logger],\n    )\n\n    if torch.cuda.is_available() and num_devices &gt; 1:\n        check_nvlink_connectivity(fabric)\n\n    fabric.launch()\n\n    fabric.print(pprint.pformat(hparams))\n    if logger_name in (\"tensorboard\", \"wandb\"):\n        fabric.logger.log_hyperparams(hparams)\n\n    main(\n        fabric,\n        num_devices,\n        seed,\n        initial_checkpoint_dir,\n        resume,\n        config,\n        data,\n        out_dir,\n        tokenizer_dir,\n        tokenizer,\n        train,\n        eval,\n        optimizer,\n        training_strategy,\n    )\n</code></pre>"},{"location":"api/whittle/prune/","title":"Prune","text":""},{"location":"api/whittle/prune/#whittle.prune","title":"whittle.prune","text":""},{"location":"api/whittle/prune/#whittle.prune.setup","title":"setup","text":"<pre><code>setup(\n    checkpoint_dir: Path,\n    out_dir: Path | None = None,\n    precision: str | None = None,\n    data: DataModule | None = None,\n    devices: int | str | None = 1,\n    num_nodes: int = 1,\n    prune: PruningArgs = PruningArgs(\n        pruning_strategy=\"mag\",\n        prune_n_weights_per_group=2,\n        weights_per_group=4,\n    ),\n    max_seq_length: int | None = 512,\n    seed: int | None = 1337,\n    access_token: str | None = None,\n    logger_name: Literal[\n        \"wandb\", \"tensorboard\", \"csv\"\n    ] = \"tensorboard\",\n) -&gt; None\n</code></pre> <p>Performs structural pruning on a specified model checkpoint and saves a new checkpoint with the pruned weights set to zero.</p> PARAMETER DESCRIPTION <code>checkpoint_dir</code> <p>The path to the base model's checkpoint directory to load for pruning.</p> <p> TYPE: <code>Path</code> </p> <code>out_dir</code> <p>Directory in which to save checkpoints and logs. If None, final checkpoint is saved in checkpoint_dir/pruning/ <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> <code>precision</code> <p>The precision to use for loading the model. Possible choices: \"bf16-true\", \"bf16-mixed\", \"32-true\".</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>devices</code> <p>How many devices/GPUs to use</p> <p> TYPE: <code>int | str | None</code> DEFAULT: <code>1</code> </p> <code>num_nodes</code> <p>How many nodes the code is being run on.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>prune</code> <p>Pruning-related arguments. See <code>whittle.args.PruneArgs</code> for details.</p> <p> TYPE: <code>PruningArgs</code> DEFAULT: <code>PruningArgs(pruning_strategy='mag', prune_n_weights_per_group=2, weights_per_group=4)</code> </p> <code>seed</code> <p>The random seed to use for reproducibility.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>1337</code> </p> <code>access_token</code> <p>Optional API token to access models with restrictions.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>whittle/prune.py</code> <pre><code>def setup(\n    checkpoint_dir: Path,\n    out_dir: Path | None = None,\n    precision: str | None = None,\n    data: DataModule | None = None,\n    devices: int | str | None = 1,\n    num_nodes: int = 1,\n    prune: PruningArgs = PruningArgs(\n        pruning_strategy=\"mag\",\n        prune_n_weights_per_group=2,\n        weights_per_group=4,\n    ),\n    max_seq_length: int | None = 512,\n    seed: int | None = 1337,\n    access_token: str | None = None,\n    logger_name: Literal[\"wandb\", \"tensorboard\", \"csv\"] = \"tensorboard\",\n) -&gt; None:\n    \"\"\"\n    Performs structural pruning on a specified model checkpoint and saves a new checkpoint with the pruned weights set to zero.\n\n    Arguments:\n        checkpoint_dir: The path to the base model's checkpoint directory to load for pruning.\n        out_dir: Directory in which to save checkpoints and logs. If None, final checkpoint is saved in checkpoint_dir/pruning/&lt;pruning_strategy&gt;\n        precision: The precision to use for loading the model. Possible choices: \"bf16-true\", \"bf16-mixed\", \"32-true\".\n        devices: How many devices/GPUs to use\n        num_nodes: How many nodes the code is being run on.\n        prune: Pruning-related arguments. See ``whittle.args.PruneArgs`` for details.\n        seed: The random seed to use for reproducibility.\n        access_token: Optional API token to access models with restrictions.\n    \"\"\"\n\n    checkpoint_dir = auto_download_checkpoint(\n        model_name=checkpoint_dir, access_token=access_token\n    )\n\n    num_devices = int(parse_devices(devices))\n\n    if out_dir is None:\n        out_dir = checkpoint_dir / \"pruning\" / prune.pruning_strategy\n    out_dir = init_out_dir(out_dir)\n\n    check_valid_checkpoint_dir(checkpoint_dir)\n    config = Config.from_file(checkpoint_dir / \"model_config.yaml\")\n    config.fix_head_size = True\n\n    precision = precision or get_default_supported_precision(training=True)\n\n    logger = choose_logger(\n        logger_name,\n        out_dir,\n        name=f\"prune-{config.name}\",\n    )\n\n    if num_devices * num_nodes &gt; 1:\n        strategy = FSDPStrategy(\n            auto_wrap_policy={Block},\n            activation_checkpointing_policy={Block},\n            state_dict_type=\"full\",\n            limit_all_gathers=True,\n            cpu_offload=False,\n        )\n    else:\n        strategy = \"auto\"\n\n    fabric = L.Fabric(\n        devices=num_devices,\n        num_nodes=num_nodes,\n        strategy=strategy,\n        precision=precision,\n        loggers=[logger],\n    )\n\n    if torch.cuda.is_available() and num_devices &gt; 1:\n        check_nvlink_connectivity(fabric)\n\n    fabric.launch(\n        main,\n        seed,\n        max_seq_length,\n        config,\n        data,\n        checkpoint_dir,\n        out_dir,\n        prune,\n    )\n</code></pre>"},{"location":"api/whittle/search_sub_networks/","title":"Search sub networks","text":""},{"location":"api/whittle/search_sub_networks/#whittle.search_sub_networks","title":"whittle.search_sub_networks","text":""},{"location":"api/whittle/search_sub_networks/#whittle.search_sub_networks.setup","title":"setup","text":"<pre><code>setup(\n    checkpoint_dir: Path,\n    out_dir: Path | None = Path(\"out/finetune/full\"),\n    precision: str | None = None,\n    devices: int | str | None = 1,\n    num_nodes: int = 1,\n    resume: bool | Literal[\"auto\"] | Path | None = False,\n    data: DataModule | None = None,\n    search: SearchArgs = SearchArgs(\n        iterations=100, log_interval=1\n    ),\n    train: TrainArgs = TrainArgs(max_seq_length=512),\n    eval: EvalArgs | None = EvalArgs(),\n    logger_name: (\n        Literal[\"wandb\", \"tensorboard\", \"csv\"] | None\n    ) = \"csv\",\n    seed: int | None = 1337,\n    access_token: str | None = None,\n    param_bins: ParamBinArgs = ParamBinArgs(),\n    performance_metric: str | None = \"val_loss\",\n    efficiency_metric: str | None = \"parameters\",\n    log_objective_names: bool | None = True,\n    save_checkpoints: bool = True,\n    fine_tuned: bool = False,\n    copy_config_files: bool = True,\n    verbose: bool = True,\n    num_workers: int = 4,\n) -&gt; None\n</code></pre> <p>Multi-objective search to select Pareto optimal set of sub-networks from trained super-network.</p> PARAMETER DESCRIPTION <code>checkpoint_dir</code> <p>The path to the base model's checkpoint directory to load for finetuning.</p> <p> TYPE: <code>Path</code> </p> <code>out_dir</code> <p>Directory in which to save checkpoints and logs. If running in a Lightning Studio Job, look for it in /teamspace/jobs//share. <p> TYPE: <code>Path | None</code> DEFAULT: <code>Path('out/finetune/full')</code> </p> <code>precision</code> <p>The precision to use for finetuning. Possible choices: \"bf16-true\", \"bf16-mixed\", \"32-true\".</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>devices</code> <p>How many devices/GPUs to use</p> <p> TYPE: <code>int | str | None</code> DEFAULT: <code>1</code> </p> <code>num_nodes</code> <p>How many nodes the code is being run on.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>resume</code> <p>Path to a checkpoint directory to resume from in case training was interrupted, or <code>True</code> to resume from the latest checkpoint in <code>out_dir</code>. An error will be raised if no checkpoint is found. Passing <code>'auto'</code> will resume from the latest checkpoint but not error if no checkpoint exists.</p> <p> TYPE: <code>bool | Literal['auto'] | Path | None</code> DEFAULT: <code>False</code> </p> <code>data</code> <p>Data-related arguments. If not provided, the default is <code>litgpt.data.TinyStories</code> or <code>litgpt.data.Alpaca</code> for fine-tuned models.</p> <p> TYPE: <code>DataModule | None</code> DEFAULT: <code>None</code> </p> <code>train</code> <p>Training-related arguments. See <code>litgpt.args.TrainArgs</code> for details.</p> <p> TYPE: <code>TrainArgs</code> DEFAULT: <code>TrainArgs(max_seq_length=512)</code> </p> <code>eval</code> <p>Evaluation-related arguments. See <code>litgpt.args.EvalArgs</code> for details.</p> <p> TYPE: <code>EvalArgs | None</code> DEFAULT: <code>EvalArgs()</code> </p> <code>search</code> <p>Search-related arguments. See <code>whittle.args.SearchArgs</code> for details.</p> <p> TYPE: <code>SearchArgs</code> DEFAULT: <code>SearchArgs(iterations=100, log_interval=1)</code> </p> <code>logger_name</code> <p>The name of the logger to send metrics to.</p> <p> TYPE: <code>Literal['wandb', 'tensorboard', 'csv'] | None</code> DEFAULT: <code>'csv'</code> </p> <code>seed</code> <p>The random seed to use for reproducibility.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>1337</code> </p> <code>access_token</code> <p>Optional API token to access models with restrictions.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>param_bins</code> <p>The parameter bins that limit the sub-network params in the search.</p> <p> TYPE: <code>ParamBinArgs</code> DEFAULT: <code>ParamBinArgs()</code> </p> <code>performance_metric</code> <p>The name of the first objective to optimize (possible - val_loss, perplexity). Defaults to \"val_loss\".</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>'val_loss'</code> </p> <code>efficiency_metric</code> <p>The name of the second objective to optimize (possible - parameters, latency, flops). Defaults to \"parameters\".</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>'parameters'</code> </p> <code>log_objective_names</code> <p>Whether to log the names of the objectives in the logger, or log as objective_1 and objective_2. Defaults to True.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>True</code> </p> <code>save_checkpoints</code> <p>Whether to save checkpoints of the sub-networks, or config + path to super-network. Defaults to True. If False, <code>lit_model.pth</code> will have the following format: <code>{'sub_network_config': sub_network_config, 'parent_dir': checkpoint_dir}</code></p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>fine_tuned</code> <p>Whether the model is fine-tuned. Defaults to False. This flag determines the dataset to use if <code>data</code> is not provided. Additionally, it changes the validation function to use for evaluation. fine_tuned=True: litgpt.finetune.lora.validate, fine_tuned=False: litgpt.pretrain.validate.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>copy_config_files</code> <p>Whether to copy the config files from the super-network to the sub-networks. Defaults to True. If set to False, we save <code>parent_dir</code> to <code>lit_model.pth</code>. If save_checkpoints is False, this argument is ignored.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>verbose</code> <p>Whether to print verbose output. Defaults to True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>num_workers</code> <p>Number of workers to use for data loading. Defaults to 4.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> Source code in <code>whittle/search_sub_networks.py</code> <pre><code>def setup(\n    checkpoint_dir: Path,\n    out_dir: Path | None = Path(\"out/finetune/full\"),\n    precision: str | None = None,\n    devices: int | str | None = 1,\n    num_nodes: int = 1,\n    resume: bool | Literal[\"auto\"] | Path | None = False,\n    data: DataModule | None = None,\n    search: SearchArgs = SearchArgs(\n        iterations=100,\n        log_interval=1,\n    ),\n    train: TrainArgs = TrainArgs(\n        max_seq_length=512,\n    ),\n    eval: EvalArgs | None = EvalArgs(),\n    logger_name: Literal[\"wandb\", \"tensorboard\", \"csv\"] | None = \"csv\",\n    seed: int | None = 1337,\n    access_token: str | None = None,\n    param_bins: ParamBinArgs = ParamBinArgs(),\n    performance_metric: str | None = \"val_loss\",\n    efficiency_metric: str | None = \"parameters\",\n    log_objective_names: bool | None = True,\n    save_checkpoints: bool = True,\n    fine_tuned: bool = False,\n    copy_config_files: bool = True,\n    verbose: bool = True,\n    num_workers: int = 4,\n) -&gt; None:\n    \"\"\"\n    Multi-objective search to select Pareto optimal set of sub-networks from trained super-network.\n\n    Arguments:\n        checkpoint_dir: The path to the base model's checkpoint directory to load for finetuning.\n        out_dir: Directory in which to save checkpoints and logs. If running in a Lightning Studio Job, look for it in\n            /teamspace/jobs/&lt;job-name&gt;/share.\n        precision: The precision to use for finetuning. Possible choices: \"bf16-true\", \"bf16-mixed\", \"32-true\".\n        devices: How many devices/GPUs to use\n        num_nodes: How many nodes the code is being run on.\n        resume: Path to a checkpoint directory to resume from in case training was interrupted, or ``True`` to resume\n            from the latest checkpoint in ``out_dir``. An error will be raised if no checkpoint is found. Passing\n            ``'auto'`` will resume from the latest checkpoint but not error if no checkpoint exists.\n        data: Data-related arguments. If not provided, the default is ``litgpt.data.TinyStories`` or ``litgpt.data.Alpaca`` for fine-tuned models.\n        train: Training-related arguments. See ``litgpt.args.TrainArgs`` for details.\n        eval: Evaluation-related arguments. See ``litgpt.args.EvalArgs`` for details.\n        search: Search-related arguments. See ``whittle.args.SearchArgs`` for details.\n        logger_name: The name of the logger to send metrics to.\n        seed: The random seed to use for reproducibility.\n        access_token: Optional API token to access models with restrictions.\n        param_bins: The parameter bins that limit the sub-network params in the search.\n        performance_metric: The name of the first objective to optimize (possible - val_loss, perplexity). Defaults to \"val_loss\".\n        efficiency_metric: The name of the second objective to optimize (possible - parameters, latency, flops). Defaults to \"parameters\".\n        log_objective_names: Whether to log the names of the objectives in the logger, or log as objective_1 and objective_2. Defaults to True.\n        save_checkpoints: Whether to save checkpoints of the sub-networks, or config + path to super-network. Defaults to True.\n            If False, `lit_model.pth` will have the following format:\n            `{'sub_network_config': sub_network_config, 'parent_dir': checkpoint_dir}`\n        fine_tuned: Whether the model is fine-tuned. Defaults to False.\n            This flag determines the dataset to use if `data` is not provided. Additionally, it changes the validation function to use for evaluation.\n            fine_tuned=True: litgpt.finetune.lora.validate, fine_tuned=False: litgpt.pretrain.validate.\n        copy_config_files: Whether to copy the config files from the super-network to the sub-networks. Defaults to True.\n            If set to False, we save `parent_dir` to `lit_model.pth`. If save_checkpoints is False, this argument is ignored.\n        verbose: Whether to print verbose output. Defaults to True.\n        num_workers: Number of workers to use for data loading. Defaults to 4.\n    \"\"\"\n    assert performance_metric in [\n        \"val_loss\",\n        \"perplexity\",\n    ], f\"Invalid objective_1: {performance_metric}, must be 'val_loss' or 'perplexity'\"\n    assert efficiency_metric in [\n        \"parameters\",\n        \"latency\",\n        \"flops\",\n    ], (\n        f\"Invalid objective_2: {efficiency_metric}, must be 'parameters', 'latency' or 'flops'\"\n    )\n\n    checkpoint_dir = auto_download_checkpoint(\n        model_name=checkpoint_dir, access_token=access_token\n    )\n\n    if data is None:\n        # import sys\n        # sys.path.append('../do-not-touch/compressing_llms')\n        # from datasets_custom.llamamini import LLaMaMini\n        # data = LLaMaMini() if fine_tuned else TinyStories()\n        data = (\n            Alpaca(num_workers=num_workers)\n            if fine_tuned\n            else TinyStories(num_workers=num_workers)\n        )\n\n    num_devices = int(parse_devices(devices))\n    out_dir = init_out_dir(out_dir)\n\n    check_valid_checkpoint_dir(checkpoint_dir)\n    config = Config.from_file(checkpoint_dir / \"model_config.yaml\")\n    config.fix_head_size = True\n\n    precision = precision or get_default_supported_precision(training=True)\n    logger = choose_logger(\n        logger_name,\n        out_dir,\n        name=f\"search-{config.name}\",\n        resume=bool(resume),\n        log_interval=search.log_interval,\n    )\n\n    if num_devices * num_nodes &gt; 1:\n        strategy = FSDPStrategy(\n            auto_wrap_policy={Block},\n            activation_checkpointing_policy={Block},\n            state_dict_type=\"full\",\n            limit_all_gathers=True,\n            cpu_offload=False,\n        )\n    else:\n        strategy = \"auto\"\n\n    fabric = L.Fabric(\n        devices=num_devices,\n        num_nodes=num_nodes,\n        strategy=strategy,\n        precision=precision,\n        loggers=logger,\n    )\n\n    if torch.cuda.is_available() and num_devices &gt; 1:\n        check_nvlink_connectivity(fabric)\n\n    fabric.launch(\n        main,\n        num_devices,\n        resume,\n        seed,\n        config,\n        data,\n        checkpoint_dir,\n        out_dir,\n        train,\n        eval,\n        search,\n        param_bins if search.search_strategy == Methods.SRS else None,\n        performance_metric,\n        efficiency_metric,\n        log_objective_names,\n        save_checkpoints,\n        fine_tuned,\n        copy_config_files,\n        verbose,\n    )\n</code></pre>"},{"location":"api/whittle/data/llamamini/","title":"Llamamini","text":""},{"location":"api/whittle/data/llamamini/#whittle.data.llamamini","title":"whittle.data.llamamini","text":"<p>Implementation derived from tloen/alpaca-lora</p>"},{"location":"api/whittle/data/llamamini/#whittle.data.llamamini.LLaMaMini","title":"LLaMaMini  <code>dataclass</code>","text":"<pre><code>LLaMaMini(\n    mask_prompt: bool = False,\n    val_split_fraction: float = 0.1,\n    prompt_style: str | PromptStyle = \"alpaca\",\n    ignore_index: int = -100,\n    seed: int = 42,\n    num_workers: int = 4,\n    include_multiturn_conversations: bool = False,\n    repo_id: str = \"MBZUAI/LaMini-instruction\",\n    access_token: str | None = getenv(\"HF_TOKEN\"),\n)\n</code></pre> <p>               Bases: <code>DataModule</code></p> <p>LLaMaMini data module for supervised finetuning.</p>"},{"location":"api/whittle/data/llamamini/#whittle.data.llamamini.LLaMaMini.access_token","title":"access_token  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>access_token: str | None = field(\n    repr=False, default=getenv(\"HF_TOKEN\")\n)\n</code></pre> <p>The Hugging Face API token to use for authentication. Can also be set through the <code>HF_TOKEN</code> environment variable.</p>"},{"location":"api/whittle/data/llamamini/#whittle.data.llamamini.LLaMaMini.ignore_index","title":"ignore_index  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ignore_index: int = -100\n</code></pre> <p>The index to use for elements to be ignored in the label.</p>"},{"location":"api/whittle/data/llamamini/#whittle.data.llamamini.LLaMaMini.include_multiturn_conversations","title":"include_multiturn_conversations  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>include_multiturn_conversations: bool = False\n</code></pre> <p>Whether to include multi-turn conversations in the dataset.</p>"},{"location":"api/whittle/data/llamamini/#whittle.data.llamamini.LLaMaMini.mask_prompt","title":"mask_prompt  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_prompt: bool = False\n</code></pre> <p>Whether to mask the prompt section from the label (with <code>ignore_index</code>).</p>"},{"location":"api/whittle/data/llamamini/#whittle.data.llamamini.LLaMaMini.num_workers","title":"num_workers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_workers: int = 4\n</code></pre> <p>How many DataLoader processes to use for loading.</p>"},{"location":"api/whittle/data/llamamini/#whittle.data.llamamini.LLaMaMini.prompt_style","title":"prompt_style  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>prompt_style: str | PromptStyle = 'alpaca'\n</code></pre> <p>The style to apply to instruction prompts. See <code>litgpt.prompts</code> for a list of available styles.</p>"},{"location":"api/whittle/data/llamamini/#whittle.data.llamamini.LLaMaMini.repo_id","title":"repo_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>repo_id: str = 'MBZUAI/LaMini-instruction'\n</code></pre> <p>The Hugging Face dataset repository ID from where to download the data.</p>"},{"location":"api/whittle/data/llamamini/#whittle.data.llamamini.LLaMaMini.seed","title":"seed  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>seed: int = 42\n</code></pre> <p>The random seed for creating the train/val splits and shuffling the dataset.</p>"},{"location":"api/whittle/data/llamamini/#whittle.data.llamamini.LLaMaMini.val_split_fraction","title":"val_split_fraction  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>val_split_fraction: float = 0.1\n</code></pre> <p>The fraction of the dataset to use for the validation dataset. The rest is used for training.</p>"},{"location":"api/whittle/eval/utils/","title":"Utils","text":""},{"location":"api/whittle/eval/utils/#whittle.eval.utils","title":"whittle.eval.utils","text":""},{"location":"api/whittle/eval/utils/#whittle.eval.utils.convert_and_evaluate","title":"convert_and_evaluate","text":"<pre><code>convert_and_evaluate(\n    model: GPT,\n    tasks: str | None = None,\n    out_dir: Path | str = \"evaluate\",\n    num_fewshot: int | None = None,\n    batch_size: int | str = 1,\n    device: str | None = None,\n    dtype: str | dtype | None = None,\n    limit: float | None = None,\n    seed: int = 1234,\n    save_filepath: Path | None = None,\n    access_token: str | None = None,\n) -&gt; None\n</code></pre> <p>Evaluate a model with the LM Evaluation Harness.</p> PARAMETER DESCRIPTION <code>model</code> <p>The instantiated model.</p> <p> TYPE: <code>GPT</code> </p> <code>out_dir</code> <p>Directory in which to save the converted checkpoints for evaluation. Saves to <code>checkpoint_dir</code>/evaluate by default.</p> <p> TYPE: <code>Path | str</code> DEFAULT: <code>'evaluate'</code> </p> <code>tasks</code> <p>CSV of task names to evaluate. Example: \"hellaswag,truthfulqa_mc2,mmlu\"</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>num_fewshot</code> <p>Number of examples in few-shot context.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>batch_size</code> <p>Batch size configuration as positive integer value (default: 1), \"auto\", in the format 'auto:N', where 'auto:4' recomputes the batch size 4 times.</p> <p> TYPE: <code>int | str</code> DEFAULT: <code>1</code> </p> <code>device</code> <p>Device to use for evaluation, for example, \"cuda\" or \"cuda:0\".</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>limit</code> <p>Limit on number of examples per task.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>Random seed.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1234</code> </p> <code>save_filepath</code> <p>The file where the results will be saved. Saves to <code>out_dir/results.json</code> by default.</p> <p> TYPE: <code>Path | None</code> DEFAULT: <code>None</code> </p> <code>access_token</code> <p>Optional API token to access models with restrictions.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>whittle/eval/utils.py</code> <pre><code>def convert_and_evaluate(\n    model: GPT,\n    tasks: str | None = None,\n    out_dir: Path | str = \"evaluate\",\n    num_fewshot: int | None = None,\n    batch_size: int | str = 1,\n    device: str | None = None,\n    dtype: str | torch.dtype | None = None,\n    limit: float | None = None,\n    seed: int = 1234,\n    save_filepath: Path | None = None,\n    access_token: str | None = None,\n) -&gt; None:\n    \"\"\"Evaluate a model with the LM Evaluation Harness.\n\n    Arguments:\n        model: The instantiated model.\n        out_dir: Directory in which to save the converted checkpoints for evaluation.\n            Saves to `checkpoint_dir`/evaluate by default.\n        tasks: CSV of task names to evaluate. Example: \"hellaswag,truthfulqa_mc2,mmlu\"\n        num_fewshot: Number of examples in few-shot context.\n        batch_size: Batch size configuration as positive integer value (default: 1),\n            \"auto\", in the format 'auto:N', where 'auto:4' recomputes the batch size 4 times.\n        device: Device to use for evaluation, for example, \"cuda\" or \"cuda:0\".\n        limit: Limit on number of examples per task.\n        seed: Random seed.\n        save_filepath: The file where the results will be saved.\n            Saves to `out_dir/results.json` by default.\n        access_token: Optional API token to access models with restrictions.\n    \"\"\"\n    if tasks is None:\n        from lm_eval.tasks import TaskManager\n\n        taskm = TaskManager()\n        print(\"\\n\".join(taskm.task_index.keys()))\n        print(\n            \"\\n\\nTo evaluate multiple tasks, you can chain the task names \"\n            \"listed above via a comma-separated list.\"\n            \"\\nFor example: `--tasks 'hellaswag,truthfulqa_mc2,mmlu'`. \"\n            \"\\nTo search for a specific task, use `litgpt evaluate list | grep task_name`.\"\n        )\n        return\n\n    pprint(locals())\n\n    if not (isinstance(batch_size, int) and batch_size &gt; 0) and not (\n        isinstance(batch_size, str) and batch_size.startswith(\"auto\")\n    ):\n        raise ValueError(\n            \"batch_size must be a positive integer, 'auto', or in the format 'auto:N'.\"\n        )\n\n    from lm_eval import evaluator\n\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    model = WhittleLM(\n        pretrained=model,\n        device=device,\n        batch_size=batch_size,\n        dtype=dtype,\n    )\n\n    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n    results = evaluator.simple_evaluate(\n        model=model,\n        tasks=tasks.split(\",\"),\n        num_fewshot=num_fewshot,\n        batch_size=batch_size,\n        device=device,\n        limit=limit,\n        random_seed=seed,\n        numpy_random_seed=seed,\n        torch_random_seed=seed,\n    )\n    out_dir = Path(out_dir)\n    out_dir.mkdir(parents=True, exist_ok=True)\n    save_filepath = (\n        out_dir / Path(\"results.json\") if save_filepath is None else Path(save_filepath)\n    )\n    prepare_results(results, save_filepath)\n</code></pre>"},{"location":"api/whittle/eval/whittle_llms/","title":"Whittle llms","text":""},{"location":"api/whittle/eval/whittle_llms/#whittle.eval.whittle_llms","title":"whittle.eval.whittle_llms","text":""},{"location":"api/whittle/eval/whittle_llms/#whittle.eval.whittle_llms.WhittleLM","title":"WhittleLM","text":"<pre><code>WhittleLM(\n    pretrained: GPT,\n    backend: (\n        Literal[\"default\", \"causal\", \"seq2seq\"] | None\n    ) = \"default\",\n    revision: str | None = \"main\",\n    subfolder=None,\n    tokenizer: (\n        str\n        | PreTrainedTokenizer\n        | PreTrainedTokenizerFast\n        | None\n    ) = None,\n    truncation: bool | None = False,\n    logits_cache: bool = True,\n    max_length=None,\n    device=\"cuda\",\n    dtype=\"auto\",\n    batch_size=1,\n    max_batch_size=64,\n    trust_remote_code: bool | None = False,\n    use_fast_tokenizer: bool | None = True,\n    add_bos_token: bool | None = False,\n    prefix_token_id: int | None = None,\n    parallelize: bool | None = False,\n    device_map_option=\"auto\",\n    max_memory_per_gpu=None,\n    max_cpu_memory=None,\n    offload_folder=\"./offload\",\n    peft: str | None = None,\n    delta: str | None = None,\n    autogptq: bool | str | None = False,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>TemplateLM</code></p> <p>An abstracted whittle model class. Enables usage with both models of <code>whittle.models.gpt.GPT</code></p> Source code in <code>whittle/eval/whittle_llms.py</code> <pre><code>def __init__(\n    self,\n    pretrained: GPT,\n    backend: Literal[\"default\", \"causal\", \"seq2seq\"] | None = \"default\",\n    # override whether the model should be treated as decoder-only (causal) or encoder-decoder (seq2seq)\n    revision: str | None = \"main\",\n    subfolder=None,\n    tokenizer: str\n    | transformers.PreTrainedTokenizer\n    | transformers.PreTrainedTokenizerFast\n    | None = None,\n    truncation: bool | None = False,\n    logits_cache: bool = True,\n    max_length=None,\n    device=\"cuda\",\n    dtype=\"auto\",\n    batch_size=1,\n    max_batch_size=64,\n    trust_remote_code: bool | None = False,\n    use_fast_tokenizer: bool | None = True,\n    add_bos_token: bool | None = False,\n    prefix_token_id: int | None = None,\n    # arguments used for splitting a model across GPUs naively.\n    # only used if `parallelize=True`.\n    parallelize: bool | None = False,\n    device_map_option=\"auto\",\n    max_memory_per_gpu=None,\n    max_cpu_memory=None,\n    offload_folder=\"./offload\",\n    # PEFT, delta weights and quantization options\n    peft: str | None = None,\n    delta: str | None = None,\n    autogptq: bool | str | None = False,\n    **kwargs,\n) -&gt; None:\n    super().__init__()\n\n    # optionally: take in an already-initialized transformers.PreTrainedModel\n    eval_logger.warning(\n        \"`pretrained` model kwarg is not of type `str`. Many other model \"\n        \"arguments may be ignored. Please do not launch via accelerate or \"\n        \"use `parallelize=True` if passing an existing model this way.\"\n    )\n    assert not parallelize, (\n        \"`parallelize=True` is not compatible with passing pre-initialized \"\n        \"model to `pretrained`\"\n    )\n    self._model = pretrained\n    self._device = device\n    self._config = self._model.config\n\n    if tokenizer:\n        assert isinstance(tokenizer, transformers.PreTrainedTokenizer) or isinstance(\n            tokenizer, transformers.PreTrainedTokenizerFast\n        )\n        self.tokenizer = tokenizer\n    else:\n        # Get tokenizer\n        model_name = self._model.name_or_path\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n            model_name,\n            revision=revision,\n            trust_remote_code=trust_remote_code,\n            use_fast=use_fast_tokenizer,\n        )\n\n    # determine which of 'causal' and 'seq2seq' backends to use\n    self._get_backend(\n        config=self.config, backend=backend, trust_remote_code=trust_remote_code\n    )\n\n    # load tokenizer so we know tokenizer vocabulary size before loading model and PEFT\n    self._create_tokenizer(\n        pretrained,\n        tokenizer,\n        revision=revision,\n        trust_remote_code=trust_remote_code,\n        use_fast_tokenizer=use_fast_tokenizer,\n    )\n\n    # access self._model through self.model property outside this method\n    if isinstance(self.model, torch.nn.Module):\n        self.model.eval()\n        self.model.tie_weights()\n    self.truncation = truncation\n    self.logits_cache = logits_cache\n    self.vocab_size = self.tokenizer.vocab_size\n    # select (or create) a pad token to use\n    self.tokenizer = configure_pad_token(self.tokenizer, model_config=self.config)\n\n    self.add_bos_token = add_bos_token\n    if \"gemma\" in getattr(self.config, \"model_type\", \"\"):\n        self.add_bos_token = True\n        eval_logger.info(\n            f\"Model type is '{self.config.model_type}', part of \"\n            f\"the Gemma family--a BOS token will be used as Gemma \"\n            f\"underperforms without it.\"\n        )\n\n    self._max_length = max_length\n    self.pretrained = pretrained\n    self.delta = delta\n    self.peft = peft\n    self.revision = revision\n    self.batch_schedule: float = 1\n    self.batch_sizes: dict = {}\n    self.max_batch_size = max_batch_size\n\n    if str(batch_size).startswith(\"auto\"):\n        batch_size = batch_size.split(\":\")\n        self.batch_size_per_gpu = batch_size[0]\n        self.batch_schedule = float(batch_size[1]) if len(batch_size) &gt; 1 else 1\n    else:\n        self.batch_size_per_gpu = int(batch_size)\n\n    # if a PreTrainedModel was passed into HFLM, we forgo distributed setup.\n    eval_logger.warning(\n        \"Passed an already-initialized model through `pretrained`, \"\n        \"assuming single-process call to evaluate() or custom \"\n        \"distributed integration\"\n    )\n    self._rank = 0\n    self._world_size = 1\n\n    self.custom_prefix_token_id = prefix_token_id\n    if prefix_token_id is not None:\n        eval_logger.info(\n            f\"Loglikelihood prefix token id used in \"\n            f\"evaluation: {self.prefix_token_id}\"\n        )\n</code></pre>"},{"location":"api/whittle/eval/whittle_llms/#whittle.eval.whittle_llms.WhittleLM.apply_chat_template","title":"apply_chat_template","text":"<pre><code>apply_chat_template(\n    chat_history: list[dict[str, str]],\n) -&gt; str\n</code></pre> <p>Method to apply a chat template to a list of chat history between user and model.</p> Source code in <code>whittle/eval/whittle_llms.py</code> <pre><code>def apply_chat_template(self, chat_history: list[dict[str, str]]) -&gt; str:\n    \"\"\"\n    Method to apply a chat template to a list of chat history between user and model.\n    \"\"\"\n    return self.tokenizer.apply_chat_template(\n        chat_history, tokenize=False, add_generation_prompt=True\n    )\n</code></pre>"},{"location":"api/whittle/eval/whittle_llms/#whittle.eval.whittle_llms.WhittleLM.tok_encode","title":"tok_encode","text":"<pre><code>tok_encode(\n    string: str,\n    left_truncate_len=None,\n    add_special_tokens=None,\n) -&gt; list[int]\n</code></pre> Source code in <code>whittle/eval/whittle_llms.py</code> <pre><code>def tok_encode(\n    self, string: str, left_truncate_len=None, add_special_tokens=None\n) -&gt; list[int]:\n    \"\"\" \"\"\"\n    # default for None - empty dict, use predefined tokenizer param\n    # used for all models except for CausalLM or predefined value\n    special_tokens_kwargs = {}\n\n    # by default for CausalLM - false or self.add_bos_token is set\n    if add_special_tokens is None:\n        if self.AUTO_MODEL_CLASS == GPT:\n            special_tokens_kwargs = {\n                \"add_special_tokens\": False or self.add_bos_token\n            }\n    # otherwise the method explicitly defines the value\n    else:\n        special_tokens_kwargs = {\"add_special_tokens\": add_special_tokens}\n\n    encoding = self.tokenizer.encode(string, **special_tokens_kwargs)\n\n    # left-truncate the encoded context to be at most `left_truncate_len` tokens long\n    if left_truncate_len:\n        encoding = encoding[-left_truncate_len:]\n\n    return encoding\n</code></pre>"},{"location":"api/whittle/eval/whittle_llms/#whittle.eval.whittle_llms.configure_pad_token","title":"configure_pad_token","text":"<pre><code>configure_pad_token(\n    tokenizer: PreTrainedTokenizerBase,\n    model_config: PretrainedConfig | None = None,\n) -&gt; PreTrainedTokenizerBase\n</code></pre> <p>This function checks if the (Hugging Face) tokenizer has a padding token and sets it if not present. Some tokenizers require special handling.</p> PARAMETER DESCRIPTION <code>tokenizer</code> <p>The tokenizer for which the padding token is to be handled.</p> <p> TYPE: <code>PreTrainedTokenizerBase</code> </p> <code>model_config</code> <p>The configuration of the model. Default is None.</p> <p> TYPE: <code>PretrainedConfig | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>PreTrainedTokenizerBase</code> <p>The tokenizer after the padding token has been handled.</p> RAISES DESCRIPTION <code>AssertionError</code> <p>If the tokenizer is of type RWKVWorldTokenizer or Rwkv5Tokenizer and the padding token id is not 0.</p> Source code in <code>whittle/eval/whittle_llms.py</code> <pre><code>def configure_pad_token(\n    tokenizer: PreTrainedTokenizerBase,\n    model_config: PretrainedConfig | None = None,\n) -&gt; PreTrainedTokenizerBase:\n    \"\"\"\n    This function checks if the (Hugging Face) tokenizer has a padding token and sets it if not present.\n    Some tokenizers require special handling.\n\n    Args:\n        tokenizer: The tokenizer for which the padding token is to be handled.\n        model_config: The configuration of the model. Default is None.\n\n    Returns:\n        The tokenizer after the padding token has been handled.\n\n    Raises:\n        AssertionError: If the tokenizer is of type RWKVWorldTokenizer or Rwkv5Tokenizer and the padding token id is not 0.\n    \"\"\"\n    if tokenizer.pad_token:\n        pass\n    elif tokenizer.unk_token:\n        tokenizer.pad_token_id = tokenizer.unk_token_id\n    elif tokenizer.eos_token:\n        tokenizer.pad_token_id = tokenizer.eos_token_id\n    else:\n        # handle special cases\n        if model_config and getattr(model_config, \"model_type\", None) == \"qwen\":\n            # Qwen's trust_remote_code tokenizer does not allow for adding special tokens\n            tokenizer.pad_token = \"&lt;|endoftext|&gt;\"\n        elif (\n            tokenizer.__class__.__name__ == \"RWKVWorldTokenizer\"\n            or tokenizer.__class__.__name__ == \"Rwkv5Tokenizer\"\n        ):\n            # The RWKV world tokenizer, does not allow for adding special tokens / setting the pad token (which is set as 0)\n            # The additional tokenizer name check is needed, as there exists rwkv4 models with neox tokenizer\n            # ---\n            # Note that the world tokenizer class name, might change in the future for the final huggingface merge\n            # https://github.com/huggingface/transformers/pull/26963\n            assert tokenizer.pad_token_id == 0\n        else:\n            tokenizer.add_special_tokens({\"pad_token\": \"&lt;|pad|&gt;\"})\n\n    return tokenizer\n</code></pre>"},{"location":"api/whittle/lora_model/config/","title":"Config","text":""},{"location":"api/whittle/lora_model/config/#whittle.lora_model.config","title":"whittle.lora_model.config","text":""},{"location":"api/whittle/lora_model/config/#whittle.lora_model.config.LoRAConfig","title":"LoRAConfig  <code>dataclass</code>","text":"<pre><code>LoRAConfig(\n    lora_r: int = 4,\n    lora_alpha: int = 1,\n    lora_dropout: float = 0.0,\n    lora_query: bool = False,\n    lora_key: bool = False,\n    lora_value: bool = False,\n    lora_projection: bool = False,\n    lora_mlp: bool = False,\n    lora_head: bool = False,\n    lora_emb: bool = False,\n)\n</code></pre> <p>               Bases: <code>Config</code></p> PARAMETER DESCRIPTION <code>lora_r</code> <p>rank of the weight update matrices. To make sense of using LoRA the rank should be smaller than the rank of the weights of the model. The rank can be as low as 1: arxiv.org/pdf/2106.09685.pdf (section 7.2)</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>lora_alpha</code> <p>alpha is needed for scaling updates as alpha/r \"This scaling helps to reduce the need to retune hyperparameters when we vary r\" arxiv.org/pdf/2106.09685.pdf (section 4.1)</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>lora_dropout</code> <p>dropout that is applied on the input in the LoRA branch (before multiplying by matrix A)</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>lora_query</code> <p>whether to apply LoRA to the query</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>lora_key</code> <p>whether to apply LoRA to the key</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>lora_value</code> <p>whether to apply LoRA to the value</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>lora_projection</code> <p>whether to apply LoRA to the projection</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>lora_mlp</code> <p>whether to apply LoRA to the MLP</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>lora_head</code> <p>whether to apply LoRA to the head</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>lora_emb</code> <p>whether to apply LoRA to the embedding</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"api/whittle/lora_model/lora_attention/","title":"Lora attention","text":""},{"location":"api/whittle/lora_model/lora_attention/#whittle.lora_model.lora_attention","title":"whittle.lora_model.lora_attention","text":""},{"location":"api/whittle/lora_model/lora_attention/#whittle.lora_model.lora_attention.CausalSelfAttention","title":"CausalSelfAttention","text":"<pre><code>CausalSelfAttention(config: LoRAConfig, block_idx: int)\n</code></pre> <p>               Bases: <code>CausalSelfAttention</code></p> Source code in <code>whittle/lora_model/lora_attention.py</code> <pre><code>def __init__(self, config: Config, block_idx: int) -&gt; None:\n    # Skip the parent class __init__ altogether and replace it to avoid\n    # useless allocations\n    super().__init__(config=config, block_idx=block_idx)\n    shape = (config.n_head + 2 * config.n_query_groups) * config.head_size\n    # key, query, value projections for all heads, but in a batch\n    self.attn = LoRAQKVLinear(\n        config=config,\n        in_features=config.n_embd,\n        out_features=shape,\n        r=config.lora_r,\n        lora_alpha=config.lora_alpha,\n        lora_dropout=config.lora_dropout,\n        enable_lora=(config.lora_query, config.lora_key, config.lora_value),\n        bias=config.bias,\n        # for MQA/GQA support\n        head_size=config.head_size,\n        n_head=config.n_head,\n        n_query_groups=config.n_query_groups,\n        fix_head_size=config.fix_head_size,\n    )\n    # output projection\n    # if `head_size` is explicitly specified in the config, `n_emd` might not be equal to `head_size * n_head`\n    self.proj = LoRALinearProj(\n        config.head_size * config.n_head,\n        config.n_embd,\n        bias=config.bias,\n        r=(config.lora_r if config.lora_projection else 0),\n        lora_alpha=config.lora_alpha,\n        lora_dropout=config.lora_dropout,\n    )\n    # disabled by default\n    self.kv_cache: KVCache | None = None\n\n    self.config = config\n    self.apply_sliding_window_attention = (\n        config.sliding_window_size is not None\n        and block_idx % config.sliding_window_layer_placing == 0\n    )\n\n    # Set current sub-network to super-network\n    self.sub_network_n_embd = self.config.n_embd\n    self.sub_network_n_head = self.config.n_head\n    self.sub_network_head_size = self.config.head_size\n    self.sub_network_qkv_shape = (\n        self.config.n_head + 2 * self.config.n_query_groups\n    ) * self.config.head_size\n    self.sub_network_query_groups = self.config.n_query_groups\n    self.sub_network_q_per_kv = (\n        self.sub_network_n_head // self.sub_network_query_groups\n    )\n    self.sub_attention_scaler = self.config.attention_scores_scalar\n    self.q_per_kv = self.config.n_head // self.config.n_query_groups\n    self.qkv_indices = None\n    self.proj_indices = None\n</code></pre>"},{"location":"api/whittle/lora_model/lora_attention/#whittle.lora_model.lora_attention.CausalSelfAttention.reset_super_network","title":"reset_super_network","text":"<pre><code>reset_super_network()\n</code></pre> <p>Resets the dimensionality of the current sub-network to the super-network dimensionality.</p> Source code in <code>whittle/models/gpt/blocks/causal_self_attention.py</code> <pre><code>def reset_super_network(self):\n    \"\"\"Resets the dimensionality of the current sub-network to the super-network dimensionality.\"\"\"\n    self.sub_network_n_embd = self.config.n_embd\n    self.sub_network_n_head = self.config.n_head\n    self.sub_network_head_size = self.config.head_size\n    self.sub_network_qkv_shape = (\n        self.config.n_head + 2 * self.config.n_query_groups\n    ) * self.config.head_size\n    self.sub_network_query_groups = self.config.n_query_groups\n    self.sub_network_q_per_kv = int(\n        self.sub_network_n_head // self.sub_network_query_groups\n    )\n    self.attn.reset_super_network()\n    self.proj.reset_super_network()\n    self.sub_attention_scaler = self.config.attention_scores_scalar\n    self.qkv_indices = None\n    self.proj_indices = None\n</code></pre>"},{"location":"api/whittle/lora_model/lora_attention/#whittle.lora_model.lora_attention.CausalSelfAttention.set_sub_network","title":"set_sub_network","text":"<pre><code>set_sub_network(\n    sub_network_n_embd: int,\n    sub_network_n_head: int,\n    sub_network_query_groups: int,\n    sub_network_head_size: int,\n)\n</code></pre> <p>Sets the CausalSelfAttention block to the specified sub-network dimensionality.</p> PARAMETER DESCRIPTION <code>sub_network_n_embd</code> <p>Embedding dimension of the sub-network</p> <p> TYPE: <code>int</code> </p> <code>sub_network_n_head</code> <p>Number of attention heads in the sub-network</p> <p> TYPE: <code>int</code> </p> <code>sub_network_query_groups</code> <p>Number of query groups for grouped-query attention (GQA).</p> <p> TYPE: <code>int</code> </p> <code>sub_network_head_size</code> <p>Size of each attention head in the sub-network.</p> <p> TYPE: <code>int</code> </p> Source code in <code>whittle/lora_model/lora_attention.py</code> <pre><code>def set_sub_network(\n    self,\n    sub_network_n_embd: int,\n    sub_network_n_head: int,\n    sub_network_query_groups: int,\n    sub_network_head_size: int,\n):\n    \"\"\"\n    Sets the CausalSelfAttention block to the specified sub-network dimensionality.\n\n    Args:\n        sub_network_n_embd: Embedding dimension of the sub-network\n        sub_network_n_head: Number of attention heads in the sub-network\n        sub_network_query_groups: Number of query groups for grouped-query attention (GQA).\n        sub_network_head_size: Size of each attention head in the sub-network.\n    \"\"\"\n    self.sub_network_n_embd = (\n        sub_network_n_embd if sub_network_n_embd else self.config.n_embd\n    )\n    self.sub_network_n_head = (\n        sub_network_n_head if sub_network_n_head else self.config.n_head\n    )\n    self.sub_network_query_groups = (\n        sub_network_query_groups\n        if sub_network_query_groups\n        else self.config.n_query_groups\n    )\n    self.sub_network_head_size = (\n        sub_network_head_size if sub_network_head_size else self.config.head_size\n    )\n    if self.config.n_query_groups == 1:\n        q_per_kv = self.sub_network_n_head\n        self.sub_network_query_groups = 1\n    elif (\n        self.config.n_head != self.config.n_query_groups\n        and self.config.n_query_groups != 1\n    ):\n        self.sub_network_query_groups = (\n            sub_network_query_groups\n            if sub_network_query_groups\n            else self.config.n_query_groups\n        )\n        q_per_kv = self.sub_network_n_head // self.config.n_query_groups\n    elif self.config.n_head == self.config.n_query_groups:\n        q_per_kv = 1\n        self.sub_network_query_groups = self.sub_network_n_head\n    self.sub_network_qkv_shape = (\n        (q_per_kv + 2) * self.sub_network_head_size * self.sub_network_query_groups\n    )\n    self.sub_network_q_per_kv = int(q_per_kv)\n    self.qkv_indices = self.get_qkv_indices()\n    self.attn.set_sub_network(\n        self.sub_network_n_embd,\n        self.sub_network_qkv_shape,\n        qkv_indices=self.qkv_indices,\n        sub_network_n_head=self.sub_network_n_head,\n        sub_network_query_groups=self.sub_network_query_groups,\n        sub_network_head_size=self.sub_network_head_size,\n        sub_network_q_per_kv=self.q_per_kv,\n    )\n    self.proj_indices = self.get_proj_indices()\n    self.proj.set_sub_network(\n        self.sub_network_head_size\n        * self.sub_network_query_groups\n        * self.sub_network_q_per_kv,\n        self.sub_network_n_embd,\n        self.proj_indices,\n    )\n    if self.config.attention_scores_scalar:\n        self.sub_attention_scaler = self.sub_network_n_embd // self.sub_network_n_head\n    else:\n        self.sub_attention_scaler = self.config.attention_scores_scalar\n</code></pre>"},{"location":"api/whittle/lora_model/lora_block/","title":"Lora block","text":""},{"location":"api/whittle/lora_model/lora_block/#whittle.lora_model.lora_block","title":"whittle.lora_model.lora_block","text":""},{"location":"api/whittle/lora_model/lora_embedding/","title":"Lora embedding","text":""},{"location":"api/whittle/lora_model/lora_embedding/#whittle.lora_model.lora_embedding","title":"whittle.lora_model.lora_embedding","text":""},{"location":"api/whittle/lora_model/lora_embedding/#whittle.lora_model.lora_embedding.LoRAEmbedding","title":"LoRAEmbedding","text":"<pre><code>LoRAEmbedding(\n    num_embeddings: int,\n    embedding_dim: int,\n    r: int = 0,\n    lora_alpha: int = 1,\n    lora_dropout: float = 0.0,\n    **kwargs: Any\n)\n</code></pre> <p>               Bases: <code>LoRALayer</code></p> This class has three weight matrices <ol> <li>Pretrained weights are stored as <code>self.linear.weight</code></li> <li>LoRA A matrix as <code>self.lora_A</code></li> <li>LoRA B matrix as <code>self.lora_B</code></li> </ol> <p>Only LoRA's A and B matrices are updated, pretrained weights stay frozen.</p> PARAMETER DESCRIPTION <code>num_embeddings</code> <p>Number of embeddings in the vocabulary.</p> <p> TYPE: <code>int</code> </p> <code>embedding_dim</code> <p>Dimension of the embedding vectors.</p> <p> TYPE: <code>int</code> </p> <code>r</code> <p>Rank of the weight update matrices.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>lora_alpha</code> <p>Alpha is needed for scaling updates as alpha/r.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>lora_dropout</code> <p>Dropout that is applied on the input in the LoRA branch (before multiplying by matrix A).</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>**kwargs</code> <p>Additional arguments to be passed to the <code>torch.nn.Embedding</code> constructor.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>whittle/lora_model/lora_embedding.py</code> <pre><code>def __init__(\n    self,\n    # \u2193 this part is for pretrained weights\n    num_embeddings: int,\n    embedding_dim: int,\n    # \u2193 the remaining part is for LoRA\n    r: int = 0,\n    lora_alpha: int = 1,\n    lora_dropout: float = 0.0,\n    **kwargs: Any,\n):\n    \"\"\"LoRA wrapper around linear class.\n\n    This class has three weight matrices:\n        1. Pretrained weights are stored as `self.linear.weight`\n        2. LoRA A matrix as `self.lora_A`\n        3. LoRA B matrix as `self.lora_B`\n    Only LoRA's A and B matrices are updated, pretrained weights stay frozen.\n\n    Args:\n        num_embeddings: Number of embeddings in the vocabulary.\n        embedding_dim: Dimension of the embedding vectors.\n        r: Rank of the weight update matrices.\n        lora_alpha: Alpha is needed for scaling updates as alpha/r.\n        lora_dropout: Dropout that is applied on the input in the LoRA branch (before multiplying by matrix A).\n        **kwargs: Additional arguments to be passed to the `torch.nn.Embedding` constructor.\n    \"\"\"\n    super().__init__(r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout)\n    self.embedding = Embedding(num_embeddings, embedding_dim, **kwargs)\n    self.num_embeddings = num_embeddings\n    self.embedding_dim = embedding_dim\n    self.sub_network_embedding_dim = embedding_dim\n    self.merged: bool = False\n    # Actual trainable parameters\n    if r &gt; 0:\n        self.lora_A = nn.Parameter(torch.empty((r, num_embeddings)))\n        self.lora_B = nn.Parameter(torch.empty((embedding_dim, r)))\n        self.scaling = self.lora_alpha / self.r\n        self.reset_parameters()\n</code></pre>"},{"location":"api/whittle/lora_model/lora_embedding/#whittle.lora_model.lora_embedding.LoRAEmbedding.get_lora_AB","title":"get_lora_AB","text":"<pre><code>get_lora_AB() -&gt; Tensor\n</code></pre> <p>Return merged lora_A and lora_B matrices with the same shape as the pretrained weights.</p> Source code in <code>whittle/lora_model/lora_embedding.py</code> <pre><code>def get_lora_AB(self) -&gt; torch.Tensor:\n    \"\"\"Return merged lora_A and lora_B matrices with the same shape as the pretrained weights.\"\"\"\n    ab = (\n        self.lora_B[: self.sub_network_embedding_dim, :] @ self.lora_A\n    ) * self.scaling\n    return ab.transpose(0, 1)\n</code></pre>"},{"location":"api/whittle/lora_model/lora_embedding/#whittle.lora_model.lora_embedding.LoRAEmbedding.merge","title":"merge","text":"<pre><code>merge() -&gt; None\n</code></pre> <p>Merges the LoRA weights into the full-rank weights (W = W + delta_W).</p> Source code in <code>whittle/lora_model/lora_embedding.py</code> <pre><code>def merge(self) -&gt; None:\n    \"\"\"Merges the LoRA weights into the full-rank weights (W = W + delta_W).\"\"\"\n    if self.r &gt; 0 and not self.merged:\n        pretrained_dtype = self.embedding.weight.data.dtype\n        lora_data = self.get_lora_AB()\n        # if only the pretrained are in quantized form - dequantize, sum with LoRA and quantize the result\n        if pretrained_dtype == torch.uint8:\n            import bitsandbytes as bnb\n\n            weight = self.embedding.weight\n            # dequantize the pretrained weights\n            weight_data = bnb.functional.dequantize_4bit(\n                weight.data, weight.quant_state\n            ).to(lora_data.dtype)\n            # add pretrained and LoRA weights\n            weight_data += lora_data\n            # assign updated weights and quantize by moving to CUDA device\n            self.embedding.weight = bnb.nn.Params4bit(\n                weight_data, requires_grad=False, **weight.__dict__\n            )\n            self.embedding.weight.cuda(weight.device)\n        else:\n            # self.linear might be on CPU and lora_data on CUDA\n            # the inplace add will preserve the dtype of linear.weight\n            self.embedding.weight.data += lora_data.to(\n                device=self.embedding.weight.data.device\n            )\n        self.merged = True\n</code></pre>"},{"location":"api/whittle/lora_model/lora_embedding/#whittle.lora_model.lora_embedding.LoRAEmbedding.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset all the weights, even including pretrained ones.</p> Source code in <code>whittle/lora_model/lora_embedding.py</code> <pre><code>def reset_parameters(self) -&gt; None:\n    \"\"\"Reset all the weights, even including pretrained ones.\"\"\"\n    if hasattr(self, \"lora_A\"):\n        # initialize A the same way as the default for nn.Linear and B to zero\n        # Wondering why 'a' is equal to math.sqrt(5)?: https://github.com/pytorch/pytorch/issues/15314\n        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n        nn.init.zeros_(self.lora_B)\n</code></pre>"},{"location":"api/whittle/lora_model/lora_gpt/","title":"Lora gpt","text":""},{"location":"api/whittle/lora_model/lora_gpt/#whittle.lora_model.lora_gpt","title":"whittle.lora_model.lora_gpt","text":""},{"location":"api/whittle/lora_model/lora_gpt/#whittle.lora_model.lora_gpt.GPT","title":"GPT","text":"<pre><code>GPT(config: LoRAConfig)\n</code></pre> <p>               Bases: <code>GPT</code></p> Source code in <code>whittle/lora_model/lora_gpt.py</code> <pre><code>def __init__(self, config: Config) -&gt; None:\n    super().__init__(config)\n    assert config.padded_vocab_size is not None\n    self.config = config\n    self.lm_head = LoRALinear(\n        config.n_embd,\n        config.padded_vocab_size,\n        bias=config.lm_head_bias,\n        r=(config.lora_r if config.lora_head else 0),\n        lora_alpha=config.lora_alpha,\n        lora_dropout=config.lora_dropout,\n    )\n    self.transformer = nn.ModuleDict(\n        dict(\n            wte=LoRAEmbedding(\n                config.padded_vocab_size,\n                config.n_embd,\n                r=(config.lora_r if config.lora_emb else 0),\n                lora_alpha=config.lora_alpha,\n                lora_dropout=config.lora_dropout,\n            ),\n            h=nn.ModuleList(Block(config, i) for i in range(config.n_layer)),\n            ln_f=self.norm_class(config.n_embd, eps=config.norm_eps),\n        )\n    )\n    self.max_layer = config.n_layer\n    self.max_seq_length = self.config.block_size\n    self.mask_cache: torch.Tensor | None = None\n\n    # Set current sub-network to super-network\n    self.sub_network_n_embd = self.config.n_embd\n    self.sub_network_intermediate_size = self.config.intermediate_size\n    self.sub_network_num_heads = self.config.n_head\n    self.sub_network_n_layers = self.config.n_layer\n    self.sub_network_head_size: int | None = self.config.head_size\n    self.sub_network_query_groups: int | None = self.config.n_query_groups\n    self.sub_network_rope_n_elem = self.config.rope_n_elem\n    self.cos: torch.Tensor\n    self.sin: torch.Tensor\n    self.config.is_encoder_decoder = False\n    self.main_input_name = \"input_pos\"\n    self._supports_cache_class = True\n    self.sub_network_head_size = None\n</code></pre>"},{"location":"api/whittle/lora_model/lora_gpt/#whittle.lora_model.lora_gpt.GPT.reset_super_network","title":"reset_super_network","text":"<pre><code>reset_super_network()\n</code></pre> <p>Resets the GPT model to the original super-network dimensionality.</p> Source code in <code>whittle/models/gpt/model.py</code> <pre><code>def reset_super_network(self):\n    \"\"\"\n    Resets the GPT model to the original super-network dimensionality.\n    \"\"\"\n    rebuild_rope = self.sub_network_rope_n_elem != self.config.rope_n_elem\n\n    self.sub_network_n_embd = self.config.n_embd\n    self.sub_network_intermediate_size = self.config.intermediate_size\n    self.sub_network_num_heads = self.config.n_head\n    self.sub_network_n_layers = self.config.n_layer\n    self.sub_network_head_size: int | None = self.config.head_size  # type: ignore\n    self.sub_network_query_groups: int | None = self.config.n_query_groups  # type: ignore\n    self.sub_network_rope_n_elem = self.config.rope_n_elem\n    self.transformer.wte.reset_super_network()\n    self.transformer.ln_f.reset_super_network()\n    for i in range(self.config.n_layer):\n        block = self.transformer.h[i]\n        block.reset_super_network()\n    self.lm_head.reset_super_network()\n\n    # rebuild the rope cache\n    if rebuild_rope:\n        self.reset_parameters()\n</code></pre>"},{"location":"api/whittle/lora_model/lora_gpt/#whittle.lora_model.lora_gpt.GPT.select_sub_network","title":"select_sub_network","text":"<pre><code>select_sub_network(config: dict[str, Any]) -&gt; None\n</code></pre> <p>Selects and sets the sub-network configuration based on the provided configuration.</p> Source code in <code>whittle/models/gpt/model.py</code> <pre><code>def select_sub_network(self, config: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Selects and sets the sub-network configuration based on the provided configuration.\n    \"\"\"\n    self.set_sub_network(\n        config[\"embed_dim\"],\n        int(config[\"mlp_ratio\"] * config[\"embed_dim\"]),\n        config[\"num_heads\"],\n        config[\"depth\"],\n        sub_network_head_size=config.get(\"head_size\", None),\n        sub_network_query_groups=config.get(\"n_query_groups\", None),\n    )\n</code></pre>"},{"location":"api/whittle/lora_model/lora_gpt/#whittle.lora_model.lora_gpt.GPT.set_sub_network","title":"set_sub_network","text":"<pre><code>set_sub_network(\n    sub_network_n_embd: int,\n    sub_network_intermediate_size: int,\n    sub_network_num_heads: int,\n    sub_network_n_layers: int,\n    sub_network_query_groups: int | None = None,\n    sub_network_head_size: int | None = None,\n) -&gt; None\n</code></pre> <p>Sets the GPT model to the specified sub-network dimensionality. Input arguments are set to the specified sub-network dimensionality.</p> PARAMETER DESCRIPTION <code>sub_network_n_embd</code> <p>Embedding dimension of the sub-network.</p> <p> TYPE: <code>int</code> </p> <code>sub_network_intermediate_size</code> <p>Intermediate size of the sub-network.</p> <p> TYPE: <code>int</code> </p> <code>sub_network_num_heads</code> <p>Number of attention heads in the sub-network.</p> <p> TYPE: <code>int</code> </p> <code>sub_network_n_layers</code> <p>Number of layers in the sub-network.</p> <p> TYPE: <code>int</code> </p> <code>sub_network_query_groups</code> <p>Number of query groups in the sub-network. Defaults to None.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>sub_network_head_size</code> <p>Size of each attention head in the sub-network. Defaults to None.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>whittle/models/gpt/model.py</code> <pre><code>def set_sub_network(\n    self,\n    sub_network_n_embd: int,\n    sub_network_intermediate_size: int,\n    sub_network_num_heads: int,\n    sub_network_n_layers: int,\n    sub_network_query_groups: int | None = None,\n    sub_network_head_size: int | None = None,\n) -&gt; None:\n    \"\"\"\n    Sets the GPT model to the specified sub-network dimensionality.\n    Input arguments are set to the specified sub-network dimensionality.\n\n    Args:\n        sub_network_n_embd: Embedding dimension of the sub-network.\n        sub_network_intermediate_size: Intermediate size of the sub-network.\n        sub_network_num_heads: Number of attention heads in the sub-network.\n        sub_network_n_layers: Number of layers in the sub-network.\n        sub_network_query_groups: Number of query groups in the sub-network. Defaults to None.\n        sub_network_head_size: Size of each attention head in the sub-network. Defaults to None.\n    \"\"\"\n    self.sub_network_n_embd = sub_network_n_embd\n    self.sub_network_intermediate_size = sub_network_intermediate_size\n    self.sub_network_num_heads = sub_network_num_heads\n    self.sub_network_n_layers = sub_network_n_layers\n    self.transformer.wte.set_sub_network(self.sub_network_n_embd)\n    self.transformer.ln_f.set_sub_network(self.sub_network_n_embd)\n    if self.config.n_query_groups == 1:\n        self.sub_network_query_groups = 1\n        self.sub_network_num_heads = (\n            sub_network_num_heads\n            if sub_network_num_heads is not None\n            else self.config.n_head\n        )\n    elif self.config.n_head != self.config.n_query_groups:\n        self.sub_network_num_heads = (\n            sub_network_num_heads\n            if sub_network_num_heads is not None\n            else self.config.n_head\n        )\n        self.sub_network_query_groups = (\n            sub_network_query_groups\n            if sub_network_query_groups is not None\n            else self.config.n_query_groups\n        )\n    else:\n        self.sub_network_query_groups = (\n            sub_network_query_groups\n            if sub_network_query_groups is not None\n            else self.config.n_head\n        )\n    if self.config.fix_head_size:\n        if sub_network_head_size is None:\n            self.sub_network_head_size = self.config.head_size\n        else:\n            self.sub_network_head_size = sub_network_head_size\n    else:\n        if sub_network_head_size is not None:\n            self.sub_network_head_size = sub_network_head_size\n        else:\n            self.sub_network_head_size = (\n                self.sub_network_n_embd // self.sub_network_num_heads\n            )\n    for i in range(self.sub_network_n_layers):\n        block = self.transformer.h[i]\n        block.set_sub_network(\n            self.sub_network_n_embd,\n            self.sub_network_intermediate_size,\n            self.sub_network_num_heads,\n            self.sub_network_query_groups,\n            self.sub_network_head_size,\n        )\n    # these change inside causal_self_attention\n    if self.sub_network_n_layers &gt; 0:\n        self.sub_network_query_groups = block.attn.sub_network_query_groups\n\n    self.lm_head.set_sub_network(\n        self.sub_network_n_embd, self.config.padded_vocab_size\n    )\n\n    # change the rope cache to match n_elem induced by subnet head size\n    self.sub_network_rope_n_elem = int(\n        self.config.rotary_percentage * self.sub_network_head_size\n    )\n    self.cos, self.sin = self.rope_cache(\n        seq_len=self._max_seq_length,\n        n_elem=self.sub_network_rope_n_elem,\n        device=self.cos.device,\n    )\n</code></pre>"},{"location":"api/whittle/lora_model/lora_linear/","title":"Lora linear","text":""},{"location":"api/whittle/lora_model/lora_linear/#whittle.lora_model.lora_linear","title":"whittle.lora_model.lora_linear","text":""},{"location":"api/whittle/lora_model/lora_linear/#whittle.lora_model.lora_linear.LoRALinear","title":"LoRALinear","text":"<pre><code>LoRALinear(\n    in_features: int,\n    out_features: int,\n    r: int = 0,\n    lora_alpha: int = 1,\n    lora_dropout: float = 0.0,\n    **kwargs: Any\n)\n</code></pre> <p>               Bases: <code>LoRALayer</code></p> This class has three weight matrices <ol> <li>Pretrained weights are stored as <code>self.linear.weight</code></li> <li>LoRA A matrix as <code>self.lora_A</code></li> <li>LoRA B matrix as <code>self.lora_B</code></li> </ol> <p>Only LoRA's A and B matrices are updated, pretrained weights stay frozen.</p> PARAMETER DESCRIPTION <code>in_features</code> <p>number of input features of the pretrained weights</p> <p> TYPE: <code>int</code> </p> <code>out_features</code> <p>number of output features of the pretrained weights</p> <p> TYPE: <code>int</code> </p> <code>r</code> <p>rank of the weight update matrices. To make sense of using LoRA the rank should be smaller than the rank of the weights of the model. The rank can be as low as 1: arxiv.org/pdf/2106.09685.pdf (section 7.2)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>lora_alpha</code> <p>alpha is needed for scaling updates as alpha/r \"This scaling helps to reduce the need to retune hyperparameters when we vary r\" arxiv.org/pdf/2106.09685.pdf (section 4.1)</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>lora_dropout</code> <p>dropout that is applied on the input in the LoRA branch (before multiplying by matrix A)</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> Source code in <code>whittle/lora_model/lora_linear.py</code> <pre><code>def __init__(\n    self,\n    # \u2193 this part is for pretrained weights\n    in_features: int,\n    out_features: int,\n    # \u2193 the remaining part is for LoRA\n    r: int = 0,\n    lora_alpha: int = 1,\n    lora_dropout: float = 0.0,\n    **kwargs: Any,\n):\n    \"\"\"LoRA wrapper around linear class.\n\n    This class has three weight matrices:\n        1. Pretrained weights are stored as `self.linear.weight`\n        2. LoRA A matrix as `self.lora_A`\n        3. LoRA B matrix as `self.lora_B`\n    Only LoRA's A and B matrices are updated, pretrained weights stay frozen.\n\n    Args:\n        in_features: number of input features of the pretrained weights\n        out_features: number of output features of the pretrained weights\n        r: rank of the weight update matrices. To make sense of using LoRA the rank should be smaller than the rank of\n            the weights of the model. The rank can be as low as 1: https://arxiv.org/pdf/2106.09685.pdf (section 7.2)\n        lora_alpha: alpha is needed for scaling updates as alpha/r\n            \"This scaling helps to reduce the need to retune hyperparameters when we vary r\"\n            https://arxiv.org/pdf/2106.09685.pdf (section 4.1)\n        lora_dropout: dropout that is applied on the input in the LoRA branch (before multiplying by matrix A)\n    \"\"\"\n    # call super for both\n    super().__init__(r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout)\n\n    self.linear = Linear(in_features, out_features, **kwargs)\n\n    self.use_bias = self.linear.use_bias\n    self.in_features = in_features\n    self.out_features = out_features\n    self.sub_network_in_features = in_features\n    self.sub_network_out_features = out_features\n    self.merged: bool = False\n    # Actual trainable parameters\n    if r &gt; 0:\n        self.lora_A = nn.Parameter(torch.empty((r, in_features)))\n        self.lora_B = nn.Parameter(torch.empty((out_features, r)))\n        self.scaling = self.lora_alpha / self.r\n        self.reset_parameters()\n</code></pre>"},{"location":"api/whittle/lora_model/lora_linear/#whittle.lora_model.lora_linear.LoRALinear.get_lora_AB","title":"get_lora_AB","text":"<pre><code>get_lora_AB() -&gt; Tensor\n</code></pre> <p>Return merged lora_A and lora_B matrices with the same shape as the pretrained weights.</p> Source code in <code>whittle/lora_model/lora_linear.py</code> <pre><code>def get_lora_AB(self) -&gt; torch.Tensor:\n    \"\"\"Return merged lora_A and lora_B matrices with the same shape as the pretrained weights.\"\"\"\n    return (\n        self.lora_B[: self.sub_network_out_features, :]\n        @ self.lora_A[:, : self.sub_network_in_features]\n    ) * self.scaling\n</code></pre>"},{"location":"api/whittle/lora_model/lora_linear/#whittle.lora_model.lora_linear.LoRALinear.merge","title":"merge","text":"<pre><code>merge() -&gt; None\n</code></pre> <p>Merges the LoRA weights into the full-rank weights (W = W + delta_W).</p> Source code in <code>whittle/lora_model/lora_linear.py</code> <pre><code>def merge(self) -&gt; None:\n    \"\"\"Merges the LoRA weights into the full-rank weights (W = W + delta_W).\"\"\"\n    if self.r &gt; 0 and not self.merged:\n        pretrained_dtype = self.linear.weight.data.dtype\n        lora_data = self.get_lora_AB()\n        # if only the pretrained are in quantized form - dequantize, sum with LoRA and quantize the result\n        if pretrained_dtype == torch.uint8:\n            import bitsandbytes as bnb\n\n            weight = self.linear.weight\n            # dequantize the pretrained weights\n            weight_data = bnb.functional.dequantize_4bit(\n                weight.data, weight.quant_state\n            ).to(lora_data.dtype)\n            # add pretrained and LoRA weights\n            weight_data += lora_data\n            # assign updated weights and quantize by moving to CUDA device\n            self.linear.weight = bnb.nn.Params4bit(\n                weight_data, requires_grad=False, **weight.__dict__\n            )\n            self.linear.weight.cuda(weight.device)\n        else:\n            # self.linear might be on CPU and lora_data on CUDA\n            # the inplace add will preserve the dtype of linear.weight\n            self.linear.weight.data += lora_data.to(\n                device=self.linear.weight.data.device\n            )\n        self.merged = True\n</code></pre>"},{"location":"api/whittle/lora_model/lora_linear/#whittle.lora_model.lora_linear.LoRALinear.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset all the weights, even including pretrained ones.</p> Source code in <code>whittle/lora_model/lora_linear.py</code> <pre><code>def reset_parameters(self) -&gt; None:\n    \"\"\"Reset all the weights, even including pretrained ones.\"\"\"\n    if hasattr(self, \"lora_A\"):\n        # initialize A the same way as the default for nn.Linear and B to zero\n        # Wondering why 'a' is equal to math.sqrt(5)?: https://github.com/pytorch/pytorch/issues/15314\n        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n        nn.init.zeros_(self.lora_B)\n</code></pre>"},{"location":"api/whittle/lora_model/lora_linear/#whittle.lora_model.lora_linear.LoRALinearProj","title":"LoRALinearProj","text":"<pre><code>LoRALinearProj(\n    in_features: int,\n    out_features: int,\n    r: int = 0,\n    lora_alpha: int = 1,\n    lora_dropout: float = 0.0,\n    **kwargs: Any\n)\n</code></pre> <p>               Bases: <code>LoRALayer</code></p> Source code in <code>whittle/lora_model/lora_linear.py</code> <pre><code>def __init__(\n    self,\n    in_features: int,\n    out_features: int,\n    r: int = 0,\n    lora_alpha: int = 1,\n    lora_dropout: float = 0.0,\n    **kwargs: Any,\n):\n    super().__init__(r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout)\n    self.linear = LinearProj(in_features, out_features, **kwargs)\n    self.use_bias = self.linear.use_bias\n    self.in_features = in_features\n    self.out_features = out_features\n    self.sub_network_in_features = in_features\n    self.sub_network_out_features = out_features\n    self.proj_indices = None\n    self.merged = False\n    # Actual trainable parameters\n    if r &gt; 0:\n        self.lora_A = nn.Parameter(torch.empty((r, in_features)))\n        self.lora_B = nn.Parameter(torch.empty((out_features, r)))\n        self.scaling = self.lora_alpha / self.r\n        self.reset_parameters()\n</code></pre>"},{"location":"api/whittle/lora_model/lora_linear/#whittle.lora_model.lora_linear.LoRALinearProj.get_lora_AB","title":"get_lora_AB","text":"<pre><code>get_lora_AB() -&gt; Tensor\n</code></pre> <p>Return merged lora_A and lora_B matrices with the same shape as the pretrained weights.</p> Source code in <code>whittle/lora_model/lora_linear.py</code> <pre><code>def get_lora_AB(self) -&gt; torch.Tensor:\n    \"\"\"Return merged lora_A and lora_B matrices with the same shape as the pretrained weights.\"\"\"\n    return (\n        self.lora_B[: self.sub_network_out_features, :]\n        @ self.lora_A[:, : self.sub_network_in_features]\n    ) * self.scaling\n</code></pre>"},{"location":"api/whittle/lora_model/lora_linear/#whittle.lora_model.lora_linear.LoRALinearProj.merge","title":"merge","text":"<pre><code>merge() -&gt; None\n</code></pre> <p>Merges the LoRA weights into the full-rank weights (W = W + delta_W).</p> Source code in <code>whittle/lora_model/lora_linear.py</code> <pre><code>def merge(self) -&gt; None:\n    \"\"\"Merges the LoRA weights into the full-rank weights (W = W + delta_W).\"\"\"\n    if self.r &gt; 0 and not self.merged:\n        pretrained_dtype = self.linear.weight.data.dtype\n        lora_data = self.get_lora_AB()\n        # if only the pretrained are in quantized form - dequantize, sum with LoRA and quantize the result\n        if pretrained_dtype == torch.uint8:\n            import bitsandbytes as bnb\n\n            weight = self.linear.weight\n            # dequantize the pretrained weights\n            weight_data = bnb.functional.dequantize_4bit(\n                weight.data, weight.quant_state\n            ).to(lora_data.dtype)\n            # add pretrained and LoRA weights\n            weight_data += lora_data\n            # assign updated weights and quantize by moving to CUDA device\n            self.linear.weight = bnb.nn.Params4bit(\n                weight_data, requires_grad=False, **weight.__dict__\n            )\n            self.linear.weight.cuda(weight.device)\n        else:\n            # self.linear might be on CPU and lora_data on CUDA\n            # the inplace add will preserve the dtype of linear.weight\n            self.linear.weight.data += lora_data.to(\n                device=self.linear.weight.data.device\n            )\n        self.merged = True\n</code></pre>"},{"location":"api/whittle/lora_model/lora_linear/#whittle.lora_model.lora_linear.LoRALinearProj.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset all the weights, even including pretrained ones.</p> Source code in <code>whittle/lora_model/lora_linear.py</code> <pre><code>def reset_parameters(self) -&gt; None:\n    \"\"\"Reset all the weights, even including pretrained ones.\"\"\"\n    if hasattr(self, \"lora_A\"):\n        # initialize A the same way as the default for nn.Linear and B to zero\n        # Wondering why 'a' is equal to math.sqrt(5)?: https://github.com/pytorch/pytorch/issues/15314\n        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n        nn.init.zeros_(self.lora_B)\n</code></pre>"},{"location":"api/whittle/lora_model/lora_mlps/","title":"Lora mlps","text":""},{"location":"api/whittle/lora_model/lora_mlps/#whittle.lora_model.lora_mlps","title":"whittle.lora_model.lora_mlps","text":""},{"location":"api/whittle/lora_model/lora_mlps/#whittle.lora_model.lora_mlps.LoRAGptNeoxMLP","title":"LoRAGptNeoxMLP","text":"<pre><code>LoRAGptNeoxMLP(config: Config)\n</code></pre> <p>               Bases: <code>GptNeoxMLP</code></p> Source code in <code>whittle/lora_model/lora_mlps.py</code> <pre><code>def __init__(self, config: Config) -&gt; None:\n    super().__init__(config)\n    self.in_features = config.n_embd\n    self.intermediate_size = config.intermediate_size\n    self.fc = LoRALinear(\n        config.n_embd,\n        config.intermediate_size,\n        bias=config.bias,\n        r=(config.lora_r if config.lora_mlp else 0),\n        lora_alpha=config.lora_alpha,\n        lora_dropout=config.lora_dropout,\n    )\n    self.proj = LoRALinear(\n        config.intermediate_size,\n        config.n_embd,\n        bias=config.bias,\n        r=(config.lora_r if config.lora_mlp else 0),\n        lora_alpha=config.lora_alpha,\n        lora_dropout=config.lora_dropout,\n    )\n\n    self.config = config\n</code></pre>"},{"location":"api/whittle/lora_model/lora_mlps/#whittle.lora_model.lora_mlps.LoRAGptNeoxMLP.reset_super_network","title":"reset_super_network","text":"<pre><code>reset_super_network()\n</code></pre> <p>Resets the MLP dimensions to the original super-network dimensionality.</p> Source code in <code>whittle/models/gpt/blocks/mlp.py</code> <pre><code>def reset_super_network(self):\n    \"\"\"Resets the MLP dimensions to the original super-network dimensionality.\"\"\"\n    self.sub_network_n_embd = self.in_features\n    self.sub_network_intermediate_size = self.intermediate_size\n\n    self.fc.reset_super_network()\n    self.proj.reset_super_network()\n</code></pre>"},{"location":"api/whittle/lora_model/lora_mlps/#whittle.lora_model.lora_mlps.LoRAGptNeoxMLP.set_sub_network","title":"set_sub_network","text":"<pre><code>set_sub_network(\n    sub_network_n_embd: int,\n    sub_network_intermediate_size: int,\n)\n</code></pre> <p>Sets the dimensionality of the current sub-network MLP layers.</p> PARAMETER DESCRIPTION <code>sub_network_n_embd</code> <p>Input and output embedding dimension of the sub-network.</p> <p> TYPE: <code>int</code> </p> <code>sub_network_intermediate_size</code> <p>Hidden layer dimension of the sub-network MLP.</p> <p> TYPE: <code>int</code> </p> Source code in <code>whittle/models/gpt/blocks/mlp.py</code> <pre><code>def set_sub_network(\n    self, sub_network_n_embd: int, sub_network_intermediate_size: int\n):\n    \"\"\"\n    Sets the dimensionality of the current sub-network MLP layers.\n\n    Args:\n       sub_network_n_embd: Input and output embedding dimension of the sub-network.\n       sub_network_intermediate_size: Hidden layer dimension of the sub-network MLP.\n    \"\"\"\n    self.sub_network_n_embd = sub_network_n_embd\n    self.sub_network_intermediate_size = sub_network_intermediate_size\n\n    self.fc.set_sub_network(\n        self.sub_network_n_embd, self.sub_network_intermediate_size\n    )\n    self.proj.set_sub_network(\n        self.sub_network_intermediate_size, self.sub_network_n_embd\n    )\n</code></pre>"},{"location":"api/whittle/lora_model/lora_mlps/#whittle.lora_model.lora_mlps.LoRALLaMAMLP","title":"LoRALLaMAMLP","text":"<pre><code>LoRALLaMAMLP(config: Config)\n</code></pre> <p>               Bases: <code>LLaMAMLP</code></p> Source code in <code>whittle/lora_model/lora_mlps.py</code> <pre><code>def __init__(self, config: Config) -&gt; None:\n    super().__init__(config)\n    self.in_features = config.n_embd\n    self.intermediate_size = config.intermediate_size\n    self.fc_1 = LoRALinear(\n        config.n_embd,\n        config.intermediate_size,\n        bias=config.bias,\n        r=(config.lora_r if config.lora_mlp else 0),\n        lora_alpha=config.lora_alpha,\n        lora_dropout=config.lora_dropout,\n    )\n    self.fc_2 = LoRALinear(\n        config.n_embd,\n        config.intermediate_size,\n        bias=config.bias,\n        r=(config.lora_r if config.lora_mlp else 0),\n        lora_alpha=config.lora_alpha,\n        lora_dropout=config.lora_dropout,\n    )\n    self.proj = LoRALinear(\n        config.intermediate_size,\n        config.n_embd,\n        bias=config.bias,\n        r=(config.lora_r if config.lora_mlp else 0),\n        lora_alpha=config.lora_alpha,\n        lora_dropout=config.lora_dropout,\n    )\n\n    self.config = config\n</code></pre>"},{"location":"api/whittle/lora_model/lora_mlps/#whittle.lora_model.lora_mlps.LoRALLaMAMLP.reset_super_network","title":"reset_super_network","text":"<pre><code>reset_super_network()\n</code></pre> <p>Reset the input dimensionality of the current sub-network to the super-network dimensionality.</p> Source code in <code>whittle/models/gpt/blocks/mlp.py</code> <pre><code>def reset_super_network(self):\n    \"\"\"Reset the input dimensionality of the current sub-network to the super-network dimensionality.\"\"\"\n    self.sub_network_n_embd = self.in_features\n    self.sub_network_intermediate_size = self.intermediate_size\n\n    self.fc_1.reset_super_network()\n    self.fc_2.reset_super_network()\n    self.proj.reset_super_network()\n</code></pre>"},{"location":"api/whittle/lora_model/lora_mlps/#whittle.lora_model.lora_mlps.LoRALLaMAMLP.set_sub_network","title":"set_sub_network","text":"<pre><code>set_sub_network(\n    sub_network_n_embd: int,\n    sub_network_intermediate_size: int,\n)\n</code></pre> <p>Sets the dimensionality of the current sub-network MLP layers.</p> PARAMETER DESCRIPTION <code>sub_network_n_embd</code> <p>Input and output embedding dimension of the sub-network.</p> <p> TYPE: <code>int</code> </p> <code>sub_network_intermediate_size</code> <p>Hidden layer dimension of the sub-network MLP.</p> <p> TYPE: <code>int</code> </p> Source code in <code>whittle/models/gpt/blocks/mlp.py</code> <pre><code>def set_sub_network(\n    self, sub_network_n_embd: int, sub_network_intermediate_size: int\n):\n    \"\"\"\n    Sets the dimensionality of the current sub-network MLP layers.\n\n    Args:\n        sub_network_n_embd: Input and output embedding dimension of the sub-network.\n        sub_network_intermediate_size: Hidden layer dimension of the sub-network MLP.\n    \"\"\"\n    self.sub_network_n_embd = sub_network_n_embd\n    self.sub_network_intermediate_size = sub_network_intermediate_size\n\n    self.fc_1.set_sub_network(\n        self.sub_network_n_embd, self.sub_network_intermediate_size\n    )\n    self.fc_2.set_sub_network(\n        self.sub_network_n_embd, self.sub_network_intermediate_size\n    )\n    self.proj.set_sub_network(\n        self.sub_network_intermediate_size, self.sub_network_n_embd\n    )\n</code></pre>"},{"location":"api/whittle/lora_model/lora_qkv_linear/","title":"Lora qkv linear","text":""},{"location":"api/whittle/lora_model/lora_qkv_linear/#whittle.lora_model.lora_qkv_linear","title":"whittle.lora_model.lora_qkv_linear","text":""},{"location":"api/whittle/lora_model/lora_qkv_linear/#whittle.lora_model.lora_qkv_linear.LoRAQKVLinear","title":"LoRAQKVLinear","text":"<pre><code>LoRAQKVLinear(\n    config,\n    in_features: int,\n    out_features: int,\n    head_size: int,\n    n_head: int,\n    n_query_groups: int,\n    fix_head_size: bool,\n    r: int = 0,\n    lora_alpha: int = 1,\n    lora_dropout: float = 0.0,\n    enable_lora: bool | tuple[bool, bool, bool] = False,\n    **kwargs: Any\n)\n</code></pre> <p>               Bases: <code>LoRALayer</code></p> Source code in <code>whittle/lora_model/lora_qkv_linear.py</code> <pre><code>def __init__(\n    self,\n    # \u2193 this part is for pretrained weights\n    config,\n    in_features: int,\n    out_features: int,\n    # \u2193 the remaining part is for LoRA\n    head_size: int,\n    n_head: int,\n    n_query_groups: int,\n    fix_head_size: bool,\n    r: int = 0,\n    lora_alpha: int = 1,\n    lora_dropout: float = 0.0,\n    enable_lora: bool | tuple[bool, bool, bool] = False,\n    **kwargs: Any,\n):\n    super().__init__(r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout)\n    self.config = config\n    assert out_features == (n_head + 2 * n_query_groups) * head_size\n    self.linear = LinearQKV(in_features, out_features, **kwargs)\n    self.use_bias = self.linear.use_bias\n    self.head_size = head_size\n    self.fix_head_size = fix_head_size\n    self.n_head = n_head\n    self.in_features = in_features\n    self.out_features = out_features\n    self.n_query_groups = n_query_groups\n    if isinstance(enable_lora, bool):\n        enable_lora = (enable_lora, enable_lora, enable_lora)\n    assert len(enable_lora) == 3\n    self.enable_lora = enable_lora\n    self.sub_network_in_features = in_features\n    self.sub_network_out_features = out_features\n    self.sub_network_head_size = head_size\n    self.sub_network_n_head = n_head\n    self.sub_network_query_groups = n_query_groups\n    self.q_per_kv = n_head // n_query_groups\n    self.sub_network_q_per_kv = self.q_per_kv\n    self.qkv_indices = None\n    self.merged = False\n    # Actual trainable parameters\n    # To better understand initialization let's imagine that we have such parameters:\n    # \u26ac in_features: 128 (embeddings_size)\n    # \u26ac out_features: 384 (3 * embedding_size)\n    # \u26ac r: 2\n    # \u26ac enable_lora: [True, False, True]\n    if r &gt; 0 and any(enable_lora):\n        self.lora_A = nn.Parameter(\n            torch.empty((r * sum(enable_lora), in_features))\n        )  # (4, 128)\n        self.enable_q, self.enable_k, self.enable_v = enable_lora\n        # qkv_shapes will be used to split a tensor with weights correctly\n        qkv_shapes = (\n            # if `head_size` is explicitly specified in the config, `n_embd` (or `in_features`)\n            # might not be equal to `head_size * n_head`, thus we use it directly here\n            self.sub_network_head_size\n            * self.sub_network_query_groups\n            * self.sub_network_q_per_kv,\n            head_size * n_query_groups,\n            head_size * n_query_groups,\n        )\n        self.qkv_shapes = [s for s in qkv_shapes if s]\n        self.lora_B = nn.Parameter(torch.empty(sum(self.qkv_shapes), r))  # (256, 2))\n        # Notes about shapes above\n        # - self.lora_A has shape (4, 128): 4 because rank is 2 and LoRA is applied only to two matrices;\n        # 128 is the input size of the x (embedding size). (4, 128) and not (128, 4) because later on in\n        # F.linear function weights are automatically transposed. In addition conv1d requires channels to\n        # be before seq length\n        # - self.lora_B has shape (256, 2): 256 because LoRA is applied only to two matrices, so the output is\n        # 128*2; 2 tells to have two channels per group for group convolution\n\n        # Scaling:\n        # This balances the pretrained model`s knowledge and the new task-specific adaptation\n        # https://lightning.ai/pages/community/tutorial/lora-llm/\n        # So, set alpha to 1.0 to fully add LoRA. If the LoRA seems to have too much effect (i.e., overfitted), set\n        # alpha to lower value. If the LoRA seems to have too little effect, set alpha to higher than 1.0. You can\n        # tune these values to your needs. This value can be even slightly greater than 1.0!\n        # https://github.com/cloneofsimo/lora\n        self.scaling = self.lora_alpha / self.r\n\n        self.reset_parameters()\n</code></pre>"},{"location":"api/whittle/lora_model/lora_qkv_linear/#whittle.lora_model.lora_qkv_linear.LoRAQKVLinear.conv1d","title":"conv1d","text":"<pre><code>conv1d(input: Tensor, weight: Tensor) -&gt; Tensor\n</code></pre> <p>An extension of the <code>torch.nn.functional.conv1d</code> function with a logic specific to grouped queries.</p> <p>Lora in litgpt is applied separately to keys, queries and values. Because of sub-network selection, we need flexible indexing to select parts of the lora_B matrix that correspond to active queries, keys and values. Since QKV are interleaved (i.e. QQKV QQKV ... QQKV), we need to select them in <code>_set_lora_ind</code>, and then call <code>_split_lora_B</code> to get the corresponding parts of the weight matrix. Then, we apply each part of the weight matrix to the corresponding input's part and concatenate the result.</p> <p>Compared to litgpt, we don't use <code>groups</code> in case the number of heads is equal to the number of query groups (grouped queries are disabled). We still need the more complex indexing because of sub-network selection.</p> PARAMETER DESCRIPTION <code>input</code> <p>input matrix of shape (B, C, T)</p> <p> TYPE: <code>Tensor</code> </p> <code>weight</code> <p>weight matrix of shape (C_output, rank, 1). \"C_output\" is defined as a sum of embedding sizes for each enabled LoRA layer (see init method of the class).</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A tensor with a shape (B, C_output, T)</p> Source code in <code>whittle/lora_model/lora_qkv_linear.py</code> <pre><code>def conv1d(self, input: torch.Tensor, weight: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"An extension of the `torch.nn.functional.conv1d` function with a logic specific to grouped queries.\n\n    Lora in litgpt is applied separately to keys, queries and values. Because of sub-network selection,\n    we need flexible indexing to select parts of the lora_B matrix that correspond to active queries, keys and values.\n    Since QKV are interleaved (i.e. QQKV QQKV ... QQKV), we need to select them in `_set_lora_ind`, and then call `_split_lora_B`\n    to get the corresponding parts of the weight matrix. Then, we apply each part of the weight matrix to the\n    corresponding input's part and concatenate the result.\n\n    Compared to litgpt, we don't use `groups` in case the number of heads is equal to the number of query groups\n    (grouped queries are disabled). We still need the more complex indexing because of sub-network selection.\n\n    Args:\n        input: input matrix of shape (B, C, T)\n        weight: weight matrix of shape (C_output, rank, 1).\n            \"C_output\" is defined as a sum of embedding sizes for each enabled LoRA layer (see init method of the class).\n\n    Returns:\n        A tensor with a shape (B, C_output, T)\n\n    \"\"\"\n\n    # Notation:\n    # \u26ac N: number of enabled LoRA layers (self.enable_lora)\n    # \u26ac C_output': embeddings size for each LoRA layer (not equal in size)\n    # \u26ac r: rank of all LoRA layers (equal in size)\n\n    # Split input tensor along the specified dimension\n    input_splitted = input.chunk(sum(self.enable_lora), dim=1)  # N * (B, C // N, T)\n\n    # Filter out empty indices and get corresponding weights\n    active_inds = filter(\n        lambda ind: len(ind) &gt; 0, [self.q_ind, self.k_ind, self.v_ind]\n    )\n\n    # Use indexing directly without `.data` and unsqueeze in the same operation\n    weight_splitted = [weight[ind].unsqueeze(-1) for ind in active_inds]\n\n    # Perform convolution in a single pass with a list comprehension\n    return [F.conv1d(inp, w) for inp, w in zip(input_splitted, weight_splitted)]\n</code></pre>"},{"location":"api/whittle/lora_model/lora_qkv_linear/#whittle.lora_model.lora_qkv_linear.LoRAQKVLinear.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> <p>Do the forward pass.</p> <p>If LoRA's weights are merged with pretrained ones then it's a simple matrix multiplication. If not, then multiply pretrained weights with input, apply LoRA on input and do summation.</p> PARAMETER DESCRIPTION <code>x</code> <p>input tensor of shape (batch_size, context_length, embedding_size)</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Output tensor of shape (batch_size, context_length, 3 * embedding_size)</p> Source code in <code>whittle/lora_model/lora_qkv_linear.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Do the forward pass.\n\n    If LoRA's weights are merged with pretrained ones then it's a simple matrix multiplication.\n    If not, then multiply pretrained weights with input, apply LoRA on input and do summation.\n\n    Args:\n        x: input tensor of shape (batch_size, context_length, embedding_size)\n\n    Returns:\n        Output tensor of shape (batch_size, context_length, 3 * embedding_size)\n    \"\"\"\n\n    # Let's assume that:\n    # \u26ac x: (64, 64, 128) or (batch_size, context_length, embedding_size)\n    # \u26ac self.linear.weight: (384, 128) or (3 * embedding_size, embedding_size)\n    # \u26ac self.lora_A.data: (4, 128)\n    # \u26ac self.lora_B.data: (256, 2)\n\n    # if weights are merged or LoRA is disabled (r &lt;= 0 or all `enable_lora` are False) - it's only a regular nn.Linear forward pass;\n    # otherwise in addition do the forward pass with LoRA weights and add it's output to the output from pretrained weights\n    pretrained = self.linear(x)\n    if self.r == 0 or not any(self.enable_lora) or self.merged:\n        return pretrained\n    after_A = F.linear(\n        self.lora_dropout(x), self.lora_A[:, : self.sub_network_in_features]\n    )  # (64, 64, 128) @ (4, 128) -&gt; (64, 64, 4)\n    # For F.conv1d:\n    # \u26ac input: input tensor of shape (mini-batch, in_channels, iW)\n    # \u26ac weight: filters of shape (out_channels, in_channels/groups, kW)\n\n    # changes compared to litgpt - we need more flexible indexing because of sub-network selection\n    after_B = self.conv1d(\n        after_A.transpose(-2, -1),  # (64, 64, 4) -&gt; (64, 4, 64)\n        self.lora_B,  # (256, 2) -&gt; (256, 2, 1) -&gt; split to (q, 2, 1), (k, 2, 1), (v, 2, 1)\n    )  # (64, 4, 64) @ (256, 2, 1) -&gt; (64, 256, 64)\n\n    lora = self.zero_pad(\n        [a.transpose(-2, -1) * self.scaling for a in after_B]\n    )  # (64, 64, 256) after zero_pad (64, 64, 384)\n    return pretrained + lora\n</code></pre>"},{"location":"api/whittle/lora_model/lora_qkv_linear/#whittle.lora_model.lora_qkv_linear.LoRAQKVLinear.get_lora_AB","title":"get_lora_AB","text":"<pre><code>get_lora_AB() -&gt; Tensor\n</code></pre> <p>Return merged lora_A and lora_B matrices with the same shape as the pretrained weights.</p> Source code in <code>whittle/lora_model/lora_qkv_linear.py</code> <pre><code>def get_lora_AB(self) -&gt; torch.Tensor:\n    \"\"\"Return merged lora_A and lora_B matrices with the same shape as the pretrained weights.\"\"\"\n    # Let's assume that:\n    # \u26ac self.linear.weight.data: (384, 128) or (3 * embedding_size, embedding_size)\n    # \u26ac self.lora_A.data: (4, 128)\n    # \u26ac self.lora_B.data: (256, 2)\n    lora = self.conv1d(\n        self.lora_A[:, : self.sub_network_in_features].data.unsqueeze(\n            0\n        ),  # (4, 128) -&gt; (1, 4, 128)\n        self.lora_B,  # (256, 2) -&gt; (256, 2, 1) -&gt; splitted (inside conv1d)\n    )  # (1, 4, 128) @ (256, 2, 1) -&gt; (1, 256, 128) -&gt; (256, 128)\n    return self.zero_pad(\n        [lora_w.squeeze(0).T * self.scaling for lora_w in lora.T]\n    ).T  # (256, 128) after zero_pad (384, 128)\n</code></pre>"},{"location":"api/whittle/lora_model/lora_qkv_linear/#whittle.lora_model.lora_qkv_linear.LoRAQKVLinear.merge","title":"merge","text":"<pre><code>merge() -&gt; None\n</code></pre> <p>Merges the LoRA weights into the full-rank weights (W = W + delta_W).</p> Source code in <code>whittle/lora_model/lora_qkv_linear.py</code> <pre><code>def merge(self) -&gt; None:\n    \"\"\"Merges the LoRA weights into the full-rank weights (W = W + delta_W).\"\"\"\n    if self.r &gt; 0 and any(self.enable_lora) and not self.merged:\n        pretrained_dtype = self.linear.weight.data.dtype\n        lora_data = self.get_lora_AB()\n        # if only the pretrained are in quantized form - dequantize, sum with LoRA and quantize the result\n        if pretrained_dtype == torch.uint8:\n            import bitsandbytes as bnb\n\n            weight = self.linear.weight\n            # dequantize the pretrained weights\n            weight_data = bnb.functional.dequantize_4bit(\n                weight.data, weight.quant_state\n            ).to(lora_data.dtype)\n            # add pretrained and LoRA weights\n            weight_data += lora_data\n            # assign updated weights and quantize by moving to CUDA device\n            self.linear.weight = bnb.nn.Params4bit(\n                weight_data, requires_grad=False, **weight.__dict__\n            )\n            self.linear.weight.cuda(weight.device)\n        else:\n            # self.linear might be on CPU and lora_data on CUDA\n            # the inplace add will preserve the dtype of linear.weight\n            self.linear.weight.data += lora_data.to(\n                device=self.linear.weight.data.device\n            )\n        self.merged = True\n</code></pre>"},{"location":"api/whittle/lora_model/lora_qkv_linear/#whittle.lora_model.lora_qkv_linear.LoRAQKVLinear.reset_parameters","title":"reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset all the weights, even including pretrained ones.</p> Source code in <code>whittle/lora_model/lora_qkv_linear.py</code> <pre><code>def reset_parameters(self) -&gt; None:\n    \"\"\"Reset all the weights, even including pretrained ones.\"\"\"\n    if hasattr(self, \"lora_A\"):\n        # initialize A the same way as the default for nn.Linear and B to zero\n        # Wondering why 'a' is equal to math.sqrt(5)?: https://github.com/pytorch/pytorch/issues/15314\n        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n        nn.init.zeros_(self.lora_B)\n</code></pre>"},{"location":"api/whittle/lora_model/lora_qkv_linear/#whittle.lora_model.lora_qkv_linear.LoRAQKVLinear.reset_super_network","title":"reset_super_network","text":"<pre><code>reset_super_network()\n</code></pre> <p>Resets the dimensionality of the current sub-network to the super-network dimensionality.</p> Source code in <code>whittle/lora_model/lora_qkv_linear.py</code> <pre><code>def reset_super_network(self):\n    \"\"\"Resets the dimensionality of the current sub-network to the super-network dimensionality.\"\"\"\n    self.sub_network_in_features = self.in_features\n    self.sub_network_out_features = self.out_features\n    self.sub_network_n_embd = self.config.n_embd\n    self.sub_network_n_head = self.config.n_head\n    self.q_per_kv = self.config.n_head // self.config.n_query_groups\n    self.sub_network_head_size = self.config.head_size\n    self.sub_network_qkv_shape = (\n        self.config.n_head + 2 * self.config.n_query_groups\n    ) * self.config.head_size\n    self.sub_network_query_groups = self.config.n_query_groups\n    self.sub_network_q_per_kv = self.q_per_kv\n\n    self.linear.reset_super_network()\n    self.sub_attention_scaler = self.config.attention_scores_scalar\n    self.qkv_indices = None\n\n    # trigger resetting the indices for LoRA\n    self._set_lora_ind()\n</code></pre>"},{"location":"api/whittle/lora_model/lora_qkv_linear/#whittle.lora_model.lora_qkv_linear.LoRAQKVLinear.zero_pad","title":"zero_pad","text":"<pre><code>zero_pad(x: list[Tensor]) -&gt; Tensor\n</code></pre> <p>Properly pad the last dimension of weight updates with zeros.</p> <p>If, based on <code>self.enable_lora</code>, we want to fine-tune queries and values, but not keys, then the weights update should be:</p> <p>[[\u0394W,\u0394W,\u0394W, ..., 0,0,0, ..., \u0394W,\u0394W,\u0394W,],  [....................................],  [\u0394W,\u0394W,\u0394W, ..., 0,0,0, ..., \u0394W,\u0394W,\u0394W,]]     \u2191              \u2191            \u2191</p>"},{"location":"api/whittle/lora_model/lora_qkv_linear/#whittle.lora_model.lora_qkv_linear.LoRAQKVLinear.zero_pad--query-key-value","title":"| query         | key       | value    |","text":"<p>For Llama2's GQA support, Q, K, and V weights are interleaved, so that weights for grouped queries are adjacent to their associated key and value weights. For example, suppose we have n_head = 12 with 3 query groups. Then along the embedding dimension the interleaved weights would look like</p> <p>[Q, Q, Q, Q, K, V, Q, Q, Q, Q, K, V, Q, Q, Q, Q, K, V],</p> <p>where each Q, K, and V has size head_size.</p> <p>In this case, the previously-described weight update applies separately to each individual block, so the update will take the form</p> <p>[[\u0394W,\u0394W,\u0394W, ..., 0,0,0, ..., \u0394W,\u0394W,\u0394W, \u0394W,\u0394W,\u0394W, ..., 0,0,0, ..., \u0394W,\u0394W,\u0394W, ...],  [.............................................................................],  [\u0394W,\u0394W,\u0394W, ..., 0,0,0, ..., \u0394W,\u0394W,\u0394W, \u0394W,\u0394W,\u0394W, ..., 0,0,0, ..., \u0394W,\u0394W,\u0394W, ...]]      \u2191              \u2191            \u2191        \u2191             \u2191            \u2191</p>"},{"location":"api/whittle/lora_model/lora_qkv_linear/#whittle.lora_model.lora_qkv_linear.LoRAQKVLinear.zero_pad--q-block-1-k-block-1-v-block-1-q-block-2-k-block-2-v-block-2","title":"| q block 1 | k block 1  | v block 1 | q block 2 |  k block 2 |  v block 2 | ...","text":"<p>Note that in the above diagram, the size of each q block will equal q_per_kv times the size of each k and v block.</p> PARAMETER DESCRIPTION <code>x</code> <p>tensor with weights update that will be padded with zeros if necessary</p> <p> TYPE: <code>list[Tensor]</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A tensor with weight updates and zeros for deselected q, k or v</p> Source code in <code>whittle/lora_model/lora_qkv_linear.py</code> <pre><code>def zero_pad(self, x: list[torch.Tensor]) -&gt; torch.Tensor:\n    \"\"\"Properly pad the last dimension of weight updates with zeros.\n\n    If, based on `self.enable_lora`, we want to fine-tune queries and values, but not keys,\n    then the weights update should be:\n\n    [[\u0394W,\u0394W,\u0394W, ..., 0,0,0, ..., \u0394W,\u0394W,\u0394W,],\n     [....................................],\n     [\u0394W,\u0394W,\u0394W, ..., 0,0,0, ..., \u0394W,\u0394W,\u0394W,]]\n        \u2191              \u2191            \u2191\n    ________________________________________\n    | query         | key       | value    |\n    ----------------------------------------\n    For Llama2's GQA support, Q, K, and V weights are interleaved, so that weights for grouped\n    queries are adjacent to their associated key and value weights.\n    For example, suppose we have n_head = 12 with 3 query groups.\n    Then along the embedding dimension the interleaved weights would look like\n\n    [Q, Q, Q, Q, K, V, Q, Q, Q, Q, K, V, Q, Q, Q, Q, K, V],\n\n    where each Q, K, and V has size head_size.\n\n    In this case, the previously-described weight update applies separately to each\n    individual block, so the update will take the form\n\n    [[\u0394W,\u0394W,\u0394W, ..., 0,0,0, ..., \u0394W,\u0394W,\u0394W, \u0394W,\u0394W,\u0394W, ..., 0,0,0, ..., \u0394W,\u0394W,\u0394W, ...],\n     [.............................................................................],\n     [\u0394W,\u0394W,\u0394W, ..., 0,0,0, ..., \u0394W,\u0394W,\u0394W, \u0394W,\u0394W,\u0394W, ..., 0,0,0, ..., \u0394W,\u0394W,\u0394W, ...]]\n         \u2191              \u2191            \u2191        \u2191             \u2191            \u2191\n    ________________________________________________________________________________\n    | q block 1 | k block 1  | v block 1 | q block 2 |  k block 2 |  v block 2 | ...\n    --------------------------------------------------------------------------------\n    Note that in the above diagram, the size of each q block will equal q_per_kv\n    times the size of each k and v block.\n\n    Args:\n        x: tensor with weights update that will be padded with zeros if necessary\n\n    Returns:\n        A tensor with weight updates and zeros for deselected q, k or v\n    \"\"\"\n    # we need to scatter the indices every time because sub-network needs zero padding\n    # and super-network lora updates have to align with the other weights\n\n    # In litgpt, there's a bug where [Q1, K1, V1, Q2, K2, V2] += [\u0394Q1, \u0394Q1, \u0394K1, \u0394K2, \u0394KV1 \u0394V2]\n    # (this occurs for all versions that still support interleaving).\n    # For super-network training, we need this to be correct.\n\n    # Let's image that:\n    # \u26ac input x has shape (64, 64, 256): (batch_size, sequence_length, embeddings_size)\n    # \u26ac embeddings_size: 128\n    # \u26ac self.linear.out_features: 384 (3 * embeddings_size)\n    # \u26ac enable_lora: [True, False, True]\n    # Then x has embeddings_size of 256 (2 * 128 as enable_lora only for query and value, not keys) and expected\n    # embeddings_size is 384 (self.linear.out_features), so that means that we need to pad from 256 to 384 with zeros, but\n    # only for key updates (this is where self.lora_ind comes in handy)\n\n    # ensures the same device and shape as the weights\n    result = x[0].new_zeros(\n        *x[0].shape[:-1], self.sub_network_out_features\n    )  # (64, 64, 384)\n\n    active_inds = [self.q_target, self.k_target, self.v_target]\n    active_inds = [ind for ind in active_inds if len(ind) &gt; 0]\n\n    for ind, weight in zip(active_inds, x):\n        result = result.index_copy_(dim=-1, index=ind, source=weight)  # (64, 64, 384)\n\n    return result\n</code></pre>"},{"location":"api/whittle/lora_model/merge/","title":"Merge","text":""},{"location":"api/whittle/lora_model/merge/#whittle.lora_model.merge","title":"whittle.lora_model.merge","text":""},{"location":"api/whittle/lora_model/merge/#whittle.lora_model.merge.merge_lora","title":"merge_lora","text":"<pre><code>merge_lora(\n    sampling_strategy: str,\n    checkpoint_dir: Path,\n    pretrained_checkpoint_dir: str | None = None,\n    precision: str | None = None,\n) -&gt; None\n</code></pre> <p>Merges the LoRA weights with the base model.</p> <p>See <code>litgpt finetune lora</code>.</p> <p>Creates a new <code>lit_model.pth</code> file by merging the LoRA weights (<code>lit_model.pth.lora</code>) with the original checkpoint weights.</p> PARAMETER DESCRIPTION <code>checkpoint_dir</code> <p>Path to the checkpoint directory with trained LoRA weights, which is the output of <code>litgpt finetune lora</code>.</p> <p> TYPE: <code>Path</code> </p> <code>pretrained_checkpoint_dir</code> <p>Optional path to the checkpoint directory with the weights of the base model corresponding to the LoRA checkpoint. By default, this will automatically be inferred from the metadata in the given <code>checkpoint_dir</code> directory. Only set this if the base model's checkpoint directory has moved or was renamed.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>precision</code> <p>Optional precision setting to instantiate the model weights in. By default, this will automatically be inferred from the metadata in the given <code>checkpoint_dir</code> directory.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>whittle/lora_model/merge.py</code> <pre><code>def merge_lora(\n    sampling_strategy: str,\n    checkpoint_dir: Path,\n    pretrained_checkpoint_dir: str | None = None,\n    precision: str | None = None,\n) -&gt; None:\n    \"\"\"Merges the LoRA weights with the base model.\n\n    See ``litgpt finetune lora``.\n\n    Creates a new ``lit_model.pth`` file by merging the LoRA weights (``lit_model.pth.lora``)\n    with the original checkpoint weights.\n\n    Arguments:\n        checkpoint_dir: Path to the checkpoint directory with trained LoRA weights, which is the output of\n            ``litgpt finetune lora``.\n        pretrained_checkpoint_dir: Optional path to the checkpoint directory with the weights of the base model\n            corresponding to the LoRA checkpoint. By default, this will automatically be inferred from the metadata\n            in the given `checkpoint_dir` directory. Only set this if the base model's checkpoint directory\n            has moved or was renamed.\n        precision: Optional precision setting to instantiate the model weights in. By default, this will\n            automatically be inferred from the metadata in the given ``checkpoint_dir`` directory.\n    \"\"\"\n    raise NotImplementedError(\"Merging is currently not supported.\")\n</code></pre>"},{"location":"api/whittle/loss/kd_loss/","title":"Kd loss","text":""},{"location":"api/whittle/loss/kd_loss/#whittle.loss.kd_loss","title":"whittle.loss.kd_loss","text":""},{"location":"api/whittle/loss/kd_loss/#whittle.loss.kd_loss.DistillLoss","title":"DistillLoss","text":"<pre><code>DistillLoss(\n    alpha: float = 1.0,\n    beta: float = 0.0,\n    temperature: float = 1.0,\n    loss: str = \"forward_kld\",\n    weight_scheme: str = \"other\",\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Custom loss function for knowledge distillation</p> <p>This loss function combines the standard cross-entropy loss with the KL divergence between the soft targets produced by a teacher model and a student model.</p> ATTRIBUTE DESCRIPTION <code>alpha</code> <p>The weight factor for the hard target loss. Higher values give more               importance to the cross entropy loss between student logits and ground truth labels.</p> <p> TYPE: <code>float</code> </p> <code>beta</code> <p>The weight factor for the soft target loss. Higher values give more                 importance to the loss between student and teacher logits.</p> <p> TYPE: <code>float</code> </p> <code>temperature</code> <p>The temperature used for distillation. Higher temperatures                  produce softer probability distributions.</p> <p> TYPE: <code>float</code> </p> <code>loss</code> <p>The loss function to use for distillation.</p> <p> TYPE: <code>str</code> </p> <code>weight_scheme</code> <p>The weight scheme to use for the distillation loss.</p> <p> TYPE: <code>str</code> </p> PARAMETER DESCRIPTION <code>alpha</code> <p>The weight factor for the hard target loss. Default is 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>beta</code> <p>The weight factor for the soft target loss. Default is 0.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>temperature</code> <p>The temperature for distillation. Default is 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>loss</code> <p>The distillation loss function to use. Default is 'forward_kld'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'forward_kld'</code> </p> <code>weight_scheme</code> <p>The weight scheme to use. Default is 'other'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'other'</code> </p> Source code in <code>whittle/loss/kd_loss.py</code> <pre><code>def __init__(\n    self,\n    alpha: float = 1.0,\n    beta: float = 0.0,\n    temperature: float = 1.0,\n    loss: str = \"forward_kld\",\n    weight_scheme: str = \"other\",\n):\n    \"\"\"\n    Initializes the DistillLoss module.\n\n    Args:\n        alpha (float): The weight factor for the hard target loss. Default is 1.0.\n        beta (float): The weight factor for the soft target loss. Default is 0.0.\n        temperature (float): The temperature for distillation. Default is 1.0.\n        loss (str): The distillation loss function to use. Default is 'forward_kld'.\n        weight_scheme (str): The weight scheme to use. Default is 'other'.\n    \"\"\"\n    super().__init__()\n\n    self.alpha = alpha\n    self.beta = beta\n    self.temperature = temperature\n    self.distill_loss = loss\n    self.weight_scheme = weight_scheme\n</code></pre>"},{"location":"api/whittle/loss/kd_loss/#whittle.loss.kd_loss.DistillLoss.forward","title":"forward","text":"<pre><code>forward(logits, labels, teacher_logits) -&gt; Tensor\n</code></pre> <p>Compute the distillation loss.</p> <p>This method computes the loss as a weighted sum of the soft target loss (KL divergence between student and teacher logits) and the hard target loss (cross-entropy loss between student logits and ground truth labels).</p> PARAMETER DESCRIPTION <code>logits</code> <p>The logits output by the student model. Shape (batch_size, num_classes).</p> <p> TYPE: <code>Tensor</code> </p> <code>labels</code> <p>The ground truth labels. Shape (batch_size).</p> <p> TYPE: <code>Tensor</code> </p> <code>teacher_logits</code> <p>The logits output by the teacher model. Shape (batch_size, num_classes).</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>torch.Tensor: The combined loss.</p> Source code in <code>whittle/loss/kd_loss.py</code> <pre><code>def forward(self, logits, labels, teacher_logits) -&gt; nn.Tensor:\n    \"\"\"\n    Compute the distillation loss.\n\n    This method computes the loss as a weighted sum of the soft target loss (KL divergence\n    between student and teacher logits) and the hard target loss (cross-entropy loss\n    between student logits and ground truth labels).\n\n    Args:\n        logits (torch.Tensor): The logits output by the student model. Shape (batch_size, num_classes).\n        labels (torch.Tensor): The ground truth labels. Shape (batch_size).\n        teacher_logits (torch.Tensor): The logits output by the teacher model. Shape (batch_size, num_classes).\n\n    Returns:\n        torch.Tensor: The combined loss.\n    \"\"\"\n    soft_target_loss = 0\n    teacher_logits = teacher_logits.detach()\n\n    hard_target_loss = F.cross_entropy(logits, labels, reduction=\"mean\")\n\n    if self.distill_loss == \"forward_kld\":\n        soft_target_loss = forward_kl(logits, teacher_logits, self.temperature)\n\n    elif self.distill_loss == \"reverse_kld\":\n        soft_target_loss = reverse_kl(logits, teacher_logits, self.temperature)\n\n    elif self.distill_loss == \"symmetric_kld\":\n        soft_target_loss = symmetric_kl(logits, teacher_logits, self.temperature)\n\n    elif self.distill_loss == \"js_distance\":\n        soft_target_loss = js_distance(logits, teacher_logits, self.temperature)\n\n    elif self.distill_loss == \"simple_cross_entropy\":\n        soft_target_loss = simple_cross_entropy(\n            logits, teacher_logits, self.temperature\n        )\n\n    elif self.distill_loss == \"cosine_similarity\":\n        soft_target_loss = cosine_similarity(logits, teacher_logits, self.temperature)\n\n    elif self.distill_loss == \"l1_loss\":\n        soft_target_loss = l1_loss(logits, teacher_logits)\n\n    elif self.distill_loss == \"l2_loss\":\n        soft_target_loss = l2_loss(logits, teacher_logits)\n\n    elif self.distill_loss == \"mmd_loss\":\n        soft_target_loss = mmd_loss(logits, teacher_logits)\n\n    else:\n        raise ValueError(f\"Invalid distillation loss: {self.distill_loss}\")\n\n    if self.weight_scheme == \"default\":\n        coefficient1 = 1.0\n        if soft_target_loss == 0:\n            coefficient2 = 1.0\n        else:\n            coefficient2 = hard_target_loss / soft_target_loss\n    else:\n        coefficient1 = self.alpha\n        coefficient2 = self.beta\n\n    total_loss = coefficient1 * hard_target_loss + coefficient2 * soft_target_loss\n\n    return total_loss\n</code></pre>"},{"location":"api/whittle/loss/losses/","title":"Losses","text":""},{"location":"api/whittle/loss/losses/#whittle.loss.losses","title":"whittle.loss.losses","text":""},{"location":"api/whittle/loss/losses/#whittle.loss.losses.mmd_loss","title":"mmd_loss","text":"<pre><code>mmd_loss(x, y, kernel='rbf')\n</code></pre> <p>Maximum Mean Discrepancy (MMD) loss between distributions x and y. You can use different kernels: 'rbf', 'linear', etc.</p> <p>Args: x: Tensor of shape [batch_size, feature_dim] y: Tensor of shape [batch_size, feature_dim] kernel: The type of kernel to use, default is 'rbf' (Radial Basis Function)</p> <p>Returns: MMD loss</p> Source code in <code>whittle/loss/losses.py</code> <pre><code>def mmd_loss(x, y, kernel=\"rbf\"):\n    \"\"\"\n    Maximum Mean Discrepancy (MMD) loss between distributions x and y.\n    You can use different kernels: 'rbf', 'linear', etc.\n\n    Args:\n    x: Tensor of shape [batch_size, feature_dim]\n    y: Tensor of shape [batch_size, feature_dim]\n    kernel: The type of kernel to use, default is 'rbf' (Radial Basis Function)\n\n    Returns:\n    MMD loss\n    \"\"\"\n\n    def rbf_kernel(x, y, sigma=1.0):\n        \"\"\"Computes the RBF kernel between two tensors.\"\"\"\n        dist = torch.cdist(x, y, p=2)  # Compute pairwise Euclidean distance\n        return torch.exp(-(dist**2) / (2 * sigma**2))\n\n    def linear_kernel(x, y):\n        \"\"\"Computes the linear kernel between two tensors.\"\"\"\n        return torch.matmul(x, y.T)\n\n    # Select kernel\n    if kernel == \"rbf\":\n        Kxx = rbf_kernel(x, x)\n        Kyy = rbf_kernel(y, y)\n        Kxy = rbf_kernel(x, y)\n    elif kernel == \"linear\":\n        Kxx = linear_kernel(x, x)\n        Kyy = linear_kernel(y, y)\n        Kxy = linear_kernel(x, y)\n    else:\n        raise ValueError(f\"Unknown kernel type: {kernel}\")\n\n    # Compute MMD loss\n    mmd_loss_value = Kxx.mean() + Kyy.mean() - 2 * Kxy.mean()\n    return mmd_loss_value\n</code></pre>"},{"location":"api/whittle/metrics/flops/","title":"Flops","text":""},{"location":"api/whittle/metrics/flops/#whittle.metrics.flops","title":"whittle.metrics.flops","text":""},{"location":"api/whittle/metrics/flops/#whittle.metrics.flops.compute_flops","title":"compute_flops","text":"<pre><code>compute_flops(\n    model: GPT,\n    batch_size: int = 1,\n    sequence_length: int = 512,\n    device: str = \"cpu\",\n    previous_device: str | None = None,\n    verbose: bool = False,\n) -&gt; float\n</code></pre> <p>Estimates the number of floating-point operations (FLOPs) for a GPT model using PyTorch Lightning.</p> <p>This function uses Lightning's measure_flops utility to estimate the FLOPs of the model's forward pass on a specified device. The model will be temporarily moved to the given device if not already there. After profiling, it will be moved back to the previous device if specified.</p> PARAMETER DESCRIPTION <code>model</code> <p>The GPT model to profile.</p> <p> TYPE: <code>GPT</code> </p> <code>batch_size</code> <p>The batch size for the input tensor.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>sequence_length</code> <p>The sequence length for the input tensor.</p> <p> TYPE: <code>int</code> DEFAULT: <code>512</code> </p> <code>device</code> <p>The device on which to run the FLOPs calculation (\"cpu\", \"cuda\", etc.).</p> <p> TYPE: <code>str</code> DEFAULT: <code>'cpu'</code> </p> <code>previous_device</code> <p>Optional device to move the model back to after profiling.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>verbose</code> <p>If True, prints debug information about profiling.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The estimated number of floating-point operations (FLOPs) for the model's forward pass.</p> Source code in <code>whittle/metrics/flops.py</code> <pre><code>def compute_flops(\n    model: GPT,\n    batch_size: int = 1,\n    sequence_length: int = 512,\n    device: str = \"cpu\",\n    previous_device: str | None = None,\n    verbose: bool = False,\n) -&gt; float:\n    \"\"\"\n    Estimates the number of floating-point operations (FLOPs) for a GPT model using PyTorch Lightning.\n\n    This function uses Lightning's measure_flops utility to estimate the FLOPs of the model's forward pass\n    on a specified device. The model will be temporarily moved to the given device if not already there.\n    After profiling, it will be moved back to the previous device if specified.\n\n    Args:\n        model: The GPT model to profile.\n        batch_size: The batch size for the input tensor.\n        sequence_length: The sequence length for the input tensor.\n        device: The device on which to run the FLOPs calculation (\"cpu\", \"cuda\", etc.).\n        previous_device: Optional device to move the model back to after profiling.\n        verbose: If True, prints debug information about profiling.\n\n    Returns:\n        The estimated number of floating-point operations (FLOPs) for the model's forward pass.\n    \"\"\"\n    if device == \"cuda\" and not torch.cuda.is_available():\n        raise RuntimeError(\"CUDA requested but is not available.\")\n\n    original_device = next(model.parameters()).device\n    if original_device.type == \"meta\":\n        raise RuntimeError(\n            \"Model is on 'meta' device; cannot run FLOPs profiling without real weights.\"\n        )\n\n    if verbose:\n        print(f\"[FLOPs] Profiling on device: {device}\")\n        print(\n            f\"[FLOPs] Model: {model.__class__.__name__}, Batch size: {batch_size}, Seq length: {sequence_length}\"\n        )\n        print(f\"[FLOPs] Original device: {original_device}, Target device: {device}\")\n\n    if str(original_device) != device:\n        model.to(device)\n\n    input_tensor = torch.randint(\n        0, model.config.padded_vocab_size, (batch_size, sequence_length), device=device\n    )\n\n    def forward_fn():\n        return model(input_tensor)\n\n    try:\n        flops = measure_flops(model, forward_fn)\n    except Exception as e:\n        raise RuntimeError(f\"Failed to compute FLOPs: {e}\")\n\n    if previous_device is not None:\n        model.to(previous_device)\n    elif str(original_device) != device:\n        model.to(original_device)\n\n    if verbose:\n        print(f\"[FLOPs] Estimated: {flops:.2e}\")\n\n    return flops\n</code></pre>"},{"location":"api/whittle/metrics/latency/","title":"Latency","text":""},{"location":"api/whittle/metrics/latency/#whittle.metrics.latency","title":"whittle.metrics.latency","text":""},{"location":"api/whittle/metrics/latency/#whittle.metrics.latency.compute_latency","title":"compute_latency","text":"<pre><code>compute_latency(\n    model: Module,\n    use_cuda: bool = False,\n    batch_size: int = 8,\n    n_samples: int = 10,\n    device: str | None = None,\n) -&gt; float\n</code></pre> <p>Profiles the latency of a PyTorch model for inference.</p> <p>This function measures the average latency of the model's forward pass over a specified number of samples using PyTorch's profiler. It supports both CPU and CUDA profiling.</p> PARAMETER DESCRIPTION <code>model</code> <p>the LitGPT profiled.</p> <p> TYPE: <code>Module</code> </p> <code>use_cuda</code> <p>If True and CUDA is available, the model will be moved to the GPU for profiling. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>batch_size</code> <p>The batch size for the input tensor. Defaults to 8.</p> <p> TYPE: <code>int</code> DEFAULT: <code>8</code> </p> <code>n_samples</code> <p>The number of samples to profile after the warm-up phase. Defaults to 10.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>device</code> <p>The device to use for profiling. If None, the device is inferred based on use_cuda. Defaults to None.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The average inference time per sample in milliseconds.</p> <p> TYPE: <code>float</code> </p> Source code in <code>whittle/metrics/latency.py</code> <pre><code>def compute_latency(\n    model: torch.nn.Module,\n    use_cuda: bool = False,\n    batch_size: int = 8,\n    n_samples: int = 10,\n    device: str | None = None,\n) -&gt; float:\n    \"\"\"\n    Profiles the latency of a PyTorch model for inference.\n\n    This function measures the average latency of the model's forward pass over a specified number of samples\n    using PyTorch's profiler. It supports both CPU and CUDA profiling.\n\n    Args:\n        model (torch.nn.Module): the LitGPT profiled.\n        use_cuda (bool, optional): If True and CUDA is available, the model will be moved to the GPU for profiling. Defaults to False.\n        batch_size (int, optional): The batch size for the input tensor. Defaults to 8.\n        n_samples (int, optional): The number of samples to profile after the warm-up phase. Defaults to 10.\n        device (Optional[str], optional): The device to use for profiling. If None, the device is inferred based on use_cuda. Defaults to None.\n\n    Returns:\n        float: The average inference time per sample in milliseconds.\n    \"\"\"\n    input_tensor = torch.randint(\n        0, model.config.padded_vocab_size, (batch_size, model.max_seq_length)\n    )\n    if device is None:\n        if use_cuda and torch.cuda.is_available():\n            model = model.cuda()\n            input_tensor = input_tensor.cuda()\n    else:\n        model = model.to(device)\n        input_tensor = input_tensor.to(device)\n\n    # Use PyTorch profiler to record compute_latency\n    model.eval()\n    with torch.profiler.profile(\n        activities=[\n            torch.profiler.ProfilerActivity.CPU,\n            torch.profiler.ProfilerActivity.CUDA,\n        ],\n        schedule=torch.profiler.schedule(wait=1, warmup=10, active=n_samples),\n    ) as profiler:\n        # Actual forward pass inside the profiler\n        with record_function(\"model_inference\"):\n            for i in range(11 + n_samples):\n                _ = model(input_tensor)\n                profiler.step()\n\n    # Summing up CPU and CUDA times from the profiler using the correct methods\n    cuda_time_us, cpu_time_us = get_total_cpu_gpu_runtime(profiler)\n\n    # Convert time to milliseconds\n    total_time_ms = (cpu_time_us + cuda_time_us) / 1000\n    return total_time_ms / n_samples\n</code></pre>"},{"location":"api/whittle/metrics/latency/#whittle.metrics.latency.get_total_cpu_gpu_runtime","title":"get_total_cpu_gpu_runtime","text":"<pre><code>get_total_cpu_gpu_runtime(prof: profile) -&gt; tuple[int, int]\n</code></pre> <p>Calculates the total runtime for CPU and GPU (CUDA) from the profiler events.</p> <p>This function extracts and sums the self CPU and CUDA times from the PyTorch profiler events. It handles both legacy and Kineto profiler events for accurate CPU and CUDA profiling.</p> PARAMETER DESCRIPTION <code>prof</code> <p>A PyTorch profiler object containing profiling events.</p> <p> TYPE: <code>profile</code> </p> RETURNS DESCRIPTION <code>tuple[int, int]</code> <p>tuple[int, int]: A tuple where the first value is the total CPU time (in microseconds),                  and the second value is the total CUDA time (in microseconds).</p> Source code in <code>whittle/metrics/latency.py</code> <pre><code>def get_total_cpu_gpu_runtime(prof: torch.profiler.profile) -&gt; tuple[int, int]:\n    \"\"\"\n    Calculates the total runtime for CPU and GPU (CUDA) from the profiler events.\n\n    This function extracts and sums the self CPU and CUDA times from the PyTorch profiler events.\n    It handles both legacy and Kineto profiler events for accurate CPU and CUDA profiling.\n\n    Args:\n        prof (torch.profiler.profile): A PyTorch profiler object containing profiling events.\n\n    Returns:\n        tuple[int, int]: A tuple where the first value is the total CPU time (in microseconds),\n                         and the second value is the total CUDA time (in microseconds).\n    \"\"\"\n    events = prof.events()\n    sum_self_cpu_time_total = sum([event.self_cpu_time_total for event in events])\n\n    sum_self_cuda_time_total = 0\n    for evt in events:\n        if evt.device_type == torch.device(\"cpu\").type:\n            # In legacy profiler, kernel info is stored in CPU events\n            if evt.is_legacy:\n                sum_self_cuda_time_total += evt.self_cuda_time_total\n        elif evt.device_type == torch.device(\"cuda\").type:\n            # In Kineto profiler, there are events with the correct device type (e.g., CUDA)\n            sum_self_cuda_time_total += evt.self_cuda_time_total\n\n    return sum_self_cpu_time_total, sum_self_cuda_time_total\n</code></pre>"},{"location":"api/whittle/metrics/mag/","title":"Mag","text":""},{"location":"api/whittle/metrics/mag/#whittle.metrics.mag","title":"whittle.metrics.mag","text":""},{"location":"api/whittle/metrics/mag/#whittle.metrics.mag.compute_weight_magnitude","title":"compute_weight_magnitude","text":"<pre><code>compute_weight_magnitude(model: GPT) -&gt; float\n</code></pre> <p>Computes the sum of the weight magnitudes of the current sub-network of a GPT model. Make sure to set the sub-network before calling this function.</p> PARAMETER DESCRIPTION <code>model</code> <p>GPT model</p> <p> TYPE: <code>GPT</code> </p> RETURNS DESCRIPTION <code>float</code> <p>magnitude of the weights of the activated sub-network</p> <p> TYPE: <code>float</code> </p> Source code in <code>whittle/metrics/mag.py</code> <pre><code>def compute_weight_magnitude(model: GPT) -&gt; float:\n    \"\"\"\n    Computes the sum of the weight magnitudes of the current sub-network of a GPT model. Make sure to set the\n    sub-network before calling this function.\n\n    Args:\n        model: GPT model\n\n    Returns:\n        float: magnitude of the weights of the activated sub-network\n    \"\"\"\n    magnitude = 0\n    magnitude += compute_weight_magnitude_linear_layer(model.lm_head)\n    magnitude += compute_weight_magnitude_embedding(model.transformer.wte)\n    for i in range(model.sub_network_n_layers):\n        block = model.transformer.h[i]\n        magnitude += compute_weight_magnitude_layer_norm(block.norm_1)\n        magnitude += compute_weight_magnitude_attention(block.attn)\n        magnitude += compute_weight_magnitude_mlp(block.mlp)\n        magnitude += compute_weight_magnitude_layer_norm(block.norm_2)\n    magnitude += compute_weight_magnitude_layer_norm(model.transformer.ln_f)\n    return magnitude\n</code></pre>"},{"location":"api/whittle/metrics/parameters/","title":"Parameters","text":""},{"location":"api/whittle/metrics/parameters/#whittle.metrics.parameters","title":"whittle.metrics.parameters","text":""},{"location":"api/whittle/metrics/parameters/#whittle.metrics.parameters.compute_parameters","title":"compute_parameters","text":"<pre><code>compute_parameters(model: GPT) -&gt; float\n</code></pre> <p>Computes parameters of the current sub-network of a GPT mmodel. Make sure to set the sub-network before calling this function.</p> Refs <p>towardsdatascience.com/how-to-estimate-the-number-of-parameters-in-transformer-models-ca0f57d8dff0</p> PARAMETER DESCRIPTION <code>model</code> <p>GPT model</p> <p> TYPE: <code>GPT</code> </p> RETURNS DESCRIPTION <code>float</code> <p>number of parameters of the activated sub-network</p> <p> TYPE: <code>float</code> </p> Source code in <code>whittle/metrics/parameters.py</code> <pre><code>def compute_parameters(model: GPT) -&gt; float:\n    \"\"\"\n    Computes parameters of the current sub-network of a GPT mmodel. Make sure to set the sub-network before\n    calling this function.\n\n    Refs:\n        https://towardsdatascience.com/how-to-estimate-the-number-of-parameters-in-transformer-models-ca0f57d8dff0\n\n    Args:\n        model: GPT model\n\n    Returns:\n        float: number of parameters of the activated sub-network\n    \"\"\"\n\n    num_params = 0\n    num_params += params_linear_layer(model.lm_head)\n    num_params += params_embedding_layer(model.transformer.wte)\n    for i in range(model.sub_network_n_layers):\n        block = model.transformer.h[i]\n        num_params += params_mlp(block.mlp)\n        num_params += params_attention_layer(block.attn)\n\n        num_params += params_layer_normalization(block.norm_1)\n        num_params += params_layer_normalization(block.norm_2)\n    num_params += params_layer_normalization(model.transformer.ln_f)\n    return num_params\n</code></pre>"},{"location":"api/whittle/models/gpt/checkpoint/","title":"Checkpoint","text":""},{"location":"api/whittle/models/gpt/checkpoint/#whittle.models.gpt.checkpoint","title":"whittle.models.gpt.checkpoint","text":""},{"location":"api/whittle/models/gpt/checkpoint/#whittle.models.gpt.checkpoint.load_checkpoint","title":"load_checkpoint","text":"<pre><code>load_checkpoint(\n    checkpoint_dir: Path,\n    model_cls: type[GPT] = GPT,\n    config_cls: type[Config] | type[LoRAConfig] = Config,\n    config_attr: dict[str, Any] | None = None,\n) -&gt; GPT\n</code></pre> <p>Load a whittle or litgpt model from a checkpoint directory.</p> PARAMETER DESCRIPTION <code>checkpoint_dir</code> <p>The directory of the checkpoint.</p> <p> TYPE: <code>Path</code> </p> <code>model_cls</code> <p>The model class to instantiate. Defaults to GPT. For LoRA, use whittle.lora.lora_model.GPT.</p> <p> TYPE: <code>type[GPT]</code> DEFAULT: <code>GPT</code> </p> <code>config_cls</code> <p>The config class to instantiate. Defaults to Config. For LoRA, use whittle.lora.config.LoRAConfig.</p> <p> TYPE: <code>type[Config] | type[LoRAConfig]</code> DEFAULT: <code>Config</code> </p> <code>config_attr</code> <p>The attributes to set in the config after init. If None, config.fix_head_size is set to True. Defaults to None.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>GPT</code> <p>The loaded model.</p> <p> TYPE: <code>GPT</code> </p> Source code in <code>whittle/models/gpt/checkpoint.py</code> <pre><code>def load_checkpoint(\n    checkpoint_dir: Path,\n    model_cls: type[GPT] = GPT,\n    config_cls: type[Config] | type[LoRAConfig] = Config,\n    config_attr: dict[str, Any] | None = None,\n) -&gt; GPT:\n    \"\"\"\n    Load a whittle or litgpt model from a checkpoint directory.\n\n    Args:\n        checkpoint_dir: The directory of the checkpoint.\n        model_cls: The model class to instantiate. Defaults to GPT. For LoRA, use whittle.lora.lora_model.GPT.\n        config_cls: The config class to instantiate. Defaults to Config. For LoRA, use whittle.lora.config.LoRAConfig.\n        config_attr: The attributes to set in the config after __init__. If None, config.fix_head_size is set to True.\n            Defaults to None.\n\n    Returns:\n        GPT: The loaded model.\n    \"\"\"\n    sub_network_config: dict[str, Any] | None = None\n    ckp = lazy_load(checkpoint_dir / \"lit_model.pth\")\n\n    # sub-network config loading (contains the config and checkpoint path of the parent)\n    sub_network_config = ckp.get(\"sub_network_config\", None)\n    parent_dir = ckp.get(\"parent_dir\", None)\n\n    # check if the checkpoint is valid only if it is not a sub-network config\n    if sub_network_config is None:\n        check_valid_checkpoint_dir(\n            checkpoint_dir,\n            ignore_tokenizer_files=parent_dir\n            is not None,  # if parent_dir is not None, tokenizer files were not copied\n        )\n    # always check the parent config validity\n    if parent_dir is not None:\n        check_valid_checkpoint_dir(Path(parent_dir), ignore_tokenizer_files=False)\n\n    # it's either a standalone litgpt model or a sub-network (depending on if there is also a parent_dir)\n    if \"model\" not in ckp:\n        # not None: sub-network, None: raw state dict\n        if parent_dir is not None:\n            checkpoint_dir = Path(parent_dir)\n            ckp = lazy_load(checkpoint_dir / \"lit_model.pth\")\n\n    config = config_cls.from_file(checkpoint_dir / \"model_config.yaml\")\n    config_attr = {\"fix_head_size\": True} if config_attr is None else config_attr\n    for k, val in config_attr.items():\n        setattr(\n            config, k, val\n        )  # some args are not passed to __init__ - e.g. for config.fix_head_size = True\n\n    model = model_cls(config)\n    # for WhittleLM - it loads AutoTokenizer inside - either we copied it to checkpoint_dir, or it is referenced in parent_dir\n    model.name_or_path = checkpoint_dir if parent_dir is None else parent_dir\n\n    model.load_state_dict(ckp[\"model\"] if \"model\" in ckp else ckp)\n    del ckp\n\n    # if the checkpoint was a sub-network, set it at this point\n    if sub_network_config is not None:\n        model.select_sub_network(sub_network_config)\n\n    return model\n</code></pre>"},{"location":"api/whittle/models/gpt/checkpoint/#whittle.models.gpt.checkpoint.save_sub_network","title":"save_sub_network","text":"<pre><code>save_sub_network(\n    super_network: GPT,\n    checkpoint_dir: Path,\n    save_dir: Path,\n    sub_network_config: dict[str, Any] | None = None,\n    save_checkpoints: bool = True,\n    copy_config_files: bool = False,\n    fabric: Fabric | None = None,\n)\n</code></pre> <p>Save a sub-network to a new directory. The sub-network can be saved in three different formats:</p> <p>a) same as litgpt models:     sub_network_dir/     - lit_model.pth ... {model: sub_network.state_dict()} or sub_network.state_dict() (former - whittle model, latter - litgpt model)     - configs (model_config.yaml, tokenizer.json, etc.) b) litgpt model with saving space (not copying the tokenizer and other configs):     sub_network_dir/     - lit_model.pth ... {model: sub_network.state_dict(), parent_dir: super-network checkpoint dir}     - model_config.yaml c) compressed checkpoint:     sub_network_dir/     - lit_model.pth ... {sub_network_config: sub_network_config, parent_dir: super-network checkpoint dir}     - model_config.yaml</p> PARAMETER DESCRIPTION <code>super_network</code> <p>The super-network model.</p> <p> TYPE: <code>GPT</code> </p> <code>checkpoint_dir</code> <p>The directory of the parent super-network checkpoint (for copying config files in a), as a parent dir in b), c)).</p> <p> TYPE: <code>Path</code> </p> <code>save_dir</code> <p>The directory to save the sub-network checkpoint.</p> <p> TYPE: <code>Path</code> </p> <code>sub_network_config</code> <p>The sub-network config to save. If None, the current active sub-network checkpoint is saved (i.e. it is necessary to call .set_sub_network before calling this function). Required if <code>save_checkpoints</code> is False. Defaults to None.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>save_checkpoints</code> <p>Whether to save the sub-network as a full checkpoint, or only the config and super-network path. Defaults to True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>copy_config_files</code> <p>Whether to copy the config files (e.g. tokenizer.json) to the save directory. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>fabric</code> <p>The fabric to use for saving the checkpoint. If None, torch.save is used. Defaults to None.</p> <p> TYPE: <code>Fabric | None</code> DEFAULT: <code>None</code> </p> Source code in <code>whittle/models/gpt/checkpoint.py</code> <pre><code>def save_sub_network(\n    super_network: GPT,\n    checkpoint_dir: Path,\n    save_dir: Path,\n    sub_network_config: dict[str, Any] | None = None,\n    save_checkpoints: bool = True,\n    copy_config_files: bool = False,\n    fabric: L.Fabric | None = None,\n):\n    \"\"\"\n    Save a sub-network to a new directory. The sub-network can be saved in three different formats:\n\n    a) same as litgpt models:\n        sub_network_dir/\n        - lit_model.pth ... {model: sub_network.state_dict()} or sub_network.state_dict() (former - whittle model, latter - litgpt model)\n        - configs (model_config.yaml, tokenizer.json, etc.)\n    b) litgpt model with saving space (not copying the tokenizer and other configs):\n        sub_network_dir/\n        - lit_model.pth ... {model: sub_network.state_dict(), parent_dir: super-network checkpoint dir}\n        - model_config.yaml\n    c) compressed checkpoint:\n        sub_network_dir/\n        - lit_model.pth ... {sub_network_config: sub_network_config, parent_dir: super-network checkpoint dir}\n        - model_config.yaml\n\n    Args:\n        super_network: The super-network model.\n        checkpoint_dir: The directory of the parent super-network checkpoint (for copying config files in a), as a parent dir in b), c)).\n        save_dir: The directory to save the sub-network checkpoint.\n        sub_network_config: The sub-network config to save. If None, the current active sub-network checkpoint is saved\n            (i.e. it is necessary to call .set_sub_network before calling this function).\n            Required if `save_checkpoints` is False.\n            Defaults to None.\n        save_checkpoints: Whether to save the sub-network as a full checkpoint, or only the config and super-network path.\n            Defaults to True.\n        copy_config_files: Whether to copy the config files (e.g. tokenizer.json) to the save directory.\n            Defaults to False.\n        fabric: The fabric to use for saving the checkpoint. If None, torch.save is used.\n            Defaults to None.\n    \"\"\"\n    if not save_checkpoints and sub_network_config is None:\n        raise ValueError(\n            \"sub_network_config must be provided when save_checkpoints is False\"\n        )\n\n    assert checkpoint_dir != save_dir, \"Checkpoint and save directories must be different\"\n    save_path = save_dir / \"lit_model.pth\"\n    save_dir.mkdir(parents=True, exist_ok=True)\n\n    # either save the extracted checkpoint, or the config + path to super-network\n    if save_checkpoints:\n        if sub_network_config is not None:\n            super_network.select_sub_network(sub_network_config)\n        else:\n            warn(\n                \"No sub-network config provided - saving the current active sub-network instead (assuming the user called .set_sub_network() before). If this is not the intended behavior, pass `sub_network_config` to `save_sub_network`.\"\n            )\n        sub_network = extract_current_sub_network(super_network)\n        super_network.reset_super_network()\n\n        # either save everything including config files, or only model_config.yaml and the weights\n        if copy_config_files:\n            copy_config_files_func(checkpoint_dir, save_dir)\n            _save_checkpoint(\n                {\"model\": sub_network.state_dict()}, save_path, fabric=fabric\n            )\n        else:\n            _save_checkpoint(\n                {\"model\": sub_network.state_dict(), \"parent_dir\": checkpoint_dir},\n                save_path,\n                fabric=fabric,\n            )\n        # the new model_config.yaml is different from the original one, so we rewrite it\n        save_config(sub_network.config, save_dir)\n    else:\n        # minimalistic checkpoint - only sub-network config and path to super-network\n        _save_checkpoint(\n            {\"sub_network_config\": sub_network_config, \"parent_dir\": checkpoint_dir},\n            save_path,\n            fabric=fabric,\n        )\n</code></pre>"},{"location":"api/whittle/models/gpt/extract/","title":"Extract","text":""},{"location":"api/whittle/models/gpt/extract/#whittle.models.gpt.extract","title":"whittle.models.gpt.extract","text":""},{"location":"api/whittle/models/gpt/extract/#whittle.models.gpt.extract.extract_current_sub_network","title":"extract_current_sub_network","text":"<pre><code>extract_current_sub_network(model: GPT) -&gt; GPT\n</code></pre> <p>Extracts the current sub-network of the super-network model. The sub-network is set by calling <code>set_sub_network</code> or <code>select_sub_network</code>. This function creates a new super-network model with the same configuration and parameters as the sub-network.</p> PARAMETER DESCRIPTION <code>model</code> <p>The original, full GPT super-network model from which the active sub-network is extracted.</p> <p> TYPE: <code>GPT</code> </p> RETURNS DESCRIPTION <code>GPT</code> <p>A new super-network model instance, initialized with parameters extracted from the original model.</p> Source code in <code>whittle/models/gpt/extract.py</code> <pre><code>def extract_current_sub_network(model: GPT) -&gt; GPT:\n    \"\"\"\n    Extracts the current sub-network of the super-network model.\n    The sub-network is set by calling `set_sub_network` or `select_sub_network`. This function\n    creates a new super-network model with the same configuration and parameters as the sub-network.\n\n    Args:\n        model: The original, full GPT super-network model from which the active sub-network is extracted.\n\n    Returns:\n        A new super-network model instance, initialized with parameters extracted from the original model.\n    \"\"\"\n    subnet_config = deepcopy(model.config)\n\n    subnet_config.n_embd = model.sub_network_n_embd\n    subnet_config.intermediate_size = model.sub_network_intermediate_size\n    if model.config.n_head != model.config.n_query_groups:\n        subnet_config.n_head = (\n            int(model.sub_network_num_heads) // model.config.n_query_groups\n        ) * model.sub_network_query_groups\n    else:\n        subnet_config.n_head = model.sub_network_num_heads\n    subnet_config.n_layer = model.sub_network_n_layers\n    subnet_config.head_size = model.sub_network_head_size\n    subnet_config.n_query_groups = model.sub_network_query_groups\n    subnet_config.rope_n_elem = model.sub_network_rope_n_elem\n    subnet_config.attention_scores_scalar = model.transformer.h[\n        0\n    ].attn.sub_attention_scaler\n\n    return extract_sub_network(model, subnet_config)\n</code></pre>"},{"location":"api/whittle/models/gpt/extract/#whittle.models.gpt.extract.extract_sub_network","title":"extract_sub_network","text":"<pre><code>extract_sub_network(\n    model: GPT, sub_network_config: Config\n) -&gt; GPT\n</code></pre> <p>Extracts a sub-network from a given model based on the specified sub-network configuration. Copies relevant layers, weights, and configurations from the full model into a sub-network model.</p> PARAMETER DESCRIPTION <code>model</code> <p>The original, full GPT model from which the sub-network is extracted.</p> <p> TYPE: <code>GPT</code> </p> <code>sub_network_config</code> <p>Configuration object for the sub-network, containing the necessary architecture specifications such as embedding size, number of heads, and number of layers.</p> <p> TYPE: <code>Config</code> </p> RETURNS DESCRIPTION <code>GPT</code> <p>A new sub-network model instance, initialized with parameters extracted from the original model.</p> Source code in <code>whittle/models/gpt/extract.py</code> <pre><code>def extract_sub_network(model: GPT, sub_network_config: Config) -&gt; GPT:\n    \"\"\"\n    Extracts a sub-network from a given model based on the specified sub-network configuration.\n    Copies relevant layers, weights, and configurations from the full model into a sub-network model.\n\n    Args:\n        model: The original, full GPT model from which the sub-network is extracted.\n        sub_network_config: Configuration object for the sub-network, containing the necessary\n            architecture specifications such as embedding size, number of heads, and number of layers.\n\n    Returns:\n        A new sub-network model instance, initialized with parameters extracted from the original model.\n    \"\"\"\n\n    sub_network = GPT(sub_network_config)\n\n    state_dict = extract_linear(model.lm_head)\n    sub_network.lm_head.load_state_dict(state_dict)\n\n    state_dict = extract_embedding(model.transformer.wte)\n    sub_network.transformer.wte.load_state_dict(state_dict)\n\n    extract_norm(model.transformer.ln_f, sub_network.transformer.ln_f)\n\n    for i in range(sub_network_config.n_layer):\n        block = model.transformer.h[i]\n        sub_network_block = sub_network.transformer.h[i]\n\n        # Attention\n        extract_attention(block.attn, sub_network_block.attn)\n\n        # MLP\n        extract_mlp(block.mlp, sub_network_block.mlp)\n\n        # norm\n        extract_norm(block.norm_1, sub_network_block.norm_1)\n        extract_norm(block.post_attention_norm, sub_network_block.post_attention_norm)\n        extract_norm(block.norm_2, sub_network_block.norm_2)\n        extract_norm(block.post_mlp_norm, sub_network_block.post_mlp_norm)\n\n    return sub_network\n</code></pre>"},{"location":"api/whittle/models/gpt/model/","title":"Model","text":""},{"location":"api/whittle/models/gpt/model/#whittle.models.gpt.model","title":"whittle.models.gpt.model","text":"<p>Full definition of a decoder-only transformer-based language model, all of it in this single file.</p> <p>Based on the nanoGPT implementation: karpathy/nanoGPT and github.com/EleutherAI/gpt-neox/tree/main/megatron/model.</p>"},{"location":"api/whittle/models/gpt/model/#whittle.models.gpt.model.GPT","title":"GPT","text":"<pre><code>GPT(config: Config)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>An extension of litgpt's GPT model with support to adapt to sub-network dimensionality.</p> Source code in <code>whittle/models/gpt/model.py</code> <pre><code>def __init__(self, config: Config) -&gt; None:\n    super().__init__()\n    assert config.padded_vocab_size is not None\n    self.config = config\n    self.lm_head = Linear(\n        config.n_embd, config.padded_vocab_size, bias=config.lm_head_bias\n    )\n    self.transformer = nn.ModuleDict(\n        dict(\n            wte=Embedding(config.padded_vocab_size, config.n_embd),\n            h=nn.ModuleList(\n                Block(config, block_idx) for block_idx in range(config.n_layer)\n            ),\n            ln_f=self.norm_class(config.n_embd, eps=config.norm_eps),\n        )\n    )\n    self.max_layer = config.n_layer\n    self.max_seq_length = self.config.block_size\n    self.mask_cache: torch.Tensor | None = None\n\n    # Set current sub-network to super-network\n    self.sub_network_n_embd = self.config.n_embd\n    self.sub_network_intermediate_size = self.config.intermediate_size\n    self.sub_network_num_heads = self.config.n_head\n    self.sub_network_n_layers = self.config.n_layer\n    self.sub_network_head_size: int | None = self.config.head_size\n    self.sub_network_query_groups: int | None = self.config.n_query_groups\n    self.sub_network_rope_n_elem = self.config.rope_n_elem\n    self.cos: torch.Tensor\n    self.sin: torch.Tensor\n    self.config.is_encoder_decoder = False\n    self.main_input_name = \"input_pos\"\n    self._supports_cache_class = True\n</code></pre>"},{"location":"api/whittle/models/gpt/model/#whittle.models.gpt.model.GPT.reset_super_network","title":"reset_super_network","text":"<pre><code>reset_super_network()\n</code></pre> <p>Resets the GPT model to the original super-network dimensionality.</p> Source code in <code>whittle/models/gpt/model.py</code> <pre><code>def reset_super_network(self):\n    \"\"\"\n    Resets the GPT model to the original super-network dimensionality.\n    \"\"\"\n    rebuild_rope = self.sub_network_rope_n_elem != self.config.rope_n_elem\n\n    self.sub_network_n_embd = self.config.n_embd\n    self.sub_network_intermediate_size = self.config.intermediate_size\n    self.sub_network_num_heads = self.config.n_head\n    self.sub_network_n_layers = self.config.n_layer\n    self.sub_network_head_size: int | None = self.config.head_size  # type: ignore\n    self.sub_network_query_groups: int | None = self.config.n_query_groups  # type: ignore\n    self.sub_network_rope_n_elem = self.config.rope_n_elem\n    self.transformer.wte.reset_super_network()\n    self.transformer.ln_f.reset_super_network()\n    for i in range(self.config.n_layer):\n        block = self.transformer.h[i]\n        block.reset_super_network()\n    self.lm_head.reset_super_network()\n\n    # rebuild the rope cache\n    if rebuild_rope:\n        self.reset_parameters()\n</code></pre>"},{"location":"api/whittle/models/gpt/model/#whittle.models.gpt.model.GPT.select_sub_network","title":"select_sub_network","text":"<pre><code>select_sub_network(config: dict[str, Any]) -&gt; None\n</code></pre> <p>Selects and sets the sub-network configuration based on the provided configuration.</p> Source code in <code>whittle/models/gpt/model.py</code> <pre><code>def select_sub_network(self, config: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Selects and sets the sub-network configuration based on the provided configuration.\n    \"\"\"\n    self.set_sub_network(\n        config[\"embed_dim\"],\n        int(config[\"mlp_ratio\"] * config[\"embed_dim\"]),\n        config[\"num_heads\"],\n        config[\"depth\"],\n        sub_network_head_size=config.get(\"head_size\", None),\n        sub_network_query_groups=config.get(\"n_query_groups\", None),\n    )\n</code></pre>"},{"location":"api/whittle/models/gpt/model/#whittle.models.gpt.model.GPT.set_sub_network","title":"set_sub_network","text":"<pre><code>set_sub_network(\n    sub_network_n_embd: int,\n    sub_network_intermediate_size: int,\n    sub_network_num_heads: int,\n    sub_network_n_layers: int,\n    sub_network_query_groups: int | None = None,\n    sub_network_head_size: int | None = None,\n) -&gt; None\n</code></pre> <p>Sets the GPT model to the specified sub-network dimensionality. Input arguments are set to the specified sub-network dimensionality.</p> PARAMETER DESCRIPTION <code>sub_network_n_embd</code> <p>Embedding dimension of the sub-network.</p> <p> TYPE: <code>int</code> </p> <code>sub_network_intermediate_size</code> <p>Intermediate size of the sub-network.</p> <p> TYPE: <code>int</code> </p> <code>sub_network_num_heads</code> <p>Number of attention heads in the sub-network.</p> <p> TYPE: <code>int</code> </p> <code>sub_network_n_layers</code> <p>Number of layers in the sub-network.</p> <p> TYPE: <code>int</code> </p> <code>sub_network_query_groups</code> <p>Number of query groups in the sub-network. Defaults to None.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>sub_network_head_size</code> <p>Size of each attention head in the sub-network. Defaults to None.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>whittle/models/gpt/model.py</code> <pre><code>def set_sub_network(\n    self,\n    sub_network_n_embd: int,\n    sub_network_intermediate_size: int,\n    sub_network_num_heads: int,\n    sub_network_n_layers: int,\n    sub_network_query_groups: int | None = None,\n    sub_network_head_size: int | None = None,\n) -&gt; None:\n    \"\"\"\n    Sets the GPT model to the specified sub-network dimensionality.\n    Input arguments are set to the specified sub-network dimensionality.\n\n    Args:\n        sub_network_n_embd: Embedding dimension of the sub-network.\n        sub_network_intermediate_size: Intermediate size of the sub-network.\n        sub_network_num_heads: Number of attention heads in the sub-network.\n        sub_network_n_layers: Number of layers in the sub-network.\n        sub_network_query_groups: Number of query groups in the sub-network. Defaults to None.\n        sub_network_head_size: Size of each attention head in the sub-network. Defaults to None.\n    \"\"\"\n    self.sub_network_n_embd = sub_network_n_embd\n    self.sub_network_intermediate_size = sub_network_intermediate_size\n    self.sub_network_num_heads = sub_network_num_heads\n    self.sub_network_n_layers = sub_network_n_layers\n    self.transformer.wte.set_sub_network(self.sub_network_n_embd)\n    self.transformer.ln_f.set_sub_network(self.sub_network_n_embd)\n    if self.config.n_query_groups == 1:\n        self.sub_network_query_groups = 1\n        self.sub_network_num_heads = (\n            sub_network_num_heads\n            if sub_network_num_heads is not None\n            else self.config.n_head\n        )\n    elif self.config.n_head != self.config.n_query_groups:\n        self.sub_network_num_heads = (\n            sub_network_num_heads\n            if sub_network_num_heads is not None\n            else self.config.n_head\n        )\n        self.sub_network_query_groups = (\n            sub_network_query_groups\n            if sub_network_query_groups is not None\n            else self.config.n_query_groups\n        )\n    else:\n        self.sub_network_query_groups = (\n            sub_network_query_groups\n            if sub_network_query_groups is not None\n            else self.config.n_head\n        )\n    if self.config.fix_head_size:\n        if sub_network_head_size is None:\n            self.sub_network_head_size = self.config.head_size\n        else:\n            self.sub_network_head_size = sub_network_head_size\n    else:\n        if sub_network_head_size is not None:\n            self.sub_network_head_size = sub_network_head_size\n        else:\n            self.sub_network_head_size = (\n                self.sub_network_n_embd // self.sub_network_num_heads\n            )\n    for i in range(self.sub_network_n_layers):\n        block = self.transformer.h[i]\n        block.set_sub_network(\n            self.sub_network_n_embd,\n            self.sub_network_intermediate_size,\n            self.sub_network_num_heads,\n            self.sub_network_query_groups,\n            self.sub_network_head_size,\n        )\n    # these change inside causal_self_attention\n    if self.sub_network_n_layers &gt; 0:\n        self.sub_network_query_groups = block.attn.sub_network_query_groups\n\n    self.lm_head.set_sub_network(\n        self.sub_network_n_embd, self.config.padded_vocab_size\n    )\n\n    # change the rope cache to match n_elem induced by subnet head size\n    self.sub_network_rope_n_elem = int(\n        self.config.rotary_percentage * self.sub_network_head_size\n    )\n    self.cos, self.sin = self.rope_cache(\n        seq_len=self._max_seq_length,\n        n_elem=self.sub_network_rope_n_elem,\n        device=self.cos.device,\n    )\n</code></pre>"},{"location":"api/whittle/models/gpt/utils/","title":"Utils","text":""},{"location":"api/whittle/models/gpt/utils/#whittle.models.gpt.utils","title":"whittle.models.gpt.utils","text":"<p>Utility functions for training and inference.</p>"},{"location":"api/whittle/models/gpt/utils/#whittle.models.gpt.utils.CycleIterator","title":"CycleIterator","text":"<pre><code>CycleIterator(iterable: Iterable)\n</code></pre> <p>An iterator that cycles through an iterable indefinitely.</p> Example <p>iterator = CycleIterator([1, 2, 3]) [next(iterator) for _ in range(5)]</p> <p>[1, 2, 3, 1, 2]</p> Note <p>Unlike <code>itertools.cycle</code>, this iterator does not cache the values of the iterable.</p> Source code in <code>whittle/models/gpt/utils.py</code> <pre><code>def __init__(self, iterable: Iterable) -&gt; None:\n    self.iterable = iterable\n    self.epoch = 0\n    self._iterator: Iterator | None = None\n</code></pre>"},{"location":"api/whittle/models/gpt/utils/#whittle.models.gpt.utils.estimate_flops","title":"estimate_flops","text":"<pre><code>estimate_flops(model: GPT, training: bool) -&gt; int\n</code></pre> <p>Measures estimated FLOPs for MFU.</p> Refs <ul> <li>ar5iv.labs.arxiv.org/html/2205.05198#A1</li> <li>ar5iv.labs.arxiv.org/html/2204.02311#A2</li> </ul> Source code in <code>whittle/models/gpt/utils.py</code> <pre><code>def estimate_flops(model: GPT, training: bool) -&gt; int:\n    \"\"\"Measures estimated FLOPs for MFU.\n\n    Refs:\n        * https://ar5iv.labs.arxiv.org/html/2205.05198#A1\n        * https://ar5iv.labs.arxiv.org/html/2204.02311#A2\n    \"\"\"\n    # using all parameters for this is a naive over estimation because not all model parameters actually contribute to\n    # this FLOP computation (e.g. embedding, norm). For this reason, the result will be higher by a fixed percentage\n    # (~10%) compared to the measured FLOPs, making those lower but more realistic.\n    # For a proper estimate, this needs a more fine-grained calculation as in Appendix A of the paper.\n    n_trainable_params = num_parameters(model, requires_grad=True)\n    trainable_flops = flops_per_param(\n        model.max_seq_length,\n        model.config.n_layer,\n        model.config.n_embd,\n        n_trainable_params,\n    )\n    # forward + backward + gradients (assumes no gradient accumulation)\n    ops_per_step = 3 if training else 1\n    n_frozen_params = num_parameters(model, requires_grad=False)\n    frozen_flops = flops_per_param(\n        model.max_seq_length, model.config.n_layer, model.config.n_embd, n_frozen_params\n    )\n    # forward + backward\n    frozen_ops_per_step = 2 if training else 1\n    return ops_per_step * trainable_flops + frozen_ops_per_step * frozen_flops\n</code></pre>"},{"location":"api/whittle/models/gpt/utils/#whittle.models.gpt.utils.get_default_supported_precision","title":"get_default_supported_precision","text":"<pre><code>get_default_supported_precision(training: bool) -&gt; str\n</code></pre> <p>Return default precision that is supported by the hardware: either <code>bf16</code> or <code>16</code>.</p> PARAMETER DESCRIPTION <code>training</code> <p><code>-mixed</code> or <code>-true</code> version of the precision to use</p> <p> TYPE: <code>bool</code> </p> RETURNS DESCRIPTION <code>str</code> <p>default precision that is suitable for the task and is supported by the hardware</p> Source code in <code>whittle/models/gpt/utils.py</code> <pre><code>def get_default_supported_precision(training: bool) -&gt; str:\n    \"\"\"\n    Return default precision that is supported by the hardware: either `bf16` or `16`.\n\n    Args:\n        training: `-mixed` or `-true` version of the precision to use\n\n    Returns:\n        default precision that is suitable for the task and is supported by the hardware\n    \"\"\"\n    from lightning.fabric.accelerators import MPSAccelerator\n\n    if MPSAccelerator.is_available() or (\n        torch.cuda.is_available() and not torch.cuda.is_bf16_supported()\n    ):\n        return \"16-mixed\" if training else \"16-true\"\n    return \"bf16-mixed\" if training else \"bf16-true\"\n</code></pre>"},{"location":"api/whittle/models/gpt/utils/#whittle.models.gpt.utils.sample_config","title":"sample_config","text":"<pre><code>sample_config(\n    choices_dict: dict,\n    layer_sampling_scheme: str = \"normal\",\n    seed: int = 0,\n) -&gt; dict\n</code></pre> <p>Sample a configuration from a dictionary of choices.</p> PARAMETER DESCRIPTION <code>choices_dict</code> <p>a dictionary of choices for each architectural dimension</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>a dictionary with the same keys as <code>choices_dict</code> and the sampled choices</p> Source code in <code>whittle/models/gpt/utils.py</code> <pre><code>def sample_config(\n    choices_dict: dict, layer_sampling_scheme: str = \"normal\", seed: int = 0\n) -&gt; dict:\n    \"\"\"Sample a configuration from a dictionary of choices.\n\n    Args:\n        choices_dict: a dictionary of choices for each architectural dimension\n\n    Returns:\n        a dictionary with the same keys as `choices_dict` and the sampled choices\n    \"\"\"\n    sampled_dict = {}\n    r = random.Random(seed)\n    # sample embed dim -&gt; held constant throughout transformer\n    sampled_dict[\"sample_embed_dim\"] = r.choice(choices_dict[\"embed_dim_choices\"])\n    # sample number of layers\n    sampled_dict[\"sample_n_layer\"] = r.choice(choices_dict[\"n_layer_choices\"])\n    # sample layer indices\n    max_layer = max(choices_dict[\"n_layer_choices\"])\n    sampled_dict[\"sample_layer_indices\"] = []\n    if layer_sampling_scheme == \"normal\":\n        sampled_dict[\"sample_layer_indices\"] = list(range(sampled_dict[\"sample_n_layer\"]))\n    elif layer_sampling_scheme == \"strided\":\n        if sampled_dict[\"sample_n_layer\"] == max_layer:\n            sampled_dict[\"sample_layer_indices\"] = list(\n                range(sampled_dict[\"sample_n_layer\"])\n            )\n        elif sampled_dict[\"sample_n_layer\"] == max_layer - 1:\n            sampled_dict[\"sample_layer_indices\"] = list(\n                range(sampled_dict[\"sample_n_layer\"])\n            )\n            sampled_dict[\"sample_layer_indices\"][-1] = max_layer - 1\n        else:\n            increment_floor = max_layer // sampled_dict[\"sample_n_layer\"]\n            increment_ceil = int(np.ceil(max_layer / sampled_dict[\"sample_n_layer\"]))\n            sampled_dict[\"sample_layer_indices\"] = [\n                0 for _ in range(sampled_dict[\"sample_n_layer\"])\n            ]\n            counter_layer = 0\n            for i in range(sampled_dict[\"sample_n_layer\"]):\n                if counter_layer &lt; (max_layer // 2):\n                    sampled_dict[\"sample_layer_indices\"][i] = counter_layer\n                    counter_layer += increment_ceil\n                else:\n                    sampled_dict[\"sample_layer_indices\"][i] = counter_layer\n                    counter_layer += increment_floor\n\n            sampled_dict[\"sample_layer_indices\"][0] = 0\n            sampled_dict[\"sample_layer_indices\"][-1] = max_layer - 1\n\n    # sample number of heads\n    sampled_dict[\"sample_n_head\"] = [\n        r.choice(choices_dict[\"n_head_choices\"]) for _ in range(max_layer)\n    ]\n    # sample mlp ratio\n    sampled_dict[\"sample_mlp_ratio\"] = [\n        r.choice(choices_dict[\"mlp_ratio_choices\"]) for _ in range(max_layer)\n    ]\n    # sample bias\n    sampled_dict[\"sample_bias\"] = r.choice(choices_dict[\"bias_choices\"])\n\n    return sampled_dict\n</code></pre>"},{"location":"api/whittle/models/gpt/utils/#whittle.models.gpt.utils.sample_config_max","title":"sample_config_max","text":"<pre><code>sample_config_max(\n    choices_dict: dict,\n    layer_sampling_scheme: str = \"normal\",\n) -&gt; dict\n</code></pre> <p>Sample a configuration from a dictionary of choices.</p> PARAMETER DESCRIPTION <code>choices_dict</code> <p>a dictionary of choices for each architectural dimension</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>a dictionary with the same keys as <code>choices_dict</code> and the sampled choices</p> Source code in <code>whittle/models/gpt/utils.py</code> <pre><code>def sample_config_max(choices_dict: dict, layer_sampling_scheme: str = \"normal\") -&gt; dict:\n    \"\"\"Sample a configuration from a dictionary of choices.\n\n    Args:\n        choices_dict: a dictionary of choices for each architectural dimension\n\n    Returns:\n        a dictionary with the same keys as `choices_dict` and the sampled choices\n    \"\"\"\n    sampled_dict = {}\n    # sample embed dim -&gt; held constant throughout transformer\n    sampled_dict[\"sample_embed_dim\"] = max(choices_dict[\"embed_dim_choices\"])\n    # sample number of layers\n    sampled_dict[\"sample_n_layer\"] = max(choices_dict[\"n_layer_choices\"])\n    # sample layer indices\n    max_layer = max(choices_dict[\"n_layer_choices\"])\n    sampled_dict[\"sample_layer_indices\"] = []\n    sampled_dict[\"sample_layer_indices\"] = list(range(sampled_dict[\"sample_n_layer\"]))\n\n    # sample number of heads\n    sampled_dict[\"sample_n_head\"] = [\n        max(choices_dict[\"n_head_choices\"]) for _ in range(max_layer)\n    ]\n    # sample mlp ratio\n    sampled_dict[\"sample_mlp_ratio\"] = [\n        max(choices_dict[\"mlp_ratio_choices\"]) for _ in range(max_layer)\n    ]\n    # sample bias\n    sampled_dict[\"sample_bias\"] = True\n\n    return sampled_dict\n</code></pre>"},{"location":"api/whittle/models/gpt/utils/#whittle.models.gpt.utils.sample_config_mid","title":"sample_config_mid","text":"<pre><code>sample_config_mid(\n    choices_dict: dict,\n    layer_sampling_scheme: str = \"normal\",\n) -&gt; dict\n</code></pre> <p>Sample a configuration from a dictionary of choices.</p> PARAMETER DESCRIPTION <code>choices_dict</code> <p>a dictionary of choices for each architectural dimension</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>a dictionary with the same keys as <code>choices_dict</code> and the sampled choices</p> Source code in <code>whittle/models/gpt/utils.py</code> <pre><code>def sample_config_mid(choices_dict: dict, layer_sampling_scheme: str = \"normal\") -&gt; dict:\n    \"\"\"Sample a configuration from a dictionary of choices.\n\n    Args:\n        choices_dict: a dictionary of choices for each architectural dimension\n\n    Returns:\n        a dictionary with the same keys as `choices_dict` and the sampled choices\n    \"\"\"\n    sampled_dict = {}\n    # sample embed dim -&gt; held constant throughout transformer\n    sampled_dict[\"sample_embed_dim\"] = choices_dict[\"embed_dim_choices\"][1]\n    # sample number of layers\n    sampled_dict[\"sample_n_layer\"] = choices_dict[\"n_layer_choices\"][1]\n    # sample layer indices\n    max_layer = choices_dict[\"n_layer_choices\"][1]\n    sampled_dict[\"sample_layer_indices\"] = []\n    sampled_dict[\"sample_layer_indices\"] = list(range(sampled_dict[\"sample_n_layer\"]))\n\n    # sample number of heads\n    sampled_dict[\"sample_n_head\"] = [\n        choices_dict[\"n_head_choices\"][1] for _ in range(max_layer)\n    ]\n    # sample mlp ratio\n    sampled_dict[\"sample_mlp_ratio\"] = [\n        choices_dict[\"mlp_ratio_choices\"][1] for _ in range(max_layer)\n    ]\n    # sample bias\n    sampled_dict[\"sample_bias\"] = True\n\n    return sampled_dict\n</code></pre>"},{"location":"api/whittle/models/gpt/utils/#whittle.models.gpt.utils.sample_config_min","title":"sample_config_min","text":"<pre><code>sample_config_min(\n    choices_dict: dict,\n    layer_sampling_scheme: str = \"normal\",\n) -&gt; dict\n</code></pre> <p>Sample a configuration from a dictionary of choices.</p> PARAMETER DESCRIPTION <code>choices_dict</code> <p>a dictionary of choices for each architectural dimension</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>a dictionary with the same keys as <code>choices_dict</code> and the sampled choices</p> Source code in <code>whittle/models/gpt/utils.py</code> <pre><code>def sample_config_min(choices_dict: dict, layer_sampling_scheme: str = \"normal\") -&gt; dict:\n    \"\"\"Sample a configuration from a dictionary of choices.\n\n    Args:\n        choices_dict: a dictionary of choices for each architectural dimension\n\n    Returns:\n        a dictionary with the same keys as `choices_dict` and the sampled choices\n    \"\"\"\n    sampled_dict = {}\n    # sample embed dim -&gt; held constant throughout transformer\n    sampled_dict[\"sample_embed_dim\"] = min(choices_dict[\"embed_dim_choices\"])\n    # sample number of layers\n    sampled_dict[\"sample_n_layer\"] = min(choices_dict[\"n_layer_choices\"])\n    # sample layer indices\n    max_layer = min(choices_dict[\"n_layer_choices\"])\n    sampled_dict[\"sample_layer_indices\"] = []\n    sampled_dict[\"sample_layer_indices\"] = list(range(sampled_dict[\"sample_n_layer\"]))\n\n    # sample number of heads\n    sampled_dict[\"sample_n_head\"] = [\n        min(choices_dict[\"n_head_choices\"]) for _ in range(max_layer)\n    ]\n    # sample mlp ratio\n    sampled_dict[\"sample_mlp_ratio\"] = [\n        min(choices_dict[\"mlp_ratio_choices\"]) for _ in range(max_layer)\n    ]\n    # sample bias\n    sampled_dict[\"sample_bias\"] = True\n\n    return sampled_dict\n</code></pre>"},{"location":"api/whittle/models/gpt/blocks/causal_self_attention/","title":"Causal self attention","text":""},{"location":"api/whittle/models/gpt/blocks/causal_self_attention/#whittle.models.gpt.blocks.causal_self_attention","title":"whittle.models.gpt.blocks.causal_self_attention","text":""},{"location":"api/whittle/models/gpt/blocks/causal_self_attention/#whittle.models.gpt.blocks.causal_self_attention.CausalSelfAttention","title":"CausalSelfAttention","text":"<pre><code>CausalSelfAttention(config: Config, block_idx: int)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Extension of litgpt's <code>litgpt.model.CausalSelfAttention</code> with support to adapt to sub-network dimensionality.</p> Source code in <code>whittle/models/gpt/blocks/causal_self_attention.py</code> <pre><code>def __init__(self, config: Config, block_idx: int) -&gt; None:\n    super().__init__()\n    shape = (config.n_head + 2 * config.n_query_groups) * config.head_size\n    # key, query, value projections for all heads, but in a batch\n    self.attn = LinearQKV(config.n_embd, shape, bias=config.bias)\n    # output projection\n    # if `head_size` is explicitly specified in the config, `n_emd` might not be equal to `head_size * n_head`\n    self.proj = LinearProj(\n        config.head_size * config.n_head, config.n_embd, bias=config.bias\n    )\n    # disabled by default\n    self.kv_cache: KVCache | None = None\n    self.apply_sliding_window_attention = (\n        config.sliding_window_size is not None\n        and block_idx % config.sliding_window_layer_placing == 0\n    )\n    self.config = config\n    # Set current sub-network to super-network\n    self.q_per_kv = config.n_head // config.n_query_groups\n    self.sub_network_n_embd = self.config.n_embd\n    self.sub_network_n_head = self.config.n_head\n    self.sub_network_head_size = self.config.head_size\n    self.sub_network_qkv_shape = (\n        (self.q_per_kv + 2) * self.config.head_size * self.config.n_query_groups\n    )\n    self.sub_network_query_groups = self.config.n_query_groups\n    self.sub_network_q_per_kv = int(\n        self.sub_network_n_head // self.sub_network_query_groups\n    )\n    self.sub_attention_scaler = self.config.attention_scores_scalar\n</code></pre>"},{"location":"api/whittle/models/gpt/blocks/causal_self_attention/#whittle.models.gpt.blocks.causal_self_attention.CausalSelfAttention.reset_super_network","title":"reset_super_network","text":"<pre><code>reset_super_network()\n</code></pre> <p>Resets the dimensionality of the current sub-network to the super-network dimensionality.</p> Source code in <code>whittle/models/gpt/blocks/causal_self_attention.py</code> <pre><code>def reset_super_network(self):\n    \"\"\"Resets the dimensionality of the current sub-network to the super-network dimensionality.\"\"\"\n    self.sub_network_n_embd = self.config.n_embd\n    self.sub_network_n_head = self.config.n_head\n    self.sub_network_head_size = self.config.head_size\n    self.sub_network_qkv_shape = (\n        self.config.n_head + 2 * self.config.n_query_groups\n    ) * self.config.head_size\n    self.sub_network_query_groups = self.config.n_query_groups\n    self.sub_network_q_per_kv = int(\n        self.sub_network_n_head // self.sub_network_query_groups\n    )\n    self.attn.reset_super_network()\n    self.proj.reset_super_network()\n    self.sub_attention_scaler = self.config.attention_scores_scalar\n    self.qkv_indices = None\n    self.proj_indices = None\n</code></pre>"},{"location":"api/whittle/models/gpt/blocks/causal_self_attention/#whittle.models.gpt.blocks.causal_self_attention.CausalSelfAttention.set_sub_network","title":"set_sub_network","text":"<pre><code>set_sub_network(\n    sub_network_n_embd: int,\n    sub_network_n_head: int,\n    sub_network_query_groups: int,\n    sub_network_head_size: int,\n)\n</code></pre> <p>Sets the CausalSelfAttention block to the specified sub-network dimensionality.</p> PARAMETER DESCRIPTION <code>sub_network_n_embd</code> <p>Embedding dimension of the sub-network</p> <p> TYPE: <code>int</code> </p> <code>sub_network_n_head</code> <p>Number of attention heads in the sub-network</p> <p> TYPE: <code>int</code> </p> <code>sub_network_query_groups</code> <p>Number of query groups for grouped-query attention (GQA).</p> <p> TYPE: <code>int</code> </p> <code>sub_network_head_size</code> <p>Size of each attention head in the sub-network.</p> <p> TYPE: <code>int</code> </p> Source code in <code>whittle/models/gpt/blocks/causal_self_attention.py</code> <pre><code>def set_sub_network(\n    self,\n    sub_network_n_embd: int,\n    sub_network_n_head: int,\n    sub_network_query_groups: int,\n    sub_network_head_size: int,\n):\n    \"\"\"\n    Sets the CausalSelfAttention block to the specified sub-network dimensionality.\n\n    Args:\n        sub_network_n_embd: Embedding dimension of the sub-network\n        sub_network_n_head: Number of attention heads in the sub-network\n        sub_network_query_groups: Number of query groups for grouped-query attention (GQA).\n        sub_network_head_size: Size of each attention head in the sub-network.\n    \"\"\"\n    self.sub_network_n_embd = (\n        sub_network_n_embd if sub_network_n_embd else self.config.n_embd\n    )\n    self.sub_network_n_head = (\n        sub_network_n_head if sub_network_n_head else self.config.n_head\n    )\n    self.sub_network_query_groups = (\n        sub_network_query_groups\n        if sub_network_query_groups\n        else self.config.n_query_groups\n    )\n    self.sub_network_head_size = (\n        sub_network_head_size if sub_network_head_size else self.config.head_size\n    )\n    if self.config.n_query_groups == 1:\n        q_per_kv = self.sub_network_n_head\n        self.sub_network_query_groups = 1\n    elif (\n        self.config.n_head != self.config.n_query_groups\n        and self.config.n_query_groups != 1\n    ):\n        self.sub_network_query_groups = (\n            sub_network_query_groups\n            if sub_network_query_groups\n            else self.config.n_query_groups\n        )\n        q_per_kv = self.sub_network_n_head // self.config.n_query_groups\n    elif self.config.n_head == self.config.n_query_groups:\n        q_per_kv = 1\n        self.sub_network_query_groups = self.sub_network_n_head\n    self.sub_network_qkv_shape = (\n        (q_per_kv + 2) * self.sub_network_head_size * self.sub_network_query_groups\n    )\n    self.sub_network_q_per_kv = int(q_per_kv)\n    self.qkv_indices = self.get_qkv_indices()\n    self.attn.set_sub_network(\n        self.sub_network_n_embd, self.sub_network_qkv_shape, self.qkv_indices\n    )\n    self.proj_indices = self.get_proj_indices()\n    self.proj.set_sub_network(\n        self.sub_network_head_size\n        * self.sub_network_query_groups\n        * self.sub_network_q_per_kv,\n        self.sub_network_n_embd,\n        self.proj_indices,\n    )\n    if self.config.attention_scores_scalar:\n        self.sub_attention_scaler = self.sub_network_n_embd // self.sub_network_n_head\n    else:\n        self.sub_attention_scaler = self.config.attention_scores_scalar\n</code></pre>"},{"location":"api/whittle/models/gpt/blocks/mlp/","title":"Mlp","text":""},{"location":"api/whittle/models/gpt/blocks/mlp/#whittle.models.gpt.blocks.mlp","title":"whittle.models.gpt.blocks.mlp","text":""},{"location":"api/whittle/models/gpt/blocks/mlp/#whittle.models.gpt.blocks.mlp.GemmaMLP","title":"GemmaMLP","text":"<pre><code>GemmaMLP(config: Config)\n</code></pre> <p>               Bases: <code>LLaMAMLP</code></p> <p>Implementation of the forward pass of LLaMAMLP network.</p> Source code in <code>whittle/models/gpt/blocks/mlp.py</code> <pre><code>def __init__(self, config: Config) -&gt; None:\n    super().__init__(config)\n</code></pre>"},{"location":"api/whittle/models/gpt/blocks/mlp/#whittle.models.gpt.blocks.mlp.GemmaMLP.reset_super_network","title":"reset_super_network","text":"<pre><code>reset_super_network()\n</code></pre> <p>Reset the input dimensionality of the current sub-network to the super-network dimensionality.</p> Source code in <code>whittle/models/gpt/blocks/mlp.py</code> <pre><code>def reset_super_network(self):\n    \"\"\"Reset the input dimensionality of the current sub-network to the super-network dimensionality.\"\"\"\n    self.sub_network_n_embd = self.in_features\n    self.sub_network_intermediate_size = self.intermediate_size\n\n    self.fc_1.reset_super_network()\n    self.fc_2.reset_super_network()\n    self.proj.reset_super_network()\n</code></pre>"},{"location":"api/whittle/models/gpt/blocks/mlp/#whittle.models.gpt.blocks.mlp.GemmaMLP.set_sub_network","title":"set_sub_network","text":"<pre><code>set_sub_network(\n    sub_network_n_embd: int,\n    sub_network_intermediate_size: int,\n)\n</code></pre> <p>Sets the dimensionality of the current sub-network MLP layers.</p> PARAMETER DESCRIPTION <code>sub_network_n_embd</code> <p>Input and output embedding dimension of the sub-network.</p> <p> TYPE: <code>int</code> </p> <code>sub_network_intermediate_size</code> <p>Hidden layer dimension of the sub-network MLP.</p> <p> TYPE: <code>int</code> </p> Source code in <code>whittle/models/gpt/blocks/mlp.py</code> <pre><code>def set_sub_network(\n    self, sub_network_n_embd: int, sub_network_intermediate_size: int\n):\n    \"\"\"\n    Sets the dimensionality of the current sub-network MLP layers.\n\n    Args:\n        sub_network_n_embd: Input and output embedding dimension of the sub-network.\n        sub_network_intermediate_size: Hidden layer dimension of the sub-network MLP.\n    \"\"\"\n    self.sub_network_n_embd = sub_network_n_embd\n    self.sub_network_intermediate_size = sub_network_intermediate_size\n\n    self.fc_1.set_sub_network(\n        self.sub_network_n_embd, self.sub_network_intermediate_size\n    )\n    self.fc_2.set_sub_network(\n        self.sub_network_n_embd, self.sub_network_intermediate_size\n    )\n    self.proj.set_sub_network(\n        self.sub_network_intermediate_size, self.sub_network_n_embd\n    )\n</code></pre>"},{"location":"api/whittle/models/gpt/blocks/mlp/#whittle.models.gpt.blocks.mlp.GptNeoxMLP","title":"GptNeoxMLP","text":"<pre><code>GptNeoxMLP(config: Config)\n</code></pre> <p>               Bases: <code>GptNeoxMLP</code></p> <p>An extension of litgp's <code>litgpt.model.GptNeoxMLP</code> with support to adapt to sub-network dimensionality.</p> Source code in <code>whittle/models/gpt/blocks/mlp.py</code> <pre><code>def __init__(self, config: Config) -&gt; None:\n    super().__init__(config)\n    self.fc = Linear(config.n_embd, config.intermediate_size, bias=config.bias)\n    self.proj = Linear(config.intermediate_size, config.n_embd, bias=config.bias)\n    self.config = config\n    self.in_features = config.n_embd\n    self.intermediate_size = config.intermediate_size\n\n    # Set current sub-network to super-network\n    self.sub_network_n_embd = self.in_features\n    self.sub_network_intermediate_size = self.intermediate_size\n</code></pre>"},{"location":"api/whittle/models/gpt/blocks/mlp/#whittle.models.gpt.blocks.mlp.GptNeoxMLP.reset_super_network","title":"reset_super_network","text":"<pre><code>reset_super_network()\n</code></pre> <p>Resets the MLP dimensions to the original super-network dimensionality.</p> Source code in <code>whittle/models/gpt/blocks/mlp.py</code> <pre><code>def reset_super_network(self):\n    \"\"\"Resets the MLP dimensions to the original super-network dimensionality.\"\"\"\n    self.sub_network_n_embd = self.in_features\n    self.sub_network_intermediate_size = self.intermediate_size\n\n    self.fc.reset_super_network()\n    self.proj.reset_super_network()\n</code></pre>"},{"location":"api/whittle/models/gpt/blocks/mlp/#whittle.models.gpt.blocks.mlp.GptNeoxMLP.set_sub_network","title":"set_sub_network","text":"<pre><code>set_sub_network(\n    sub_network_n_embd: int,\n    sub_network_intermediate_size: int,\n)\n</code></pre> <p>Sets the dimensionality of the current sub-network MLP layers.</p> PARAMETER DESCRIPTION <code>sub_network_n_embd</code> <p>Input and output embedding dimension of the sub-network.</p> <p> TYPE: <code>int</code> </p> <code>sub_network_intermediate_size</code> <p>Hidden layer dimension of the sub-network MLP.</p> <p> TYPE: <code>int</code> </p> Source code in <code>whittle/models/gpt/blocks/mlp.py</code> <pre><code>def set_sub_network(\n    self, sub_network_n_embd: int, sub_network_intermediate_size: int\n):\n    \"\"\"\n    Sets the dimensionality of the current sub-network MLP layers.\n\n    Args:\n       sub_network_n_embd: Input and output embedding dimension of the sub-network.\n       sub_network_intermediate_size: Hidden layer dimension of the sub-network MLP.\n    \"\"\"\n    self.sub_network_n_embd = sub_network_n_embd\n    self.sub_network_intermediate_size = sub_network_intermediate_size\n\n    self.fc.set_sub_network(\n        self.sub_network_n_embd, self.sub_network_intermediate_size\n    )\n    self.proj.set_sub_network(\n        self.sub_network_intermediate_size, self.sub_network_n_embd\n    )\n</code></pre>"},{"location":"api/whittle/models/gpt/blocks/mlp/#whittle.models.gpt.blocks.mlp.LLaMAMLP","title":"LLaMAMLP","text":"<pre><code>LLaMAMLP(config: Config)\n</code></pre> <p>               Bases: <code>LLaMAMLP</code></p> <p>An extension of litgp's <code>litgpt.model.LLaMAMLP</code> with support to adapt to sub-network dimensionality.</p> Source code in <code>whittle/models/gpt/blocks/mlp.py</code> <pre><code>def __init__(self, config: Config) -&gt; None:\n    super().__init__(config)\n    self.fc_1 = Linear(config.n_embd, config.intermediate_size, bias=config.bias)\n    self.fc_2 = Linear(config.n_embd, config.intermediate_size, bias=config.bias)\n    self.proj = Linear(config.intermediate_size, config.n_embd, bias=config.bias)\n    self.in_features = config.n_embd\n    self.intermediate_size = config.intermediate_size\n    self.sub_network_n_embd: int | None = None\n    self.sub_network_intermediate_size: int | None = None\n    self.config = config\n</code></pre>"},{"location":"api/whittle/models/gpt/blocks/mlp/#whittle.models.gpt.blocks.mlp.LLaMAMLP.reset_super_network","title":"reset_super_network","text":"<pre><code>reset_super_network()\n</code></pre> <p>Reset the input dimensionality of the current sub-network to the super-network dimensionality.</p> Source code in <code>whittle/models/gpt/blocks/mlp.py</code> <pre><code>def reset_super_network(self):\n    \"\"\"Reset the input dimensionality of the current sub-network to the super-network dimensionality.\"\"\"\n    self.sub_network_n_embd = self.in_features\n    self.sub_network_intermediate_size = self.intermediate_size\n\n    self.fc_1.reset_super_network()\n    self.fc_2.reset_super_network()\n    self.proj.reset_super_network()\n</code></pre>"},{"location":"api/whittle/models/gpt/blocks/mlp/#whittle.models.gpt.blocks.mlp.LLaMAMLP.set_sub_network","title":"set_sub_network","text":"<pre><code>set_sub_network(\n    sub_network_n_embd: int,\n    sub_network_intermediate_size: int,\n)\n</code></pre> <p>Sets the dimensionality of the current sub-network MLP layers.</p> PARAMETER DESCRIPTION <code>sub_network_n_embd</code> <p>Input and output embedding dimension of the sub-network.</p> <p> TYPE: <code>int</code> </p> <code>sub_network_intermediate_size</code> <p>Hidden layer dimension of the sub-network MLP.</p> <p> TYPE: <code>int</code> </p> Source code in <code>whittle/models/gpt/blocks/mlp.py</code> <pre><code>def set_sub_network(\n    self, sub_network_n_embd: int, sub_network_intermediate_size: int\n):\n    \"\"\"\n    Sets the dimensionality of the current sub-network MLP layers.\n\n    Args:\n        sub_network_n_embd: Input and output embedding dimension of the sub-network.\n        sub_network_intermediate_size: Hidden layer dimension of the sub-network MLP.\n    \"\"\"\n    self.sub_network_n_embd = sub_network_n_embd\n    self.sub_network_intermediate_size = sub_network_intermediate_size\n\n    self.fc_1.set_sub_network(\n        self.sub_network_n_embd, self.sub_network_intermediate_size\n    )\n    self.fc_2.set_sub_network(\n        self.sub_network_n_embd, self.sub_network_intermediate_size\n    )\n    self.proj.set_sub_network(\n        self.sub_network_intermediate_size, self.sub_network_n_embd\n    )\n</code></pre>"},{"location":"api/whittle/models/gpt/blocks/transformer_block/","title":"Transformer block","text":""},{"location":"api/whittle/models/gpt/blocks/transformer_block/#whittle.models.gpt.blocks.transformer_block","title":"whittle.models.gpt.blocks.transformer_block","text":""},{"location":"api/whittle/models/gpt/blocks/transformer_block/#whittle.models.gpt.blocks.transformer_block.Block","title":"Block","text":"<pre><code>Block(config: Config, block_idx: int)\n</code></pre> <p>               Bases: <code>Block</code></p> <p>An extension of litgpt's Transformer Block with support to adapt to sub-network dimensionality.</p> Source code in <code>whittle/models/gpt/blocks/transformer_block.py</code> <pre><code>def __init__(self, config: Config, block_idx: int) -&gt; None:\n    super().__init__(config, block_idx)\n    self.config = config\n    if not config.parallel_residual and config.shared_attention_norm:\n        raise NotImplementedError(\n            \"No checkpoint amongst the ones we support uses this configuration\"\n            \" (non-parallel residual and shared attention norm).\"\n        )\n\n    self.norm_1 = self.norm_class()(config.n_embd, eps=config.norm_eps)\n    self.attn = CausalSelfAttention(config, block_idx)\n    self.post_attention_norm = (\n        self.norm_class()(config.n_embd, eps=config.norm_eps)\n        if config.post_attention_norm\n        else nn.Identity()\n    )\n    self.norm_2: LayerNorm | RMSNorm | None = (\n        None\n        if config.shared_attention_norm\n        else self.norm_class()(config.n_embd, eps=config.norm_eps)\n    )\n    self.mlp = self.mlp_class()(config)\n    self.post_mlp_norm = (\n        self.norm_class()(config.n_embd, eps=config.norm_eps)\n        if config.post_mlp_norm\n        else nn.Identity()\n    )\n    # Set current sub-network to super-network\n    self.sub_network_n_embd = self.config.n_embd\n    self.sub_network_intermediate_size = self.config.intermediate_size\n    self.sub_network_num_heads = self.config.n_head\n</code></pre>"},{"location":"api/whittle/models/gpt/blocks/transformer_block/#whittle.models.gpt.blocks.transformer_block.Block.reset_super_network","title":"reset_super_network","text":"<pre><code>reset_super_network()\n</code></pre> <p>Resets the layers in the Block to it's original super-network dimensionality.</p> Source code in <code>whittle/models/gpt/blocks/transformer_block.py</code> <pre><code>def reset_super_network(self):\n    \"\"\"\n    Resets the layers in the Block to it's original super-network dimensionality.\n    \"\"\"\n    self.sub_network_n_embd = self.config.n_embd\n    self.sub_network_intermediate_size = self.config.intermediate_size\n    self.sub_network_num_heads = self.config.n_head\n    self.norm_1.reset_super_network()\n    self.attn.reset_super_network()\n    if not self.config.shared_attention_norm:\n        self.norm_2.reset_super_network()\n    self.mlp.reset_super_network()\n    if isinstance(self.post_attention_norm, LayerNorm) or isinstance(\n        self.post_attention_norm, RMSNorm\n    ):\n        self.post_attention_norm.reset_super_network()\n    if isinstance(self.post_mlp_norm, LayerNorm) or isinstance(\n        self.post_mlp_norm, RMSNorm\n    ):\n        self.post_mlp_norm.reset_super_network()\n</code></pre>"},{"location":"api/whittle/models/gpt/blocks/transformer_block/#whittle.models.gpt.blocks.transformer_block.Block.set_sub_network","title":"set_sub_network","text":"<pre><code>set_sub_network(\n    sub_network_n_embd: int,\n    sub_network_intermediate_size: int,\n    sub_network_num_heads: int,\n    sub_network_query_groups: int,\n    sub_network_head_size: int,\n) -&gt; None\n</code></pre> <p>Set the Block to the specified sub-network dimensionality.</p> PARAMETER DESCRIPTION <code>sub_network_n_embd</code> <p>Embedding dimension of the sub-network.</p> <p> TYPE: <code>int</code> </p> <code>sub_network_intermediate_size</code> <p>Intermediate size of the sub-network.</p> <p> TYPE: <code>int</code> </p> <code>sub_network_num_heads</code> <p>Number of attention heads in the sub-network.</p> <p> TYPE: <code>int</code> </p> <code>sub_network_query_groups</code> <p>Number of query groups in the sub-network.</p> <p> TYPE: <code>int</code> </p> <code>sub_network_head_size</code> <p>Size of each attention head in the sub-network.</p> <p> TYPE: <code>int</code> </p> Source code in <code>whittle/models/gpt/blocks/transformer_block.py</code> <pre><code>def set_sub_network(\n    self,\n    sub_network_n_embd: int,\n    sub_network_intermediate_size: int,\n    sub_network_num_heads: int,\n    sub_network_query_groups: int,\n    sub_network_head_size: int,\n) -&gt; None:\n    \"\"\"\n    Set the Block to the specified sub-network dimensionality.\n\n    Args:\n        sub_network_n_embd: Embedding dimension of the sub-network.\n        sub_network_intermediate_size: Intermediate size of the sub-network.\n        sub_network_num_heads: Number of attention heads in the sub-network.\n        sub_network_query_groups: Number of query groups in the sub-network.\n        sub_network_head_size: Size of each attention head in the sub-network.\n    \"\"\"\n    self.sub_network_n_embd = sub_network_n_embd\n    self.sub_network_intermediate_size = sub_network_intermediate_size\n    self.sub_network_num_heads = sub_network_num_heads\n    self.norm_1.set_sub_network(self.sub_network_n_embd)\n    self.attn.set_sub_network(\n        self.sub_network_n_embd,\n        self.sub_network_num_heads,\n        sub_network_query_groups,\n        sub_network_head_size,\n    )\n    if isinstance(self.post_attention_norm, LayerNorm) or isinstance(\n        self.post_attention_norm, RMSNorm\n    ):\n        self.post_attention_norm.set_sub_network(self.sub_network_n_embd)\n    if not self.config.shared_attention_norm and self.norm_2 is not None:\n        self.norm_2.set_sub_network(self.sub_network_n_embd)\n    self.mlp.set_sub_network(\n        self.sub_network_n_embd, self.sub_network_intermediate_size\n    )\n    if isinstance(self.post_mlp_norm, LayerNorm) or isinstance(\n        self.post_mlp_norm, RMSNorm\n    ):\n        self.post_mlp_norm.set_sub_network(self.sub_network_n_embd)\n</code></pre>"},{"location":"api/whittle/modules/embedding/","title":"Embedding","text":""},{"location":"api/whittle/modules/embedding/#whittle.modules.embedding","title":"whittle.modules.embedding","text":""},{"location":"api/whittle/modules/embedding/#whittle.modules.embedding.Embedding","title":"Embedding","text":"<pre><code>Embedding(\n    num_embeddings: int,\n    embedding_dim: int,\n    padding_idx: int | None = None,\n    max_norm: float | None = None,\n    norm_type: float = 2.0,\n    scale_grad_by_freq: bool = False,\n    sparse: bool = False,\n    device=None,\n    dtype=None,\n)\n</code></pre> <p>               Bases: <code>Embedding</code></p> <p>An extension of PyTorch's torch.nn.Embedding with support to sub-sample weights corresponding to the sub-network dimensionality</p> Source code in <code>whittle/modules/embedding.py</code> <pre><code>def __init__(\n    self,\n    num_embeddings: int,\n    embedding_dim: int,\n    padding_idx: int | None = None,\n    max_norm: float | None = None,\n    norm_type: float = 2.0,\n    scale_grad_by_freq: bool = False,\n    sparse: bool = False,\n    device=None,\n    dtype=None,\n) -&gt; None:\n    super().__init__(\n        num_embeddings,\n        embedding_dim,\n        padding_idx,\n        max_norm,\n        norm_type,\n        scale_grad_by_freq,\n        sparse,\n        device,\n        dtype,\n    )\n\n    # the embedding dimensionality of the current sub-network\n    self.sub_network_embedding_dim: int | None = embedding_dim\n</code></pre>"},{"location":"api/whittle/modules/embedding/#whittle.modules.embedding.Embedding.reset_super_network","title":"reset_super_network","text":"<pre><code>reset_super_network()\n</code></pre> <p>Reset the embedding dimensionality of the current sub-network to the super-network dimensionality</p> Source code in <code>whittle/modules/embedding.py</code> <pre><code>def reset_super_network(self):\n    \"\"\"Reset the embedding dimensionality of the current sub-network to the super-network dimensionality\"\"\"\n    self.sub_network_embedding_dim = self.embedding_dim\n</code></pre>"},{"location":"api/whittle/modules/embedding/#whittle.modules.embedding.Embedding.set_sub_network","title":"set_sub_network","text":"<pre><code>set_sub_network(sub_network_embedding_dim: int)\n</code></pre> <p>Set the embedding dimensionality of the current sub-network.</p> Source code in <code>whittle/modules/embedding.py</code> <pre><code>def set_sub_network(self, sub_network_embedding_dim: int):\n    \"\"\"Set the embedding dimensionality of the current sub-network.\"\"\"\n    self.sub_network_embedding_dim = sub_network_embedding_dim\n</code></pre>"},{"location":"api/whittle/modules/layernorm/","title":"Layernorm","text":""},{"location":"api/whittle/modules/layernorm/#whittle.modules.layernorm","title":"whittle.modules.layernorm","text":""},{"location":"api/whittle/modules/layernorm/#whittle.modules.layernorm.LayerNorm","title":"LayerNorm","text":"<pre><code>LayerNorm(in_features: int, eps: float = 1e-05)\n</code></pre> <p>               Bases: <code>LayerNorm</code></p> <p>An extension of PyTorch's <code>torch.nn.LayerNorm</code> with support  with support to sub-sample weights corresponding to the sub-network dimensionality.</p> Source code in <code>whittle/modules/layernorm.py</code> <pre><code>def __init__(self, in_features: int, eps: float = 1e-5):\n    super().__init__(in_features, eps)\n    self.in_features = in_features\n\n    # Set current sub-network to super-network\n    self.sub_network_in_features = self.in_features\n</code></pre>"},{"location":"api/whittle/modules/layernorm/#whittle.modules.layernorm.LayerNorm.reset_super_network","title":"reset_super_network","text":"<pre><code>reset_super_network()\n</code></pre> <p>Reset the input dimensionality of the current sub-network to the super-network dimensionality.</p> Source code in <code>whittle/modules/layernorm.py</code> <pre><code>def reset_super_network(self):\n    \"\"\"Reset the input dimensionality of the current sub-network to the super-network dimensionality.\"\"\"\n    self.sub_network_in_features = self.in_features\n</code></pre>"},{"location":"api/whittle/modules/layernorm/#whittle.modules.layernorm.LayerNorm.set_sub_network","title":"set_sub_network","text":"<pre><code>set_sub_network(sub_network_in_features: int)\n</code></pre> <p>Set the input dimensionality of the current sub-network.</p> Source code in <code>whittle/modules/layernorm.py</code> <pre><code>def set_sub_network(self, sub_network_in_features: int):\n    \"\"\"Set the input dimensionality of the current sub-network.\"\"\"\n    self.sub_network_in_features = sub_network_in_features\n</code></pre>"},{"location":"api/whittle/modules/linear/","title":"Linear","text":""},{"location":"api/whittle/modules/linear/#whittle.modules.linear","title":"whittle.modules.linear","text":""},{"location":"api/whittle/modules/linear/#whittle.modules.linear.Linear","title":"Linear","text":"<pre><code>Linear(\n    in_features: int,\n    out_features: int,\n    bias: bool = True,\n    device=None,\n    dtype=None,\n)\n</code></pre> <p>               Bases: <code>Linear</code></p> <p>An extension of PyTorch's torch.nn.Linear with flexible input and output dimensionality corresponding to sub-network</p> Source code in <code>whittle/modules/linear.py</code> <pre><code>def __init__(\n    self,\n    in_features: int,\n    out_features: int,\n    bias: bool = True,\n    device=None,\n    dtype=None,\n):\n    super().__init__(in_features, out_features, bias, device, dtype)\n\n    # Set the current sub-network dimensions equal to super-network\n    self.sub_network_in_features = in_features\n    self.sub_network_out_features = out_features\n    self.use_bias = bias\n</code></pre>"},{"location":"api/whittle/modules/linear/#whittle.modules.linear.Linear.reset_super_network","title":"reset_super_network","text":"<pre><code>reset_super_network()\n</code></pre> <p>Reset the linear transformation dimensions of the current sub-network to the super-network dimensionality.</p> Source code in <code>whittle/modules/linear.py</code> <pre><code>def reset_super_network(self):\n    \"\"\"Reset the linear transformation dimensions of the current sub-network to the super-network dimensionality.\"\"\"\n    self.sub_network_in_features = self.in_features\n    self.sub_network_out_features = self.out_features\n</code></pre>"},{"location":"api/whittle/modules/linear/#whittle.modules.linear.Linear.set_sub_network","title":"set_sub_network","text":"<pre><code>set_sub_network(\n    sub_network_in_features: int,\n    sub_network_out_features: int,\n)\n</code></pre> <p>Set the linear transformation dimensions of the current sub-network.</p> Source code in <code>whittle/modules/linear.py</code> <pre><code>def set_sub_network(\n    self, sub_network_in_features: int, sub_network_out_features: int\n):\n    \"\"\"Set the linear transformation dimensions of the current sub-network.\"\"\"\n    self.sub_network_in_features = sub_network_in_features\n    self.sub_network_out_features = sub_network_out_features\n</code></pre>"},{"location":"api/whittle/modules/linear/#whittle.modules.linear.LinearProj","title":"LinearProj","text":"<pre><code>LinearProj(\n    in_features: int,\n    out_features: int,\n    bias: bool = True,\n    device=None,\n    dtype=None,\n)\n</code></pre> <p>               Bases: <code>Linear</code></p> <p>An extension of Linear to support Projection Indexing</p> Source code in <code>whittle/modules/linear.py</code> <pre><code>def __init__(\n    self,\n    in_features: int,\n    out_features: int,\n    bias: bool = True,\n    device=None,\n    dtype=None,\n):\n    super().__init__(in_features, out_features, bias, device, dtype)\n\n    # Set the current sub-network dimensions equal to super-network\n    self.sub_network_in_features = in_features\n    self.sub_network_out_features = out_features\n    self.use_bias = bias\n    self.proj_indices = None\n</code></pre>"},{"location":"api/whittle/modules/linear/#whittle.modules.linear.LinearProj.reset_super_network","title":"reset_super_network","text":"<pre><code>reset_super_network()\n</code></pre> <p>Reset the linear transformation dimensions of the current sub-network to the super-network dimensionality.</p> Source code in <code>whittle/modules/linear.py</code> <pre><code>def reset_super_network(self):\n    \"\"\"Reset the linear transformation dimensions of the current sub-network to the super-network dimensionality.\"\"\"\n    self.sub_network_in_features = self.in_features\n    self.sub_network_out_features = self.out_features\n    self.proj_indices = None\n</code></pre>"},{"location":"api/whittle/modules/linear/#whittle.modules.linear.LinearProj.set_sub_network","title":"set_sub_network","text":"<pre><code>set_sub_network(\n    sub_network_in_features: int,\n    sub_network_out_features: int,\n    proj_indices: Tensor,\n)\n</code></pre> <p>Set the linear transformation dimensions of the current sub-network.</p> Source code in <code>whittle/modules/linear.py</code> <pre><code>def set_sub_network(\n    self,\n    sub_network_in_features: int,\n    sub_network_out_features: int,\n    proj_indices: torch.Tensor,\n):\n    \"\"\"Set the linear transformation dimensions of the current sub-network.\"\"\"\n    self.sub_network_in_features = sub_network_in_features\n    self.sub_network_out_features = sub_network_out_features\n    self.proj_indices = proj_indices\n</code></pre>"},{"location":"api/whittle/modules/linear/#whittle.modules.linear.LinearQKV","title":"LinearQKV","text":"<pre><code>LinearQKV(\n    in_features: int,\n    out_features: int,\n    bias: bool = True,\n    device=None,\n    dtype=None,\n)\n</code></pre> <p>               Bases: <code>Linear</code></p> <p>An extension of Linear to support QKV Indexing</p> Source code in <code>whittle/modules/linear.py</code> <pre><code>def __init__(\n    self,\n    in_features: int,\n    out_features: int,\n    bias: bool = True,\n    device=None,\n    dtype=None,\n):\n    super().__init__(in_features, out_features, bias, device, dtype)\n\n    # Set the current sub-network dimensions equal to super-network\n    self.sub_network_in_features = in_features\n    self.sub_network_out_features = out_features\n    self.use_bias = bias\n    self.qkv_indices = None\n</code></pre>"},{"location":"api/whittle/modules/linear/#whittle.modules.linear.LinearQKV.reset_super_network","title":"reset_super_network","text":"<pre><code>reset_super_network()\n</code></pre> <p>Reset the linear transformation dimensions of the current sub-network to the super-network dimensionality.</p> Source code in <code>whittle/modules/linear.py</code> <pre><code>def reset_super_network(self):\n    \"\"\"Reset the linear transformation dimensions of the current sub-network to the super-network dimensionality.\"\"\"\n    self.sub_network_in_features = self.in_features\n    self.sub_network_out_features = self.out_features\n    self.qkv_indices = None\n</code></pre>"},{"location":"api/whittle/modules/linear/#whittle.modules.linear.LinearQKV.set_sub_network","title":"set_sub_network","text":"<pre><code>set_sub_network(\n    sub_network_in_features: int,\n    sub_network_out_features: int,\n    qkv_indices=None,\n    sub_network_n_head=None,\n    sub_network_query_groups=None,\n    sub_network_head_size=None,\n    sub_network_q_per_kv=None,\n)\n</code></pre> <p>Set the linear transformation dimensions of the current sub-network.</p> Source code in <code>whittle/modules/linear.py</code> <pre><code>def set_sub_network(\n    self,\n    sub_network_in_features: int,\n    sub_network_out_features: int,\n    qkv_indices=None,\n    sub_network_n_head=None,\n    sub_network_query_groups=None,\n    sub_network_head_size=None,\n    sub_network_q_per_kv=None,\n):\n    \"\"\"Set the linear transformation dimensions of the current sub-network.\"\"\"\n    self.sub_network_in_features = sub_network_in_features\n    self.sub_network_out_features = sub_network_out_features\n    self.qkv_indices = qkv_indices\n</code></pre>"},{"location":"api/whittle/modules/rmsnorm/","title":"Rmsnorm","text":""},{"location":"api/whittle/modules/rmsnorm/#whittle.modules.rmsnorm","title":"whittle.modules.rmsnorm","text":""},{"location":"api/whittle/modules/rmsnorm/#whittle.modules.rmsnorm.RMSNorm","title":"RMSNorm","text":"<pre><code>RMSNorm(\n    in_features: int,\n    dim: int = -1,\n    eps: float = 1e-06,\n    add_unit_offset: bool = False,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Root Mean Square Layer Normalization.</p> <p>Derived from github.com/bzhangGo/rmsnorm/blob/master/rmsnorm_torch.py. BSD 3-Clause License: github.com/bzhangGo/rmsnorm/blob/master/LICENSE.</p> Source code in <code>whittle/modules/rmsnorm.py</code> <pre><code>def __init__(\n    self,\n    in_features: int,\n    dim: int = -1,\n    eps: float = 1e-6,\n    add_unit_offset: bool = False,\n) -&gt; None:\n    super().__init__()\n    self.in_features = in_features\n    self.weight = torch.nn.Parameter(torch.ones(in_features))\n    self.eps = eps\n    self.dim = dim\n    self.add_unit_offset = add_unit_offset\n    self.sub_network_in_features: int | None = in_features\n</code></pre>"},{"location":"api/whittle/pruning/pruners/base_pruner/","title":"Base pruner","text":""},{"location":"api/whittle/pruning/pruners/base_pruner/#whittle.pruning.pruners.base_pruner","title":"whittle.pruning.pruners.base_pruner","text":""},{"location":"api/whittle/pruning/pruners/base_pruner/#whittle.pruning.pruners.base_pruner.Pruner","title":"Pruner","text":""},{"location":"api/whittle/pruning/pruners/base_pruner/#whittle.pruning.pruners.base_pruner.Pruner.__call__","title":"__call__","text":"<pre><code>__call__(\n    model: GPT,\n    prune_n: int = 2,\n    prune_m: int = 4,\n    **kwargs: Any\n) -&gt; float\n</code></pre> <p>Generic pruning interface that handles structural pruning methods, such as WANDA or magnitude-based pruning.</p> <p>Args:      model: The model to be pruned.      prune_n: Number of weights to prune per group.      prune_m: Total number of weights per group.      **kwargs: Additional arguments specific to Wanda and SparseGPT.</p> <p>Returns:      float: The sparsity ratio of the pruned model.</p> Source code in <code>whittle/pruning/pruners/base_pruner.py</code> <pre><code>def __call__(\n    self,\n    model: GPT,\n    prune_n: int = 2,\n    prune_m: int = 4,\n    **kwargs: Any,\n) -&gt; float:\n    \"\"\"\n    Generic pruning interface that handles structural pruning methods, such as WANDA or magnitude-based pruning.\n\n     Args:\n         model: The model to be pruned.\n         prune_n: Number of weights to prune per group.\n         prune_m: Total number of weights per group.\n         **kwargs: Additional arguments specific to Wanda and SparseGPT.\n\n     Returns:\n         float: The sparsity ratio of the pruned model.\n    \"\"\"\n\n    model.reset_super_network()\n    total_parameters = self.compute_parameters(model.transformer.h)\n    self._prune(model, prune_n, prune_m, **kwargs)\n    return self.count_sparse_parameters(model.transformer.h) / total_parameters\n</code></pre>"},{"location":"api/whittle/pruning/pruners/base_pruner/#whittle.pruning.pruners.base_pruner.Pruner.compute_parameters","title":"compute_parameters  <code>staticmethod</code>","text":"<pre><code>compute_parameters(model: GPT) -&gt; int\n</code></pre> <p>Compute the total number of parameters in the model.</p> Source code in <code>whittle/pruning/pruners/base_pruner.py</code> <pre><code>@staticmethod\ndef compute_parameters(model: GPT) -&gt; int:\n    \"\"\"\n    Compute the total number of parameters in the model.\n    \"\"\"\n    return sum(p.numel() for p in model.parameters())\n</code></pre>"},{"location":"api/whittle/pruning/pruners/base_pruner/#whittle.pruning.pruners.base_pruner.Pruner.count_sparse_parameters","title":"count_sparse_parameters  <code>staticmethod</code>","text":"<pre><code>count_sparse_parameters(model: GPT) -&gt; float\n</code></pre> <p>Count the number of non-zero parameters in the model.</p> Source code in <code>whittle/pruning/pruners/base_pruner.py</code> <pre><code>@staticmethod\ndef count_sparse_parameters(model: GPT) -&gt; float:\n    \"\"\"\n    Count the number of non-zero parameters in the model.\n    \"\"\"\n    params = 0\n    for _, p in model.named_parameters():\n        params += torch.sum(p.data != 0).item()\n    return float(params)\n</code></pre>"},{"location":"api/whittle/pruning/pruners/magnitude/","title":"Magnitude","text":""},{"location":"api/whittle/pruning/pruners/magnitude/#whittle.pruning.pruners.magnitude","title":"whittle.pruning.pruners.magnitude","text":""},{"location":"api/whittle/pruning/pruners/magnitude/#whittle.pruning.pruners.magnitude.MagnitudePruner","title":"MagnitudePruner","text":"<p>               Bases: <code>Pruner</code></p>"},{"location":"api/whittle/pruning/pruners/magnitude/#whittle.pruning.pruners.magnitude.MagnitudePruner.compute_parameters","title":"compute_parameters  <code>staticmethod</code>","text":"<pre><code>compute_parameters(model: GPT) -&gt; int\n</code></pre> <p>Compute the total number of parameters in the model.</p> Source code in <code>whittle/pruning/pruners/base_pruner.py</code> <pre><code>@staticmethod\ndef compute_parameters(model: GPT) -&gt; int:\n    \"\"\"\n    Compute the total number of parameters in the model.\n    \"\"\"\n    return sum(p.numel() for p in model.parameters())\n</code></pre>"},{"location":"api/whittle/pruning/pruners/magnitude/#whittle.pruning.pruners.magnitude.MagnitudePruner.count_sparse_parameters","title":"count_sparse_parameters  <code>staticmethod</code>","text":"<pre><code>count_sparse_parameters(model: GPT) -&gt; float\n</code></pre> <p>Count the number of non-zero parameters in the model.</p> Source code in <code>whittle/pruning/pruners/base_pruner.py</code> <pre><code>@staticmethod\ndef count_sparse_parameters(model: GPT) -&gt; float:\n    \"\"\"\n    Count the number of non-zero parameters in the model.\n    \"\"\"\n    params = 0\n    for _, p in model.named_parameters():\n        params += torch.sum(p.data != 0).item()\n    return float(params)\n</code></pre>"},{"location":"api/whittle/pruning/pruners/sparsegpt/","title":"Sparsegpt","text":""},{"location":"api/whittle/pruning/pruners/sparsegpt/#whittle.pruning.pruners.sparsegpt","title":"whittle.pruning.pruners.sparsegpt","text":""},{"location":"api/whittle/pruning/pruners/sparsegpt/#whittle.pruning.pruners.sparsegpt.SparseGPTPruner","title":"SparseGPTPruner","text":"<p>               Bases: <code>Pruner</code></p>"},{"location":"api/whittle/pruning/pruners/sparsegpt/#whittle.pruning.pruners.sparsegpt.SparseGPTPruner.__call__","title":"__call__","text":"<pre><code>__call__(\n    model: GPT,\n    prune_n: int = 2,\n    prune_m: int = 4,\n    **kwargs: Any\n) -&gt; float\n</code></pre> <p>Generic pruning interface that handles structural pruning methods, such as WANDA or magnitude-based pruning.</p> <p>Args:      model: The model to be pruned.      prune_n: Number of weights to prune per group.      prune_m: Total number of weights per group.      **kwargs: Additional arguments specific to Wanda and SparseGPT.</p> <p>Returns:      float: The sparsity ratio of the pruned model.</p> Source code in <code>whittle/pruning/pruners/base_pruner.py</code> <pre><code>def __call__(\n    self,\n    model: GPT,\n    prune_n: int = 2,\n    prune_m: int = 4,\n    **kwargs: Any,\n) -&gt; float:\n    \"\"\"\n    Generic pruning interface that handles structural pruning methods, such as WANDA or magnitude-based pruning.\n\n     Args:\n         model: The model to be pruned.\n         prune_n: Number of weights to prune per group.\n         prune_m: Total number of weights per group.\n         **kwargs: Additional arguments specific to Wanda and SparseGPT.\n\n     Returns:\n         float: The sparsity ratio of the pruned model.\n    \"\"\"\n\n    model.reset_super_network()\n    total_parameters = self.compute_parameters(model.transformer.h)\n    self._prune(model, prune_n, prune_m, **kwargs)\n    return self.count_sparse_parameters(model.transformer.h) / total_parameters\n</code></pre>"},{"location":"api/whittle/pruning/pruners/sparsegpt/#whittle.pruning.pruners.sparsegpt.SparseGPTPruner.compute_parameters","title":"compute_parameters  <code>staticmethod</code>","text":"<pre><code>compute_parameters(model: GPT) -&gt; int\n</code></pre> <p>Compute the total number of parameters in the model.</p> Source code in <code>whittle/pruning/pruners/base_pruner.py</code> <pre><code>@staticmethod\ndef compute_parameters(model: GPT) -&gt; int:\n    \"\"\"\n    Compute the total number of parameters in the model.\n    \"\"\"\n    return sum(p.numel() for p in model.parameters())\n</code></pre>"},{"location":"api/whittle/pruning/pruners/sparsegpt/#whittle.pruning.pruners.sparsegpt.SparseGPTPruner.count_sparse_parameters","title":"count_sparse_parameters  <code>staticmethod</code>","text":"<pre><code>count_sparse_parameters(model: GPT) -&gt; float\n</code></pre> <p>Count the number of non-zero parameters in the model.</p> Source code in <code>whittle/pruning/pruners/base_pruner.py</code> <pre><code>@staticmethod\ndef count_sparse_parameters(model: GPT) -&gt; float:\n    \"\"\"\n    Count the number of non-zero parameters in the model.\n    \"\"\"\n    params = 0\n    for _, p in model.named_parameters():\n        params += torch.sum(p.data != 0).item()\n    return float(params)\n</code></pre>"},{"location":"api/whittle/pruning/pruners/wanda/","title":"Wanda","text":""},{"location":"api/whittle/pruning/pruners/wanda/#whittle.pruning.pruners.wanda","title":"whittle.pruning.pruners.wanda","text":""},{"location":"api/whittle/pruning/pruners/wanda/#whittle.pruning.pruners.wanda.WandaPruner","title":"WandaPruner","text":"<p>               Bases: <code>Pruner</code></p>"},{"location":"api/whittle/pruning/pruners/wanda/#whittle.pruning.pruners.wanda.WandaPruner.__call__","title":"__call__","text":"<pre><code>__call__(\n    model: GPT,\n    prune_n: int = 2,\n    prune_m: int = 4,\n    **kwargs: Any\n) -&gt; float\n</code></pre> <p>Generic pruning interface that handles structural pruning methods, such as WANDA or magnitude-based pruning.</p> <p>Args:      model: The model to be pruned.      prune_n: Number of weights to prune per group.      prune_m: Total number of weights per group.      **kwargs: Additional arguments specific to Wanda and SparseGPT.</p> <p>Returns:      float: The sparsity ratio of the pruned model.</p> Source code in <code>whittle/pruning/pruners/base_pruner.py</code> <pre><code>def __call__(\n    self,\n    model: GPT,\n    prune_n: int = 2,\n    prune_m: int = 4,\n    **kwargs: Any,\n) -&gt; float:\n    \"\"\"\n    Generic pruning interface that handles structural pruning methods, such as WANDA or magnitude-based pruning.\n\n     Args:\n         model: The model to be pruned.\n         prune_n: Number of weights to prune per group.\n         prune_m: Total number of weights per group.\n         **kwargs: Additional arguments specific to Wanda and SparseGPT.\n\n     Returns:\n         float: The sparsity ratio of the pruned model.\n    \"\"\"\n\n    model.reset_super_network()\n    total_parameters = self.compute_parameters(model.transformer.h)\n    self._prune(model, prune_n, prune_m, **kwargs)\n    return self.count_sparse_parameters(model.transformer.h) / total_parameters\n</code></pre>"},{"location":"api/whittle/pruning/pruners/wanda/#whittle.pruning.pruners.wanda.WandaPruner.compute_parameters","title":"compute_parameters  <code>staticmethod</code>","text":"<pre><code>compute_parameters(model: GPT) -&gt; int\n</code></pre> <p>Compute the total number of parameters in the model.</p> Source code in <code>whittle/pruning/pruners/base_pruner.py</code> <pre><code>@staticmethod\ndef compute_parameters(model: GPT) -&gt; int:\n    \"\"\"\n    Compute the total number of parameters in the model.\n    \"\"\"\n    return sum(p.numel() for p in model.parameters())\n</code></pre>"},{"location":"api/whittle/pruning/pruners/wanda/#whittle.pruning.pruners.wanda.WandaPruner.count_sparse_parameters","title":"count_sparse_parameters  <code>staticmethod</code>","text":"<pre><code>count_sparse_parameters(model: GPT) -&gt; float\n</code></pre> <p>Count the number of non-zero parameters in the model.</p> Source code in <code>whittle/pruning/pruners/base_pruner.py</code> <pre><code>@staticmethod\ndef count_sparse_parameters(model: GPT) -&gt; float:\n    \"\"\"\n    Count the number of non-zero parameters in the model.\n    \"\"\"\n    params = 0\n    for _, p in model.named_parameters():\n        params += torch.sum(p.data != 0).item()\n    return float(params)\n</code></pre>"},{"location":"api/whittle/pruning/utils/catcher/","title":"Catcher","text":""},{"location":"api/whittle/pruning/utils/catcher/#whittle.pruning.utils.catcher","title":"whittle.pruning.utils.catcher","text":""},{"location":"api/whittle/pruning/utils/catcher/#whittle.pruning.utils.catcher.Catcher","title":"Catcher","text":"<pre><code>Catcher(module: Module, inps: Tensor, cache: dict)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>A helper module that intercepts inputs for calibration purposes.</p> PARAMETER DESCRIPTION <code>module</code> <p>The original layer to wrap.</p> <p> TYPE: <code>Module</code> </p> <code>inps</code> <p>Tensor to store intercepted inputs.</p> <p> TYPE: <code>Tensor</code> </p> <code>cache</code> <p>A dictionary to store intermediary states.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>whittle/pruning/utils/catcher.py</code> <pre><code>def __init__(self, module: nn.Module, inps: torch.Tensor, cache: dict):\n    super().__init__()\n    self.module = module\n    self.inps = inps\n    self.cache = cache\n</code></pre>"},{"location":"api/whittle/pruning/utils/catcher/#whittle.pruning.utils.catcher.Catcher.forward","title":"forward","text":"<pre><code>forward(\n    inp: Tensor,\n    cos: Tensor,\n    sin: Tensor,\n    mask: Tensor,\n    input_pos: Tensor,\n) -&gt; None\n</code></pre> <p>Forward pass that captures the input data.</p> PARAMETER DESCRIPTION <code>inp</code> <p>Input tensor.</p> <p> TYPE: <code>Tensor</code> </p> <code>cos</code> <p>Rotary embeddings.</p> <p> TYPE: <code>Tensor</code> </p> <code>sin</code> <p>Rotary embeddings.</p> <p> TYPE: <code>Tensor</code> </p> <code>mask</code> <p>Attention mask tensor.</p> <p> TYPE: <code>Tensor</code> </p> <code>input_pos</code> <p>Position IDs.</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>whittle/pruning/utils/catcher.py</code> <pre><code>def forward(\n    self,\n    inp: torch.Tensor,\n    cos: torch.Tensor,\n    sin: torch.Tensor,\n    mask: torch.Tensor,\n    input_pos: torch.Tensor,\n) -&gt; None:\n    \"\"\"\n    Forward pass that captures the input data.\n\n    Args:\n        inp: Input tensor.\n        cos: Rotary embeddings.\n        sin: Rotary embeddings.\n        mask: Attention mask tensor.\n        input_pos: Position IDs.\n    \"\"\"\n    self.inps[self.cache[\"i\"]] = inp\n    self.cache[\"i\"] += 1\n    self.cache[\"attention_mask\"] = mask\n    self.cache[\"position_ids\"] = input_pos\n    raise ValueError  # Trigger exception to exit\n</code></pre>"},{"location":"api/whittle/pruning/utils/layerwrapper/","title":"Layerwrapper","text":""},{"location":"api/whittle/pruning/utils/layerwrapper/#whittle.pruning.utils.layerwrapper","title":"whittle.pruning.utils.layerwrapper","text":""},{"location":"api/whittle/pruning/utils/layerwrapper/#whittle.pruning.utils.layerwrapper.WrappedGPT","title":"WrappedGPT","text":"<pre><code>WrappedGPT(\n    layer: Module,\n    layer_id: int = 0,\n    layer_name: str = \"none\",\n)\n</code></pre> <p>GPT layer wrapper that enables the calculation and updating of scaling factors for pruning. Scaling factors are used to determine the importance of each row in the weight matrix during pruning.</p> Source code in <code>whittle/pruning/utils/layerwrapper.py</code> <pre><code>def __init__(\n    self, layer: nn.Module, layer_id: int = 0, layer_name: str = \"none\"\n) -&gt; None:\n    self.layer = layer\n    self.dev = self.layer.weight.device\n    self.rows = layer.weight.data.shape[0]\n    self.columns = layer.weight.data.shape[1]\n\n    self.scaler_row = torch.zeros((self.columns), device=self.dev)\n    self.nsamples = 0\n\n    self.layer_id = layer_id\n    self.layer_name = layer_name\n</code></pre>"},{"location":"api/whittle/pruning/utils/layerwrapper/#whittle.pruning.utils.layerwrapper.WrappedGPT.add_batch","title":"add_batch","text":"<pre><code>add_batch(inp: Tensor, out: Tensor) -&gt; None\n</code></pre> <p>Computes and updates the L2 norms of the input tensor (row-wise) for each batch passed through the layer. These norms serve as scaling factors (<code>scaler_row</code>).</p> PARAMETER DESCRIPTION <code>inp</code> <p>Input tensor.</p> <p> TYPE: <code>Tensor</code> </p> <code>out</code> <p>Output tensor.</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>whittle/pruning/utils/layerwrapper.py</code> <pre><code>def add_batch(self, inp: torch.Tensor, out: torch.Tensor) -&gt; None:\n    \"\"\"\n\n    Computes and updates the L2 norms of the input tensor (row-wise) for each\n    batch passed through the layer. These norms serve as scaling factors (`scaler_row`).\n\n    Args:\n        inp: Input tensor.\n        out: Output tensor.\n\n    \"\"\"\n    if len(inp.shape) == 2:\n        inp = inp.unsqueeze(0)\n    tmp = inp.shape[0]\n    if isinstance(self.layer, nn.Linear):\n        if len(inp.shape) == 3:\n            inp = inp.reshape((-1, inp.shape[-1]))\n        inp = inp.t()\n\n    self.scaler_row *= self.nsamples / (self.nsamples + tmp)\n    self.nsamples += tmp\n\n    inp = inp.type(torch.float32)\n    self.scaler_row += torch.norm(inp, p=2, dim=1) ** 2 / self.nsamples\n</code></pre>"},{"location":"api/whittle/pruning/utils/sparsegpt/","title":"Sparsegpt","text":""},{"location":"api/whittle/pruning/utils/sparsegpt/#whittle.pruning.utils.sparsegpt","title":"whittle.pruning.utils.sparsegpt","text":""},{"location":"api/whittle/pruning/utils/sparsegpt/#whittle.pruning.utils.sparsegpt.SparseGPT","title":"SparseGPT","text":"<pre><code>SparseGPT(layer: Module)\n</code></pre> <p>Class implementation of the SparseGPT algorithm for pruning GPT models. github.com/IST-DASLab/sparsegpt/tree/f5c25005a61f96a0933ca2f95705a963585aafaa</p> Source code in <code>whittle/pruning/utils/sparsegpt.py</code> <pre><code>def __init__(self, layer: nn.Module):\n    self.layer = layer\n    self.dev = self.layer.weight.device\n    W = layer.weight.data.clone()\n    if isinstance(self.layer, nn.Conv2d):\n        W = W.flatten(1)\n    if isinstance(self.layer, transformers.Conv1D):\n        W = W.t()\n    self.rows = W.shape[0]\n    self.columns = W.shape[1]\n    self.H = torch.zeros((self.columns, self.columns), device=self.dev)\n    self.nsamples = 0\n</code></pre>"},{"location":"api/whittle/pruning/utils/sparsegpt/#whittle.pruning.utils.sparsegpt.SparseGPT.add_batch","title":"add_batch","text":"<pre><code>add_batch(inp: Tensor, out: Tensor) -&gt; None\n</code></pre> <p>Add a batch of input and output tensors to the layer and update the Hessian matrix.</p> PARAMETER DESCRIPTION <code>inp</code> <p>The input tensor.</p> <p> </p> <code>out</code> <p>The output tensor.</p> <p> </p> Source code in <code>whittle/pruning/utils/sparsegpt.py</code> <pre><code>def add_batch(self, inp: torch.Tensor, out: torch.Tensor) -&gt; None:\n    \"\"\"\n    Add a batch of input and output tensors to the layer and update the Hessian matrix.\n\n    Args:\n        inp : The input tensor.\n        out : The output tensor.\n    \"\"\"\n    if len(inp.shape) == 2:\n        inp = inp.unsqueeze(0)\n    tmp = inp.shape[0]\n    if isinstance(self.layer, nn.Linear) or isinstance(\n        self.layer, transformers.Conv1D\n    ):\n        if len(inp.shape) == 3:\n            inp = inp.reshape((-1, inp.shape[-1]))\n        inp = inp.t()\n    self.H *= self.nsamples / (self.nsamples + tmp)\n    self.nsamples += tmp\n    inp = math.sqrt(2 / self.nsamples) * inp.float()\n    self.H += inp.matmul(inp.t())\n</code></pre>"},{"location":"api/whittle/pruning/utils/sparsegpt/#whittle.pruning.utils.sparsegpt.SparseGPT.fasterprune","title":"fasterprune","text":"<pre><code>fasterprune(\n    sparsity: float | None,\n    prune_n: int = 0,\n    prune_m: int = 0,\n    blocksize: int = 128,\n    percdamp: float = 0.01,\n) -&gt; None\n</code></pre> <p>Perform structured pruning on the model's weights using the SparseGPT algorithm.</p> PARAMETER DESCRIPTION <code>sparsity</code> <p>The target sparsity level, used when prune_n is set to 0.</p> <p> TYPE: <code>float | None</code> </p> <code>prune_n</code> <p>Number of weights to prune per group.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>prune_m</code> <p>Total number of weights per group.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>blocksize</code> <p>The block size for pruning, determines the number of columns from the weight         matrix that are processed at a time during pruning.</p> <p> TYPE: <code>int</code> DEFAULT: <code>128</code> </p> <code>percdamp</code> <p>The percentage of damping to the Hessian matrix.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.01</code> </p> Source code in <code>whittle/pruning/utils/sparsegpt.py</code> <pre><code>def fasterprune(\n    self,\n    sparsity: float | None,\n    prune_n: int = 0,\n    prune_m: int = 0,\n    blocksize: int = 128,\n    percdamp: float = 0.01,\n) -&gt; None:\n    \"\"\"\n    Perform structured pruning on the model's weights using the SparseGPT algorithm.\n\n    Args:\n        sparsity: The target sparsity level, used when prune_n is set to 0.\n        prune_n: Number of weights to prune per group.\n        prune_m: Total number of weights per group.\n        blocksize: The block size for pruning, determines the number of columns from the weight\n                    matrix that are processed at a time during pruning.\n        percdamp: The percentage of damping to the Hessian matrix.\n    \"\"\"\n    W = self.layer.weight.data.clone()\n    if isinstance(self.layer, nn.Conv2d):\n        W = W.flatten(1)\n    if isinstance(self.layer, transformers.Conv1D):\n        W = W.t()\n    W = W.float()\n\n    H = self.H\n    del self.H\n    dead = torch.diag(H) == 0\n    H[dead, dead] = 1\n    W[:, dead] = 0\n\n    Losses = torch.zeros(self.rows, device=self.dev)\n\n    damp = percdamp * torch.mean(torch.diag(H))\n    diag = torch.arange(self.columns, device=self.dev)\n    H[diag, diag] += damp\n    H = torch.linalg.cholesky(H)\n    H = torch.cholesky_inverse(H)\n    H = torch.linalg.cholesky(H, upper=True)\n    Hinv = H\n\n    mask = None\n\n    for i1 in range(0, self.columns, blocksize):\n        i2 = min(i1 + blocksize, self.columns)\n        count = i2 - i1\n\n        W1 = W[:, i1:i2].clone()\n        Q1 = torch.zeros_like(W1)\n        Err1 = torch.zeros_like(W1)\n        Losses1 = torch.zeros_like(W1)\n        Hinv1 = Hinv[i1:i2, i1:i2]\n\n        if prune_n == 0:\n            if mask is not None:\n                mask1 = mask[:, i1:i2]\n            else:\n                tmp = W1**2 / (torch.diag(Hinv1).reshape((1, -1))) ** 2\n                thresh = torch.sort(tmp.flatten())[0][int(tmp.numel() * sparsity)]\n                mask1 = tmp &lt;= thresh\n        else:\n            mask1 = torch.zeros_like(W1) == 1\n\n        for i in range(count):\n            w = W1[:, i]\n            d = Hinv1[i, i]\n\n            if prune_n != 0 and i % prune_m == 0:\n                tmp = (\n                    W1[:, i : (i + prune_m)] ** 2\n                    / (torch.diag(Hinv1)[i : (i + prune_m)].reshape((1, -1))) ** 2\n                )\n                mask1.scatter_(\n                    1, i + torch.topk(tmp, prune_n, dim=1, largest=False)[1], True\n                )\n\n            q = w.clone()\n            q[mask1[:, i]] = 0\n\n            Q1[:, i] = q\n            Losses1[:, i] = (w - q) ** 2 / d**2\n\n            err1 = (w - q) / d\n            W1[:, i:] -= err1.unsqueeze(1).matmul(Hinv1[i, i:].unsqueeze(0))\n            Err1[:, i] = err1\n\n        W[:, i1:i2] = Q1\n        Losses += torch.sum(Losses1, 1) / 2\n\n        W[:, i2:] -= Err1.matmul(Hinv[i1:i2, i2:])\n\n    # torch.cuda.synchronize()\n    if isinstance(self.layer, transformers.Conv1D):\n        W = W.t()\n    self.layer.weight.data = W.reshape(self.layer.weight.shape).to(\n        self.layer.weight.data.dtype\n    )\n</code></pre>"},{"location":"api/whittle/sampling/grid_samplers/","title":"Grid samplers","text":""},{"location":"api/whittle/sampling/grid_samplers/#whittle.sampling.grid_samplers","title":"whittle.sampling.grid_samplers","text":""},{"location":"api/whittle/sampling/grid_samplers/#whittle.sampling.grid_samplers.FixedParamGridSampler","title":"FixedParamGridSampler","text":"<pre><code>FixedParamGridSampler(\n    search_space: dict[str, Any] | Any,\n    num_configs: int = 21,\n    n_trials: int = 5000,\n    seed: int | None = None,\n    cast_search_space: bool = True,\n)\n</code></pre> <p>               Bases: <code>RandomSampler</code></p> <p>FixedParamGridSampler creates a fixed grid of sampled configurations, each at a different parameter range. It samples uniformly just as StratifiedRandomSampler.</p> PARAMETER DESCRIPTION <code>search_space</code> <p>The search space from which to sample.</p> <p> TYPE: <code>dict[str, Any] | Any</code> </p> <code>num_configs</code> <p>The number of configurations to sample for the grid.</p> <p> TYPE: <code>int</code> DEFAULT: <code>21</code> </p> <code>n_trials</code> <p>The number of trials to sample before an error is thrown (not possible to find a network in a param range).</p> <p> TYPE: <code>int</code> DEFAULT: <code>5000</code> </p> <code>seed</code> <p>Seed for the random number generator. Defaults to None.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>cast_search_space</code> <p>Whether to cast the search space. Defaults to True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>whittle/sampling/grid_samplers.py</code> <pre><code>def __init__(\n    self,\n    search_space: dict[str, Any] | Any,\n    num_configs: int = 21,\n    n_trials: int = 5000,\n    seed: int | None = None,\n    cast_search_space: bool = True,\n):\n    super().__init__(search_space, seed=seed, cast_search_space=cast_search_space)\n    self.search_space = search_space\n    self.n_trials = n_trials\n    self.seed = seed\n\n    self.args = ParamBinArgs()\n    self.args.empty_bin_tolerance = 0\n    self.args.num_bins = num_configs\n    self.args.start_bin_size = 1\n\n    self.grid: list[dict[str, Any]] = []\n</code></pre>"},{"location":"api/whittle/sampling/grid_samplers/#whittle.sampling.grid_samplers.StratifiedRandomSampler","title":"StratifiedRandomSampler","text":"<pre><code>StratifiedRandomSampler(\n    search_space: dict[str, Any] | Any,\n    params_estimator: Callable,\n    seed: int | None = None,\n    param_bins: ParamBinArgs | None = None,\n    max_tries: int = 10000,\n    cast_search_space: bool = True,\n)\n</code></pre> <p>               Bases: <code>RandomSampler</code></p> <p>StratifiedRandomSampler samples configurations from a given search space using a random state. It maintains a set of bins to ensure that the configurations are sampled uniformly based on their parameter count.</p> PARAMETER DESCRIPTION <code>search_space</code> <p>The search space from which to sample.</p> <p> TYPE: <code>dict[str, Any] | Any</code> </p> <code>seed</code> <p>Seed for the random number generator. Defaults to None.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>param_bins</code> <p>The parameter bins that limit the sub-network params in the search.</p> <p> TYPE: <code>ParamBinArgs | None</code> DEFAULT: <code>None</code> </p> <code>cast_search_space</code> <p>Whether to cast the search space to config aligned with arguments of GPT.set_sub_network(). Defaults to True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>whittle/sampling/grid_samplers.py</code> <pre><code>def __init__(\n    self,\n    search_space: dict[str, Any] | Any,\n    params_estimator: Callable,\n    seed: int | None = None,\n    param_bins: ParamBinArgs | None = None,\n    max_tries: int = 10000,\n    cast_search_space: bool = True,\n):\n    param_bins = param_bins if param_bins is not None else ParamBinArgs()\n    super().__init__(search_space, seed=seed, cast_search_space=cast_search_space)\n    self.param_bins = ParamBins(\n        self.get_smallest_sub_network(),\n        self.get_largest_sub_network(),\n        params_estimator,\n        num_bins=param_bins.num_bins,\n        log_bins=param_bins.log_bins,\n        start_bin_size=param_bins.start_bin_size,\n        empty_bin_tolerance=param_bins.empty_bin_tolerance,\n    )\n\n    self.max_tries = max_tries\n</code></pre>"},{"location":"api/whittle/sampling/grid_samplers/#whittle.sampling.grid_samplers.StratifiedRandomSampler.get_largest_sub_network","title":"get_largest_sub_network","text":"<pre><code>get_largest_sub_network() -&gt; dict[str, Any]\n</code></pre> <p>gets the largest sub-network configuration from the search space.</p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>The largest sub-network configuration.</p> Source code in <code>whittle/sampling/random_sampler.py</code> <pre><code>def get_largest_sub_network(self) -&gt; dict[str, Any]:\n    \"\"\"\n    gets the largest sub-network configuration from the search space.\n\n    Returns:\n        The largest sub-network configuration.\n    \"\"\"\n\n    config = {}\n    for k, v in self.search_space.config_space.items():\n        if isinstance(v, Domain):\n            if isinstance(v, Categorical):\n                if all(isinstance(e, (int, float)) for e in v.categories):\n                    config[k] = max(v.categories)\n                else:\n                    warnings.warn(\n                        \"Warning: Categoricals are non-integers, check if largest network is as intended\"\n                    )\n                    config[k] = v.categories[-1]\n            else:\n                config[k] = v.upper\n\n    return self.search_space.cast(config) if self.cast_search_space else config\n</code></pre>"},{"location":"api/whittle/sampling/grid_samplers/#whittle.sampling.grid_samplers.StratifiedRandomSampler.get_medium_sub_network","title":"get_medium_sub_network","text":"<pre><code>get_medium_sub_network() -&gt; dict[str, Any]\n</code></pre> <p>Gets the medium sub-network configuration from the search space.</p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>The medium sub-network configuration.</p> Source code in <code>whittle/sampling/random_sampler.py</code> <pre><code>def get_medium_sub_network(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Gets the medium sub-network configuration from the search space.\n\n    Returns:\n        The medium sub-network configuration.\n    \"\"\"\n    config = {}\n    for k, v in self.search_space.config_space.items():\n        if isinstance(v, Domain):\n            if isinstance(v, Categorical):\n                if all(isinstance(e, (int, float)) for e in v.categories):\n                    config[k] = int(np.median(v.categories))\n                else:\n                    warnings.warn(\n                        \"Warning: Categoricals are non-integers, check if medium network is as intended\"\n                    )\n                    config[k] = v.categories[len(v.categories) // 2]\n            else:\n                config[k] = (v.lower + v.upper) // 2\n\n    return self.search_space.cast(config) if self.cast_search_space else config\n</code></pre>"},{"location":"api/whittle/sampling/grid_samplers/#whittle.sampling.grid_samplers.StratifiedRandomSampler.get_smallest_sub_network","title":"get_smallest_sub_network","text":"<pre><code>get_smallest_sub_network() -&gt; dict[str, Any]\n</code></pre> <p>Gets the smallest sub-network configuration from the search space.</p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>The smallest sub-network configuration.</p> Source code in <code>whittle/sampling/random_sampler.py</code> <pre><code>def get_smallest_sub_network(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Gets the smallest sub-network configuration from the search space.\n\n    Returns:\n        The smallest sub-network configuration.\n    \"\"\"\n    config = {}\n    for k, v in self.search_space.config_space.items():\n        if isinstance(v, Domain):\n            if isinstance(v, Categorical):\n                if all(isinstance(e, (int, float)) for e in v.categories):\n                    config[k] = min(v.categories)\n                else:\n                    warnings.warn(\n                        \"Warning: Categoricals are non-integers, check if smallest network is as intended\"\n                    )\n                    config[k] = v.categories[0]\n            else:\n                config[k] = v.lower\n\n    return self.search_space.cast(config) if self.cast_search_space else config\n</code></pre>"},{"location":"api/whittle/sampling/grid_samplers/#whittle.sampling.grid_samplers.StratifiedRandomSampler.sample","title":"sample","text":"<pre><code>sample() -&gt; dict[str, Any]\n</code></pre> <p>Gets the smallest sub-network configuration from the search space.</p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>The smallest sub-network configuration.</p> Source code in <code>whittle/sampling/grid_samplers.py</code> <pre><code>def sample(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Gets the smallest sub-network configuration from the search space.\n\n    Returns:\n        The smallest sub-network configuration.\n    \"\"\"\n    tries = 0\n    while True:\n        config = super().sample()\n\n        # find a bin for the config, if not found, continue sampling\n        if self.param_bins.put_in_bin(config):\n            break\n\n        if tries &gt; self.max_tries:\n            raise ValueError(\n                \"Could not find a valid configuration in StratifiedRandomSampler. Try increasing max_tries or increasing empty_bin_tolerance.\"\n            )\n    return config\n</code></pre>"},{"location":"api/whittle/sampling/param_bins/","title":"Param bins","text":""},{"location":"api/whittle/sampling/param_bins/#whittle.sampling.param_bins","title":"whittle.sampling.param_bins","text":""},{"location":"api/whittle/sampling/random_sampler/","title":"Random sampler","text":""},{"location":"api/whittle/sampling/random_sampler/#whittle.sampling.random_sampler","title":"whittle.sampling.random_sampler","text":""},{"location":"api/whittle/sampling/random_sampler/#whittle.sampling.random_sampler.BaseSampler","title":"BaseSampler","text":"<pre><code>BaseSampler()\n</code></pre> <p>BaseSampler is the base class that all samplers inherit from.</p> Source code in <code>whittle/sampling/random_sampler.py</code> <pre><code>def __init__(self):\n    self.grid = None\n</code></pre>"},{"location":"api/whittle/sampling/random_sampler/#whittle.sampling.random_sampler.RandomSampler","title":"RandomSampler","text":"<pre><code>RandomSampler(\n    search_space: dict[str, Any] | Any,\n    seed: int | None = None,\n    cast_search_space: bool = True,\n)\n</code></pre> <p>               Bases: <code>BaseSampler</code></p> <p>RandomSampler samples configurations from a given search space using a random state.</p> PARAMETER DESCRIPTION <code>search_space</code> <p>The search space from which to sample.</p> <p> TYPE: <code>dict[str, Any] | Any</code> </p> <code>seed</code> <p>Seed for the random number generator. Defaults to None.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>cast_search_space</code> <p>Whether to cast the search space. Defaults to True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>whittle/sampling/random_sampler.py</code> <pre><code>def __init__(\n    self,\n    search_space: dict[str, Any] | Any,\n    seed: int | None = None,\n    cast_search_space: bool = True,\n):\n    self.search_space = (\n        SimpleSearchSpace(search_space)\n        if isinstance(search_space, dict)\n        else search_space\n    )\n    self.rng = np.random.RandomState(seed)\n    self.cast_search_space = cast_search_space\n</code></pre>"},{"location":"api/whittle/sampling/random_sampler/#whittle.sampling.random_sampler.RandomSampler.get_largest_sub_network","title":"get_largest_sub_network","text":"<pre><code>get_largest_sub_network() -&gt; dict[str, Any]\n</code></pre> <p>gets the largest sub-network configuration from the search space.</p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>The largest sub-network configuration.</p> Source code in <code>whittle/sampling/random_sampler.py</code> <pre><code>def get_largest_sub_network(self) -&gt; dict[str, Any]:\n    \"\"\"\n    gets the largest sub-network configuration from the search space.\n\n    Returns:\n        The largest sub-network configuration.\n    \"\"\"\n\n    config = {}\n    for k, v in self.search_space.config_space.items():\n        if isinstance(v, Domain):\n            if isinstance(v, Categorical):\n                if all(isinstance(e, (int, float)) for e in v.categories):\n                    config[k] = max(v.categories)\n                else:\n                    warnings.warn(\n                        \"Warning: Categoricals are non-integers, check if largest network is as intended\"\n                    )\n                    config[k] = v.categories[-1]\n            else:\n                config[k] = v.upper\n\n    return self.search_space.cast(config) if self.cast_search_space else config\n</code></pre>"},{"location":"api/whittle/sampling/random_sampler/#whittle.sampling.random_sampler.RandomSampler.get_medium_sub_network","title":"get_medium_sub_network","text":"<pre><code>get_medium_sub_network() -&gt; dict[str, Any]\n</code></pre> <p>Gets the medium sub-network configuration from the search space.</p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>The medium sub-network configuration.</p> Source code in <code>whittle/sampling/random_sampler.py</code> <pre><code>def get_medium_sub_network(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Gets the medium sub-network configuration from the search space.\n\n    Returns:\n        The medium sub-network configuration.\n    \"\"\"\n    config = {}\n    for k, v in self.search_space.config_space.items():\n        if isinstance(v, Domain):\n            if isinstance(v, Categorical):\n                if all(isinstance(e, (int, float)) for e in v.categories):\n                    config[k] = int(np.median(v.categories))\n                else:\n                    warnings.warn(\n                        \"Warning: Categoricals are non-integers, check if medium network is as intended\"\n                    )\n                    config[k] = v.categories[len(v.categories) // 2]\n            else:\n                config[k] = (v.lower + v.upper) // 2\n\n    return self.search_space.cast(config) if self.cast_search_space else config\n</code></pre>"},{"location":"api/whittle/sampling/random_sampler/#whittle.sampling.random_sampler.RandomSampler.get_smallest_sub_network","title":"get_smallest_sub_network","text":"<pre><code>get_smallest_sub_network() -&gt; dict[str, Any]\n</code></pre> <p>Gets the smallest sub-network configuration from the search space.</p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>The smallest sub-network configuration.</p> Source code in <code>whittle/sampling/random_sampler.py</code> <pre><code>def get_smallest_sub_network(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Gets the smallest sub-network configuration from the search space.\n\n    Returns:\n        The smallest sub-network configuration.\n    \"\"\"\n    config = {}\n    for k, v in self.search_space.config_space.items():\n        if isinstance(v, Domain):\n            if isinstance(v, Categorical):\n                if all(isinstance(e, (int, float)) for e in v.categories):\n                    config[k] = min(v.categories)\n                else:\n                    warnings.warn(\n                        \"Warning: Categoricals are non-integers, check if smallest network is as intended\"\n                    )\n                    config[k] = v.categories[0]\n            else:\n                config[k] = v.lower\n\n    return self.search_space.cast(config) if self.cast_search_space else config\n</code></pre>"},{"location":"api/whittle/sampling/random_sampler/#whittle.sampling.random_sampler.RandomSampler.sample","title":"sample","text":"<pre><code>sample() -&gt; dict[str, Any]\n</code></pre> <p>Gets a random sub-network configuration from the search space.</p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>A random sub-network configuration.</p> Source code in <code>whittle/sampling/random_sampler.py</code> <pre><code>def sample(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Gets a random sub-network configuration from the search space.\n\n    Returns:\n        A random sub-network configuration.\n    \"\"\"\n    config = {}\n    for hp_name, hparam in self.search_space.config_space.items():\n        if isinstance(hparam, Domain):\n            config[hp_name] = hparam.sample(random_state=self.rng)\n\n    return self.search_space.cast(config) if self.cast_search_space else config\n</code></pre>"},{"location":"api/whittle/sampling/samplers/","title":"Samplers","text":""},{"location":"api/whittle/sampling/samplers/#whittle.sampling.samplers","title":"whittle.sampling.samplers","text":""},{"location":"api/whittle/search/ask_tell_scheduler/","title":"Ask tell scheduler","text":""},{"location":"api/whittle/search/ask_tell_scheduler/#whittle.search.ask_tell_scheduler","title":"whittle.search.ask_tell_scheduler","text":""},{"location":"api/whittle/search/ask_tell_scheduler/#whittle.search.ask_tell_scheduler.AskTellScheduler","title":"AskTellScheduler","text":"<pre><code>AskTellScheduler(base_scheduler: TrialScheduler)\n</code></pre> Source code in <code>whittle/search/ask_tell_scheduler.py</code> <pre><code>def __init__(self, base_scheduler: TrialScheduler):\n    self.bscheduler = base_scheduler\n    self.trial_counter = 0\n    self.completed_experiments = {}\n</code></pre>"},{"location":"api/whittle/search/ask_tell_scheduler/#whittle.search.ask_tell_scheduler.AskTellScheduler.ask","title":"ask","text":"<pre><code>ask() -&gt; Trial\n</code></pre> <p>Ask the scheduler for new trial to run</p> RETURNS DESCRIPTION <code>Trial</code> <p>Trial to run</p> Source code in <code>whittle/search/ask_tell_scheduler.py</code> <pre><code>def ask(self) -&gt; Trial:\n    \"\"\"\n    Ask the scheduler for new trial to run\n\n    Returns:\n        Trial to run\n    \"\"\"\n    trial_suggestion = self.bscheduler.suggest(self.trial_counter)\n    trial = Trial(\n        trial_id=self.trial_counter,\n        config=trial_suggestion.config,\n        creation_time=datetime.datetime.now(),\n    )\n    self.trial_counter += 1\n    return trial\n</code></pre>"},{"location":"api/whittle/search/ask_tell_scheduler/#whittle.search.ask_tell_scheduler.AskTellScheduler.best_trial","title":"best_trial","text":"<pre><code>best_trial(metris: str) -&gt; TrialResult\n</code></pre> RETURNS DESCRIPTION <code>TrialResult</code> <p>the best trial according to the provided metric.</p> Source code in <code>whittle/search/ask_tell_scheduler.py</code> <pre><code>def best_trial(self, metris: str) -&gt; TrialResult:\n    \"\"\"\n    Returns:\n        the best trial according to the provided metric.\n    \"\"\"\n    if self.bscheduler.mode == \"max\":\n        sign = 1.0\n    else:\n        sign = -1.0\n\n    return max(\n        [value for key, value in self.completed_experiments.items()],\n        key=lambda trial: sign * trial.metrics[metris],\n    )\n</code></pre>"},{"location":"api/whittle/search/ask_tell_scheduler/#whittle.search.ask_tell_scheduler.AskTellScheduler.tell","title":"tell","text":"<pre><code>tell(trial: Trial, experiment_result: dict[str, float])\n</code></pre> <p>Feed experiment results back to the Scheduler.</p> PARAMETER DESCRIPTION <code>trial</code> <p>Trial that was run.</p> <p> TYPE: <code>Trial</code> </p> <code>experiment_result</code> <p>{metric: value} dictionary with experiment results.</p> <p> TYPE: <code>dict[str, float]</code> </p> Source code in <code>whittle/search/ask_tell_scheduler.py</code> <pre><code>def tell(self, trial: Trial, experiment_result: dict[str, float]):\n    \"\"\"\n    Feed experiment results back to the Scheduler.\n\n    Args:\n        trial: Trial that was run.\n        experiment_result: {metric: value} dictionary with experiment results.\n\n    \"\"\"\n    trial_result = trial.add_results(\n        metrics=experiment_result,\n        status=Status.completed,\n        training_end_time=datetime.datetime.now(),\n    )\n    self.bscheduler.on_trial_complete(trial=trial, result=experiment_result)\n    self.completed_experiments[trial_result.trial_id] = trial_result\n</code></pre>"},{"location":"api/whittle/search/baselines/","title":"Baselines","text":""},{"location":"api/whittle/search/baselines/#whittle.search.baselines","title":"whittle.search.baselines","text":""},{"location":"api/whittle/search/local_search/","title":"Local search","text":""},{"location":"api/whittle/search/local_search/#whittle.search.local_search","title":"whittle.search.local_search","text":""},{"location":"api/whittle/search/local_search/#whittle.search.local_search.LS","title":"LS","text":"<pre><code>LS(\n    config_space: dict[str, Any],\n    metric: list[str],\n    mode: list[str] | str = \"min\",\n    start_point: dict[str, Any] | None = None,\n    random_seed: int | None = None,\n    points_to_evaluate: list[dict] | None = None,\n    **kwargs: Any\n)\n</code></pre> <p>               Bases: <code>FIFOScheduler</code></p> <p>Local Search Scheduler for hyperparameter optimization.</p> <p>This scheduler uses a local search strategy to explore the configuration space. It extends the FIFOScheduler and integrates with the LocalSearch searcher.</p> PARAMETER DESCRIPTION <code>config_space</code> <p>Configuration space for the evaluation function.</p> <p> TYPE: <code>dict[str, Any]</code> </p> <code>metric</code> <p>List of metrics to optimize.</p> <p> TYPE: <code>list[str]</code> </p> <code>mode</code> <p>Optimization mode, either \"min\" or \"max\". Defaults to \"min\".</p> <p> TYPE: <code>list[str] | str</code> DEFAULT: <code>'min'</code> </p> <code>start_point</code> <p>Starting point for the search. Defaults to None.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>random_seed</code> <p>Random seed for reproducibility. Defaults to None.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>points_to_evaluate</code> <p>Initial points to evaluate. Defaults to None.</p> <p> TYPE: <code>list[dict] | None</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <p>Additional arguments for the FIFOScheduler.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>whittle/search/local_search.py</code> <pre><code>def __init__(\n    self,\n    config_space: dict[str, Any],\n    metric: list[str],\n    mode: list[str] | str = \"min\",\n    start_point: dict[str, Any] | None = None,\n    random_seed: int | None = None,\n    points_to_evaluate: list[dict] | None = None,\n    **kwargs: Any,\n):\n    super().__init__(\n        config_space=config_space,\n        metric=metric,\n        mode=mode,\n        searcher=LocalSearch(\n            config_space=config_space,\n            metric=metric,\n            start_point=start_point,\n            mode=mode,\n            random_seed=random_seed,\n            points_to_evaluate=points_to_evaluate,\n        ),\n        random_seed=random_seed,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/whittle/search/local_search/#whittle.search.local_search.LocalSearch","title":"LocalSearch","text":"<pre><code>LocalSearch(\n    config_space: dict[str, Any],\n    metric: list[str] | str,\n    points_to_evaluate: list[dict] | None = None,\n    start_point: dict | None = None,\n    mode: list[str] | str = \"min\",\n    **kwargs: Any\n)\n</code></pre> <p>               Bases: <code>StochasticSearcher</code></p> <p>Local Search algorithm for hyperparameter optimization.</p> <p>This searcher uses a local search strategy to explore the configuration space. It extends the StochasticSearcher and used searcher input parameter in LS scheduler.</p> PARAMETER DESCRIPTION <code>config_space</code> <p>Configuration space for the evaluation function.</p> <p> TYPE: <code>dict[str, Any]</code> </p> <code>metric</code> <p>List of metrics to optimize.</p> <p> TYPE: <code>list[str] | str</code> </p> <code>points_to_evaluate</code> <p>Initial points to evaluate. Defaults to None.</p> <p> TYPE: <code>list[dict] | None</code> DEFAULT: <code>None</code> </p> <code>start_point</code> <p>Starting point for the search. Defaults to None.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>Optimization mode, either \"min\" or \"max\". Defaults to \"min\".</p> <p> TYPE: <code>list[str] | str</code> DEFAULT: <code>'min'</code> </p> <code>**kwargs</code> <p>Additional arguments for the StochasticSearcher.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>whittle/search/local_search.py</code> <pre><code>def __init__(\n    self,\n    config_space: dict[str, Any],\n    metric: list[str] | str,\n    points_to_evaluate: list[dict] | None = None,\n    start_point: dict | None = None,\n    mode: list[str] | str = \"min\",\n    **kwargs: Any,\n):\n    if start_point is None:\n        self.start_point = {\n            k: v.sample() if isinstance(v, Domain) else v\n            for k, v in config_space.items()\n        }\n    else:\n        self.start_point = start_point\n\n    self._pareto_front: list[PopulationElement] = []\n\n    if points_to_evaluate is None:\n        points_to_evaluate = [self.start_point]\n    else:\n        points_to_evaluate.append(self.start_point)\n\n    super().__init__(\n        config_space,\n        metric,\n        mode=mode,\n        points_to_evaluate=points_to_evaluate,\n        **kwargs,\n    )\n    if isinstance(self._mode, list):\n        self._metric_op: dict[str, Any] = {\n            metric: 1 if mode == \"min\" else -1\n            for metric, mode in zip(metric, self._mode)\n        }\n    else:\n        if self._mode == \"min\":\n            self._metric_op = dict(zip(self._metric, [1.0] * len(self._metric)))\n        elif self._mode == \"max\":\n            self._metric_op = dict(zip(self._metric, [-1.0] * len(self._metric)))\n</code></pre>"},{"location":"api/whittle/search/local_search/#whittle.search.local_search.PopulationElement","title":"PopulationElement  <code>dataclass</code>","text":"<pre><code>PopulationElement(\n    trial_id: int, config: dict, result: dict\n)\n</code></pre> <p>Internal PBT state tracked per-trial.</p>"},{"location":"api/whittle/search/multi_objective/","title":"Multi objective","text":""},{"location":"api/whittle/search/multi_objective/#whittle.search.multi_objective","title":"whittle.search.multi_objective","text":""},{"location":"api/whittle/search/multi_objective/#whittle.search.multi_objective.get_pareto_optimal","title":"get_pareto_optimal","text":"<pre><code>get_pareto_optimal(costs: ndarray) -&gt; NDArray[bool_]\n</code></pre> <p>Find the pareto-optimal point.</p> PARAMETER DESCRIPTION <code>costs</code> <p>array containing the costs for each objective asscoiated with each point (n_points, 2).</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>NDArray[bool_]</code> <p>(n_points, 1) indicator if point is on pareto front or not.</p> Source code in <code>whittle/search/multi_objective.py</code> <pre><code>def get_pareto_optimal(costs: np.ndarray) -&gt; npt.NDArray[np.bool_]:\n    \"\"\"\n    Find the pareto-optimal point.\n\n    Args:\n        costs: array containing the costs for each objective asscoiated with each point (n_points, 2).\n\n    Returns:\n        (n_points, 1) indicator if point is on pareto front or not.\n\n    \"\"\"\n    assert isinstance(costs, np.ndarray)\n    assert costs.ndim == 2\n\n    # first assume all points are pareto optimal\n    is_pareto = np.ones(costs.shape[0], dtype=bool)\n    for i, c in enumerate(costs):\n        if is_pareto[i]:\n            # determine all points that have a smaller cost\n            all_with_lower_costs = np.any(costs &lt; c, axis=1)\n            keep_on_front = np.logical_and(all_with_lower_costs, is_pareto)\n            is_pareto = keep_on_front\n            is_pareto[i] = True  # keep self\n    return is_pareto\n</code></pre>"},{"location":"api/whittle/search/search/","title":"Search","text":""},{"location":"api/whittle/search/search/#whittle.search.search","title":"whittle.search.search","text":""},{"location":"api/whittle/search/search/#whittle.search.search.multi_objective_search","title":"multi_objective_search","text":"<pre><code>multi_objective_search(\n    objective: Callable[..., Any],\n    search_space: dict,\n    search_strategy: str = \"random_search\",\n    num_samples: int = 100,\n    objective_kwargs: dict[str, Any] | None = None,\n    logger: Logger | None = None,\n    seed: int | None = None,\n    param_bins: ParamBins | None = None,\n    objective_1_name: str = \"objective_1\",\n    objective_2_name: str = \"objective_2\",\n    verbose: bool = True,\n) -&gt; dict[str, Any]\n</code></pre> <p>Search for the Pareto-optimal sub-networks using the specified strategy.</p> PARAMETER DESCRIPTION <code>objective</code> <p>The objective function to optimize.</p> <p> TYPE: <code>Callable[..., Any]</code> </p> <code>search_space</code> <p>The search space for the optimization.</p> <p> TYPE: <code>dict</code> </p> <code>search_strategy</code> <p>The search strategy to use. Defaults to \"random_search\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'random_search'</code> </p> <code>num_samples</code> <p>The number of samples to evaluate. Defaults to 100.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>objective_kwargs</code> <p>Keyword arguments for the objective function. Defaults to None.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>logger</code> <p>The lightning logger to send metrics to. Defaults to None.</p> <p> TYPE: <code>Logger | None</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The random seed for reproducibility. Defaults to None.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>param_bins</code> <p>The parameter bins that limit the sub-network params in the search. The configs from ask() are rejected if they fit into a bin that is full. The bin size is increased if all bins are full. Defaults to None.</p> <p> TYPE: <code>ParamBins | None</code> DEFAULT: <code>None</code> </p> <code>objective_1_name</code> <p>The name of the first objective. Defaults to \"objective_1\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'objective_1'</code> </p> <code>objective_2_name</code> <p>The name of the second objective. Defaults to \"objective_2\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'objective_2'</code> </p> <code>verbose</code> <p>Whether to have a verbose tqdm output.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <p>Returns:     The results of the search, including Pareto-optimal solutions.</p> Source code in <code>whittle/search/search.py</code> <pre><code>def multi_objective_search(\n    objective: Callable[..., Any],\n    search_space: dict,\n    search_strategy: str = \"random_search\",\n    num_samples: int = 100,\n    objective_kwargs: dict[str, Any] | None = None,\n    logger: Logger | None = None,\n    seed: int | None = None,\n    param_bins: ParamBins | None = None,\n    objective_1_name: str = \"objective_1\",\n    objective_2_name: str = \"objective_2\",\n    verbose: bool = True,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Search for the Pareto-optimal sub-networks using the specified strategy.\n\n    Args:\n        objective: The objective function to optimize.\n        search_space: The search space for the optimization.\n        search_strategy: The search strategy to use.\n            Defaults to \"random_search\".\n        num_samples: The number of samples to evaluate.\n            Defaults to 100.\n        objective_kwargs: Keyword arguments for the objective function.\n            Defaults to None.\n        logger: The lightning logger to send metrics to.\n            Defaults to None.\n        seed: The random seed for reproducibility.\n            Defaults to None.\n        param_bins: The parameter bins that limit the sub-network params in the search.\n            The configs from ask() are rejected if they fit into a bin that is full.\n            The bin size is increased if all bins are full.\n            Defaults to None.\n        objective_1_name: The name of the first objective.\n            Defaults to \"objective_1\".\n        objective_2_name: The name of the second objective.\n            Defaults to \"objective_2\".\n        verbose: Whether to have a verbose tqdm output.\n    Returns:\n        The results of the search, including Pareto-optimal solutions.\n\n    \"\"\"\n    metrics = [\"objective_1\", \"objective_2\"]\n    if seed is None:\n        seed = np.random.randint(0, 1000000)\n\n    assert search_strategy in methods\n\n    base_scheduler = methods[search_strategy](\n        MethodArguments(\n            config_space=search_space,\n            metrics=metrics,\n            mode=[\"min\", \"min\"],\n            random_seed=seed,\n            param_bins=param_bins,\n        )\n    )\n\n    scheduler = AskTellScheduler(base_scheduler=base_scheduler)\n\n    costs = np.empty((num_samples, 2))\n    runtime: list[float] = []\n    configs: list[dict[str, Any]] = []\n    start_time = time.time()\n\n    for i in tqdm(range(num_samples), disable=not verbose):\n        trial_suggestion = scheduler.ask()\n\n        objective_1, objective_2 = objective(\n            trial_suggestion.config, **(objective_kwargs or {})\n        )\n\n        scheduler.tell(\n            trial_suggestion, {\"objective_1\": objective_1, \"objective_2\": objective_2}\n        )\n\n        # bookkeeping\n        costs[i][0] = float(objective_1)\n        costs[i][1] = float(objective_2)\n        configs.append(trial_suggestion.config)\n\n        runtime.append(time.time() - start_time)\n\n        observation = {\n            \"iteration\": i,\n            objective_1_name: float(objective_1),\n            objective_2_name: float(objective_2),\n            \"runtime\": runtime[-1],\n        }\n\n        if logger is not None:\n            logger.log_metrics(observation)\n    idx = get_pareto_optimal(costs)\n\n    results = {\n        \"costs\": costs,\n        \"configs\": configs,\n        \"runtime\": runtime,\n        \"is_pareto_optimal\": [bool(i) for i in idx],\n    }\n    return results\n</code></pre>"},{"location":"api/whittle/search/search_spaces/","title":"Search spaces","text":""},{"location":"api/whittle/search/search_spaces/#whittle.search.search_spaces","title":"whittle.search.search_spaces","text":""},{"location":"api/whittle/search/stratified_search/","title":"Stratified search","text":""},{"location":"api/whittle/search/stratified_search/#whittle.search.stratified_search","title":"whittle.search.stratified_search","text":""},{"location":"api/whittle/search/stratified_search/#whittle.search.stratified_search.StratifiedRandomSearch","title":"StratifiedRandomSearch","text":"<pre><code>StratifiedRandomSearch(\n    config_space: dict[str, Any],\n    metric: list[str],\n    param_bins: ParamBins,\n    mode: list[str] | str = \"min\",\n    start_point: dict[str, Any] | None = None,\n    random_seed: int | None = None,\n    points_to_evaluate: list[dict] | None = None,\n    **kwargs: Any\n)\n</code></pre> <p>               Bases: <code>FIFOScheduler</code></p> <p>Stratified Random Search (SRS) is a search strategy that samples configurations uniformly at random from the search space. It is a simple baseline for hyperparameter optimization.</p> PARAMETER DESCRIPTION <code>config_space</code> <p>The configuration space to sample from.</p> <p> TYPE: <code>dict[str, Any]</code> </p> <code>metric</code> <p>The metric to optimize.</p> <p> TYPE: <code>list[str]</code> </p> <code>param_bins</code> <p>The parameter bins that limit the sub-network params in the search. The configs from ask() are rejected if they fit into a bin that is full. The bin size is increased if all bins are full.</p> <p> TYPE: <code>ParamBins</code> </p> <code>mode</code> <p>The optimization mode for the metric. Defaults to \"min\".</p> <p> TYPE: <code>list[str] | str</code> DEFAULT: <code>'min'</code> </p> <code>start_point</code> <p>Optional. The starting point for the search. Defaults to None.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> <code>random_seed</code> <p>Optional. The random seed for reproducibility. Defaults to None.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>points_to_evaluate</code> <p>Optional. The initial configurations to evaluate. Defaults to None.</p> <p> TYPE: <code>list[dict] | None</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <p>Additional arguments for the scheduler.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>whittle/search/stratified_search.py</code> <pre><code>def __init__(\n    self,\n    config_space: dict[str, Any],\n    metric: list[str],\n    param_bins: ParamBins,\n    mode: list[str] | str = \"min\",\n    start_point: dict[str, Any] | None = None,\n    random_seed: int | None = None,\n    points_to_evaluate: list[dict] | None = None,\n    **kwargs: Any,\n):\n    super().__init__(\n        config_space=config_space,\n        metric=metric,\n        mode=mode,\n        searcher=StratifiedRandomSearcher(\n            config_space=config_space,\n            metric=metric,\n            start_point=start_point,\n            mode=mode,\n            random_seed=random_seed,\n            points_to_evaluate=points_to_evaluate,\n            param_bins=param_bins,\n        ),\n        random_seed=random_seed,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/whittle/search/stratified_search/#whittle.search.stratified_search.StratifiedRandomSearcher","title":"StratifiedRandomSearcher","text":"<pre><code>StratifiedRandomSearcher(\n    config_space: dict[str, Any],\n    metric: list[str] | str,\n    param_bins: ParamBins,\n    sample_patience: int = 10000,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>RandomSearcher</code></p> <p>Searcher which randomly samples configurations to try next. If a configuration gets in a full bin (we already sampled enough configurations with a similar parameter count), it is rejected and a new configuration is sampled.</p> <pre><code>metric: The metric to optimize.\nparam_bins: The parameter bins that limit the sub-network params in the search.\n    The configs from ask() are rejected if they fit into a bin that is full.\n    The bin size is increased if all bins are full.\nsample_patience: The number of rejected samples to try before raising an error.\n    Defaults to 10000.\n**kwargs: Additional arguments for the searcher.\n</code></pre> Source code in <code>whittle/search/stratified_search.py</code> <pre><code>def __init__(\n    self,\n    config_space: dict[str, Any],\n    metric: list[str] | str,\n    param_bins: ParamBins,\n    sample_patience: int = 10000,\n    **kwargs,\n):\n    \"\"\"\n    Args:\n        config_space: The configuration space to sample from.\n        metric: The metric to optimize.\n        param_bins: The parameter bins that limit the sub-network params in the search.\n            The configs from ask() are rejected if they fit into a bin that is full.\n            The bin size is increased if all bins are full.\n        sample_patience: The number of rejected samples to try before raising an error.\n            Defaults to 10000.\n        **kwargs: Additional arguments for the searcher.\n    \"\"\"\n    super().__init__(\n        config_space,\n        metric=metric,\n        **kwargs,\n    )\n    self.param_bins = param_bins\n    self.sample_patience = sample_patience\n</code></pre>"},{"location":"api/whittle/training_strategies/ats/","title":"Ats","text":""},{"location":"api/whittle/training_strategies/ats/#whittle.training_strategies.ats","title":"whittle.training_strategies.ats","text":""},{"location":"api/whittle/training_strategies/ats/#whittle.training_strategies.ats.ATS","title":"ATS","text":"<pre><code>ATS(random_samples: int = 1, **kwargs: Any)\n</code></pre> <p>               Bases: <code>BaseTrainingStrategy</code></p> <p>ATS strategy.</p> <p>Follows the approach by Mohtashami et al. and updates a set of randomly sampled sub-networks if if the current step is even, otherwise it updates the super-network.</p> refs <p>Masked Training of Neural Networks with Partial Gradients Amirkeivan Mohtashami, Martin Jaggi, Sebastian Stich Proceedings of The 25th International Conference on Artificial Intelligence and Statistics arxiv.org/abs/2106.08895</p> PARAMETER DESCRIPTION <code>random_samples</code> <p>the number of randomly sampled sub-networks to sample and update in each step</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>**kwargs</code> <p>kwargs of <code>BaseTrainingStrategy</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>whittle/training_strategies/ats.py</code> <pre><code>def __init__(self, random_samples: int = 1, **kwargs: Any):\n    \"\"\"\n    Initialises an `ATS` strategy.\n\n    Args:\n        random_samples: the number of randomly sampled sub-networks to sample and update in each step\n        **kwargs: kwargs of `BaseTrainingStrategy`\n    \"\"\"\n    super().__init__(**kwargs)\n    self.random_samples = random_samples\n    self.current_step = 0\n</code></pre>"},{"location":"api/whittle/training_strategies/ats/#whittle.training_strategies.ats.ATS.__call__","title":"__call__","text":"<pre><code>__call__(model, inputs, outputs, scale_loss=1, **kwargs)\n</code></pre> <p>Updates a set of randomly sampled sub-networks if the current step is odd. Else, it updates the super-network.</p> Source code in <code>whittle/training_strategies/ats.py</code> <pre><code>def __call__(self, model, inputs, outputs, scale_loss=1, **kwargs):\n    \"\"\"\n    Updates a set of randomly sampled sub-networks if the current step is odd. Else, it updates the\n    super-network.\n    \"\"\"\n    total_loss = 0\n    y_supernet = model(inputs)\n    if self.current_step % 2 == 0:\n        # update random sub-networks\n        for i in range(self.random_samples):\n            config = self.sampler.sample()\n            model.set_sub_network(**config)\n            y_hat = model(inputs)\n            if self.kd_loss is not None:\n                loss = self.kd_loss(y_hat, outputs, y_supernet)\n            else:\n                loss = self.loss_function(y_hat, outputs)\n            loss *= scale_loss\n            loss.backward() if self.fabric is None else self.fabric.backward(loss)\n            model.reset_super_network()\n\n            total_loss += loss.item()\n    else:\n        y_hat = model(inputs)\n        if self.kd_loss is not None:\n            loss = self.kd_loss(y_hat, outputs, y_supernet)\n        else:\n            loss = self.loss_function(y_hat, outputs)\n        loss *= scale_loss\n        loss.backward() if self.fabric is None else self.fabric.backward(loss)\n        total_loss = loss.item()\n    self.current_step += 1\n    return total_loss\n</code></pre>"},{"location":"api/whittle/training_strategies/base_strategy/","title":"Base strategy","text":""},{"location":"api/whittle/training_strategies/base_strategy/#whittle.training_strategies.base_strategy","title":"whittle.training_strategies.base_strategy","text":""},{"location":"api/whittle/training_strategies/base_strategy/#whittle.training_strategies.base_strategy.BaseTrainingStrategy","title":"BaseTrainingStrategy","text":"<pre><code>BaseTrainingStrategy(\n    sampler: BaseSampler,\n    loss_function: Callable,\n    kd_loss: Callable | None = None,\n    device: str = \"cuda\",\n    lora: bool = False,\n    fabric: Fabric | None = None,\n    **kwargs\n)\n</code></pre> <p>Base Training Strategy.</p> <p>Base class that all training strategies inherit from.</p> <pre><code>sampler: sampler that returns a sub-network when called\nloss_function: loss function to compute the loss of a sub-network\ndevice: device to run the model on\n**kwargs:\n</code></pre> Source code in <code>whittle/training_strategies/base_strategy.py</code> <pre><code>def __init__(\n    self,\n    sampler: BaseSampler,\n    loss_function: Callable,\n    kd_loss: Callable | None = None,\n    device: str = \"cuda\",\n    lora: bool = False,\n    fabric: Fabric | None = None,\n    **kwargs,\n):\n    \"\"\"\n    Initialises a `BaseTrainingStrategy`\n    Args:\n        sampler: sampler that returns a sub-network when called\n        loss_function: loss function to compute the loss of a sub-network\n        device: device to run the model on\n        **kwargs:\n    \"\"\"\n    self.sampler = sampler\n    self.loss_function = loss_function\n    self.device = device\n    self.kd_loss = kd_loss\n    self.lora = lora\n    self.fabric = fabric\n    if isinstance(self.kd_loss, DistillLoss):\n        if not isinstance(loss_function, torch.nn.CrossEntropyLoss):\n            raise TypeError(\n                \"KD Loss not yet supported: Expected torch.nn.CrossEntropyLoss\"\n            )\n</code></pre>"},{"location":"api/whittle/training_strategies/random/","title":"Random","text":""},{"location":"api/whittle/training_strategies/random/#whittle.training_strategies.random","title":"whittle.training_strategies.random","text":""},{"location":"api/whittle/training_strategies/random/#whittle.training_strategies.random.RandomStrategy","title":"RandomStrategy","text":"<pre><code>RandomStrategy(random_samples: int = 1, **kwargs: Any)\n</code></pre> <p>               Bases: <code>BaseTrainingStrategy</code></p> <p>Random strategy.</p> <p>Randomly samples and updates <code>random_samples</code> sub-networks in each step.</p> PARAMETER DESCRIPTION <code>random_samples</code> <p>the number of randomly sampled sub-networks to sample and update in each step</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>**kwargs</code> <p>kwargs of <code>BaseTrainingStrategy</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>whittle/training_strategies/random.py</code> <pre><code>def __init__(self, random_samples: int = 1, **kwargs: Any):\n    \"\"\"\n    Initialises a `RandomStrategy`\n\n    Args:\n        random_samples: the number of randomly sampled sub-networks to sample and update in each step\n        **kwargs: kwargs of `BaseTrainingStrategy`\n    \"\"\"\n    super().__init__(**kwargs)\n    self.random_samples = random_samples\n</code></pre>"},{"location":"api/whittle/training_strategies/random/#whittle.training_strategies.random.RandomStrategy.__call__","title":"__call__","text":"<pre><code>__call__(model, inputs, outputs, scale_loss=1, **kwargs)\n</code></pre> <p>Updates randomly sampled sub-networks in each step.</p> Source code in <code>whittle/training_strategies/random.py</code> <pre><code>def __call__(self, model, inputs, outputs, scale_loss=1, **kwargs):\n    \"\"\"Updates randomly sampled sub-networks in each step.\"\"\"\n    total_loss = 0\n    for i in range(self.random_samples):\n        config = self.sampler.sample()\n        model.set_sub_network(**config)\n        loss = self.compute_loss(model, inputs, outputs)\n        loss *= scale_loss\n        loss.backward() if self.fabric is None else self.fabric.backward(loss)\n        model.reset_super_network()\n\n        total_loss += loss.item()\n    return total_loss\n</code></pre>"},{"location":"api/whittle/training_strategies/random_linear/","title":"Random linear","text":""},{"location":"api/whittle/training_strategies/random_linear/#whittle.training_strategies.random_linear","title":"whittle.training_strategies.random_linear","text":""},{"location":"api/whittle/training_strategies/random_linear/#whittle.training_strategies.random_linear.RandomLinearStrategy","title":"RandomLinearStrategy","text":"<pre><code>RandomLinearStrategy(\n    total_number_of_steps: int,\n    random_samples: int = 1,\n    **kwargs: Any\n)\n</code></pre> <p>               Bases: <code>BaseTrainingStrategy</code></p> <p>Random linear strategy.</p> <p>Updates <code>random_samples</code> randomly sampled sub-network with probability <code>p</code> or the super-network with <code>1 - p</code>. <code>p</code> linearly increases with the step count.</p> refs <p>Structural Pruning of Pre-trained Language Models via Neural Architecture Search Aaron Klein, Jacek Golebiowski, Xingchen Ma, Valerio Perrone, Cedric Archambeau arxiv.org/abs/2405.02267</p> <p>Understanding and Simplifying One-Shot Architecture Search Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, Quoc Le International Conference on Machine Learning (ICML) 2018 proceedings.mlr.press/v80/bender18a/bender18a.pdf</p> PARAMETER DESCRIPTION <code>total_number_of_steps</code> <p>the number of steps the optimization runs for</p> <p> TYPE: <code>int</code> </p> <code>random_samples</code> <p>the number of randomly sampled sub-networks to sample and update in each step</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>**kwargs</code> <p>kwargs of <code>BaseTrainingStrategy</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>whittle/training_strategies/random_linear.py</code> <pre><code>def __init__(\n    self, total_number_of_steps: int, random_samples: int = 1, **kwargs: Any\n):\n    \"\"\"\n    Initialises a `RandomLinearStrategy`\n\n    Args:\n        total_number_of_steps: the number of steps the optimization runs for\n        random_samples: the number of randomly sampled sub-networks to sample and update in each step\n        **kwargs: kwargs of `BaseTrainingStrategy`\n    \"\"\"\n    super().__init__(**kwargs)\n    self.random_samples = random_samples\n    self.total_number_of_steps = total_number_of_steps\n    self.current_step = 0\n    self.rate = np.linspace(0.0, 1, total_number_of_steps)\n</code></pre>"},{"location":"api/whittle/training_strategies/sandwich/","title":"Sandwich","text":""},{"location":"api/whittle/training_strategies/sandwich/#whittle.training_strategies.sandwich","title":"whittle.training_strategies.sandwich","text":""},{"location":"api/whittle/training_strategies/sandwich/#whittle.training_strategies.sandwich.SandwichStrategy","title":"SandwichStrategy","text":"<pre><code>SandwichStrategy(random_samples: int = 2, **kwargs: Any)\n</code></pre> <p>               Bases: <code>BaseTrainingStrategy</code></p> <p>Sandwich strategy.</p> <p>In each step, the sandwich strategy updates the super-network, the smallest, and a set of randomly sampled sub-networks.</p> refs <p>Universally Slimmable Networks and Improved Training Techniques Jiahui Yu, Thomas Huang International Conference on Computer Vision 2019 arxiv.org/abs/1903.05134</p> PARAMETER DESCRIPTION <code>random_samples</code> <p>the number of randomly sampled sub-networks to sample and update in each step</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>**kwargs</code> <p>kwargs of <code>BaseTrainingStrategy</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>whittle/training_strategies/sandwich.py</code> <pre><code>def __init__(self, random_samples: int = 2, **kwargs: Any):\n    \"\"\"\n    Initialises a `SandwichStrategy`\n\n    Args:\n        random_samples: the number of randomly sampled sub-networks to sample and update in each step\n        **kwargs: kwargs of `BaseTrainingStrategy`\n    \"\"\"\n    super().__init__(**kwargs)\n    self.random_samples = random_samples\n</code></pre>"},{"location":"api/whittle/training_strategies/standard/","title":"Standard","text":""},{"location":"api/whittle/training_strategies/standard/#whittle.training_strategies.standard","title":"whittle.training_strategies.standard","text":""},{"location":"api/whittle/training_strategies/standard/#whittle.training_strategies.standard.StandardStrategy","title":"StandardStrategy","text":"<pre><code>StandardStrategy(**kwargs: Any)\n</code></pre> <p>               Bases: <code>BaseTrainingStrategy</code></p> <p>Standard strategy.</p> <p>Implements the standard update rule and updates all weights of the super-network.</p> PARAMETER DESCRIPTION <code>**kwargs</code> <p>kwargs of <code>BaseTrainingStrategy</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>whittle/training_strategies/standard.py</code> <pre><code>def __init__(self, **kwargs: Any):\n    \"\"\"\n    Initialises a `StandardStrategy`\n\n    Args:\n        **kwargs: kwargs of `BaseTrainingStrategy`\n    \"\"\"\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api/whittle/training_strategies/strategies/","title":"Strategies","text":""},{"location":"api/whittle/training_strategies/strategies/#whittle.training_strategies.strategies","title":"whittle.training_strategies.strategies","text":""},{"location":"api/whittle/tutorials/gpt_utils/","title":"Gpt utils","text":""},{"location":"api/whittle/tutorials/gpt_utils/#whittle.tutorials.gpt_utils","title":"whittle.tutorials.gpt_utils","text":""},{"location":"api/whittle/tutorials/gpt_utils/#whittle.tutorials.gpt_utils.estimate_loss","title":"estimate_loss","text":"<pre><code>estimate_loss(model: Module, eval_iters: int)\n</code></pre> <p>Function to evaluate the model on train &amp; valid splits.</p> Source code in <code>whittle/tutorials/gpt_utils.py</code> <pre><code>@torch.no_grad()\ndef estimate_loss(model: nn.Module, eval_iters: int):\n    \"\"\"Function to evaluate the model on train &amp; valid splits.\"\"\"\n    out = {}\n    model.eval()\n    for split in [\"train\", \"valid\"]:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            B, T = X.shape\n            # evaluate loss on the batch\n            logits = model(X)\n            logits = logits.view(B * T, -1)\n            targets = Y.view(B * T)\n            loss = F.cross_entropy(logits, targets)\n            # logits = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n</code></pre>"},{"location":"api/whittle/tutorials/gpt_utils/#whittle.tutorials.gpt_utils.get_batch","title":"get_batch","text":"<pre><code>get_batch(\n    split: str,\n    block_size: int = 8,\n    batch_size: int = 4,\n    device: str = \"cuda\",\n)\n</code></pre> <p>Gets a randomized batch from the split of data chosen.</p>"},{"location":"api/whittle/tutorials/gpt_utils/#whittle.tutorials.gpt_utils.get_batch--arguments","title":"Arguments","text":"<p>split : str, {\"train\", \"valid\"} block_size : int     The context length for predictions, that is, a sentence length batch_size : int     The batch size, that is, the number of sentences</p> Source code in <code>whittle/tutorials/gpt_utils.py</code> <pre><code>def get_batch(split: str, block_size: int = 8, batch_size: int = 4, device: str = \"cuda\"):\n    \"\"\"Gets a randomized batch from the split of data chosen.\n\n    Arguments\n    ---------\n    split : str, {\"train\", \"valid\"}\n    block_size : int\n        The context length for predictions, that is, a sentence length\n    batch_size : int\n        The batch size, that is, the number of sentences\n    \"\"\"\n    # generate a small batch of data of inputs x and targets y\n    assert split in [\"train\", \"valid\"]\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    data = train_data if split == \"train\" else valid_data\n    # generating random indices as markers in the full text document\n    # such that they are a starting point to the sentence of length\n    # `block_size` that will be a data point in the batch\n    ix = torch.randint(low=0, high=len(data) - block_size, size=(batch_size,))\n    # extracting a sentence of length `block_size` for every\n    # random starting point in `ix`\n    x = torch.stack([data[i : i + block_size] for i in ix])\n    # extracting a sentence of length `block_size` for every\n    # random starting point in `ix` + 1 (shifted to right)\n    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n</code></pre>"},{"location":"api/whittle/tutorials/gpt_utils/#whittle.tutorials.gpt_utils.to_tokens","title":"to_tokens","text":"<pre><code>to_tokens(example)\n</code></pre> <p>Function to tokenize a string using BPE.</p> Source code in <code>whittle/tutorials/gpt_utils.py</code> <pre><code>def to_tokens(example):\n    \"\"\"Function to tokenize a string using BPE.\"\"\"\n    enc = tiktoken.get_encoding(\"gpt2\")\n    ids = enc.encode_ordinary(\n        example[\"text\"]\n    )  # encode_ordinary ignores any special tokens\n    ids.append(enc.eot_token)  # add the end of text token, e.g. 50256 for gpt2 bpe\n    # note: I think eot should be prepended not appended... hmm. it's called \"eot\" though...\n    out = {\"ids\": ids, \"len\": len(ids)}\n    return out\n</code></pre>"}]}